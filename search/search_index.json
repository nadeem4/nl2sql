{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"NL2SQL Platform Documentation","text":"<p>This documentation describes the current, production-grade runtime behavior of NL2SQL as implemented in code. It is engineered for platform engineers, system architects, and contributors who need a precise mental model of the architecture, contracts, and operational behavior.</p>"},{"location":"#problem-this-system-solves","title":"Problem this system solves","text":"<p>NL2SQL converts natural language requests into deterministic, validated SQL across one or more datasources. It enforces schema-grounded planning, policy constraints, and reproducible execution. The system is built for multi-datasource enterprise environments where correctness, safety, and observability matter more than conversational flexibility.</p>"},{"location":"#design-philosophy","title":"Design philosophy","text":"<ul> <li>Determinism first: stable IDs, deterministic DAG layering, and structured ASTs ensure the same input yields the same orchestration structure.</li> <li>Schema grounding: planning is constrained by a schema snapshot retrieved via structured chunks.</li> <li>Explicit validation gates: logical validation enforces schema and RBAC constraints before execution.</li> <li>Modularity: adapters, subgraphs, and executors are capability-driven and replaceable.</li> <li>Isolation: execution and indexing can be offloaded to sandboxed process pools.</li> <li>Observability: structured logging, metrics, and audit events are first-class.</li> </ul>"},{"location":"#non-functional-goals","title":"Non-functional goals","text":"<ul> <li>Reliability under partial failures (circuit breakers, retry semantics, safe failure).</li> <li>Extensibility via plugins and registries (datasources, subgraphs, executors).</li> <li>Cost awareness (row limits, byte limits, optional dry run/cost estimate).</li> <li>Security by default (RBAC, policy-based table access, audit logging).</li> </ul>"},{"location":"#high-level-flow","title":"High-level flow","text":"<pre><code>flowchart TD\n    User[User Query] --&gt; Resolver[DatasourceResolverNode]\n    Resolver --&gt; Decomposer[DecomposerNode]\n    Decomposer --&gt; Planner[GlobalPlannerNode]\n    Planner --&gt; Router[Scan Layer Router]\n    Router --&gt; Subgraph[SQL Agent Subgraph]\n    Subgraph --&gt; Router\n    Router --&gt; Aggregator[EngineAggregatorNode]\n    Aggregator --&gt; Synthesizer[AnswerSynthesizerNode]</code></pre>"},{"location":"#core-entry-points","title":"Core entry points","text":"<ul> <li>Pipeline runtime: <code>nl2sql.pipeline.runtime.run_with_graph</code></li> <li>Graph builder: <code>nl2sql.pipeline.graph.build_graph</code></li> <li>Application context: <code>nl2sql.context.NL2SQLContext</code></li> </ul>"},{"location":"#navigate-the-docs","title":"Navigate the docs","text":"<ul> <li><code>architecture/overview.md</code> for end-to-end system architecture and subsystem boundaries.</li> <li><code>architecture/pipeline.md</code> for LangGraph pipeline flow, routing, and execution DAG behavior.</li> <li><code>architecture/graph_state.md</code>, <code>architecture/determinism.md</code>, and <code>architecture/invariants.md</code> for state, determinism, and enforced rules.</li> <li><code>architecture/failure_recovery.md</code> for failure domains and retry scope.</li> <li><code>schema/store.md</code> and <code>architecture/indexing.md</code> for schema contracts, chunking, and retrieval.</li> <li><code>execution/sandbox.md</code> for execution isolation and concurrency details.</li> <li><code>adapters/architecture.md</code> for plugin discovery and capability-based routing.</li> <li><code>observability/stack.md</code> and <code>observability/error-handling.md</code> for metrics, logging, and error contracts.</li> </ul>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#core-concepts","title":"Core concepts","text":"<ul> <li>ArtifactRef: Metadata reference to persisted query results (URI, backend, schema_version).</li> <li>Chunk: Typed, vector-indexed schema slice used for retrieval (datasource, table, column, relationship).</li> <li>ExecutionDAG: Deterministic logical DAG of scan/combine/post operations.</li> <li>GraphState: Shared state across the control graph pipeline.</li> <li>SchemaContract: Canonical schema structure for a datasource (tables, columns, foreign keys).</li> <li>SchemaMetadata: Descriptive metadata and statistics for schema elements.</li> <li>SchemaSnapshot: Pair of <code>SchemaContract</code> + <code>SchemaMetadata</code>.</li> <li>SubQuery: Decomposed, datasource-scoped query intent from the decomposer.</li> <li>Subgraph: LangGraph subgraph that handles a specific execution capability (e.g., SQL agent).</li> <li>ResultFrame: Adapter response containing rows/columns, row_count, and error metadata.</li> <li>VectorStore: Chroma-backed vector index for schema chunks.</li> <li>RBAC: Role-based access control enforcing datasource/table permissions.</li> </ul>"},{"location":"adapters/","title":"Supported Adapters","text":"<p>The NL2SQL Platform supports a variety of datasources through specialized adapters. Each adapter is designed to handle the specific idiosyncrasies of its underlying database engine, from connection strings to specialized <code>EXPLAIN</code> plans.</p>"},{"location":"adapters/#sql-adapters","title":"SQL Adapters","text":"<p>We provide first-class support for the following SQL databases via SQLAlchemy.</p> Adapter Description Status PostgreSQL Full support including <code>EXPLAIN</code>, JSONB, and SSL. \ud83d\udfe2 Stable MySQL Support for 5.7+ and 8.0. Includes <code>MAX_EXECUTION_TIME</code> management. \ud83d\udfe2 Stable Microsoft SQL Server Enterprise support via <code>pyodbc</code> and <code>T-SQL</code> dialect. \ud83d\udfe1 Beta SQLite File-based local development. \ud83d\udfe2 Stable"},{"location":"adapters/#core-libraries","title":"Core Libraries","text":"<p>For developers building their own adapters, we provide detailed reference documentation for our core SDKs.</p> <ul> <li>Adapter SDK Reference: The core interface (<code>DatasourceAdapter</code>) that all adapters must implement.</li> <li>SQLAlchemy Adapter Reference: The helper base class (<code>BaseSQLAlchemyAdapter</code>) for building SQL-based adapters.</li> </ul>"},{"location":"adapters/#missing-your-database","title":"Missing your database?","text":"<p>Can't find what you need? Check out the Building Adapters guide to see how to implement your own.</p>"},{"location":"adapters/#configuration","title":"Configuration","text":"<p>All adapters are configured in your <code>configs/datasources.yaml</code> file.</p> <pre><code>version: 1\ndatasources:\n  - id: \"sales_db\"\n    connection:\n      type: \"postgres\"\n      host: \"${env:DB_HOST}\"\n      port: 5432\n      # ... see specific adapter docs for full reference\n</code></pre>"},{"location":"adapters/architecture/","title":"Plugin / Adapter Architecture","text":"<p>Adapters integrate NL2SQL with external datasources. Each adapter implements a protocol contract, is discovered via Python entry points, and is registered in the <code>DatasourceRegistry</code>.</p>"},{"location":"adapters/architecture/#discovery-and-registration","title":"Discovery and registration","text":"<pre><code>flowchart TD\n    Config[configs/datasources.yaml] --&gt; Registry[DatasourceRegistry]\n    Registry --&gt; Discovery[discover_adapters()]\n    Discovery --&gt; EntryPoints[entry_points('nl2sql.adapters')]\n    EntryPoints --&gt; AdapterClass[Adapter Class]\n    AdapterClass --&gt; AdapterInstance[DatasourceAdapterProtocol instance]</code></pre>"},{"location":"adapters/architecture/#core-adapter-contract","title":"Core adapter contract","text":"<pre><code>classDiagram\n    class DatasourceAdapterProtocol {\n        +capabilities() Set\n        +connect()\n        +fetch_schema_snapshot()\n        +execute(AdapterRequest) ResultFrame\n        +get_dialect() str\n    }\n    class AdapterRequest {\n        +plan_type\n        +payload\n        +limits\n    }\n    class ResultFrame {\n        +success\n        +columns\n        +rows\n        +row_count\n        +error\n    }</code></pre> <p>See <code>sdk.md</code> for the authoritative adapter interface reference and required fields/methods.</p>"},{"location":"adapters/architecture/#capability-driven-routing","title":"Capability-driven routing","text":"<p>Adapters expose capabilities (e.g., <code>supports_sql</code>, <code>supports_schema_introspection</code>). These capabilities drive:</p> <ul> <li>Subgraph selection (<code>resolve_subgraph()</code> in routing).</li> <li>Executor selection (<code>ExecutorRegistry.get_executor()</code>).</li> </ul> <pre><code>flowchart TD\n    Adapter[DatasourceAdapterProtocol] --&gt; Caps[capabilities()]\n    Caps --&gt; Exec[ExecutorRegistry]\n    Caps --&gt; Subgraph[resolve_subgraph()]\n    Exec --&gt; Service[Executor Service]\n    Subgraph --&gt; Graph[Subgraph Selection]</code></pre>"},{"location":"adapters/architecture/#multi-datasource-routing","title":"Multi-datasource routing","text":"<p>The control graph can resolve multiple datasources for a single user query. <code>DecomposerNode</code> produces sub-queries scoped to individual datasources. Each sub-query is then routed to a subgraph that matches its adapter capabilities.</p>"},{"location":"adapters/architecture/#extensibility-model","title":"Extensibility model","text":"<p>To add a new adapter:</p> <ol> <li>Implement <code>DatasourceAdapterProtocol</code> (or extend a base adapter).</li> <li>Publish the adapter class as an <code>nl2sql.adapters</code> entry point.</li> <li>Configure the datasource in <code>configs/datasources.yaml</code>.</li> </ol>"},{"location":"adapters/architecture/#source-references","title":"Source references","text":"<ul> <li>Adapter protocol: <code>packages/adapter-sdk/src/nl2sql_adapter_sdk/protocols.py</code></li> <li>Adapter discovery: <code>packages/core/src/nl2sql/datasources/discovery.py</code></li> <li>Datasource registry: <code>packages/core/src/nl2sql/datasources/registry.py</code></li> <li>Example adapter: <code>packages/adapter-sqlalchemy/src/nl2sql_sqlalchemy_adapter/adapter.py</code></li> </ul>"},{"location":"adapters/development/","title":"Building Adapters Guide","text":"<p>The NL2SQL Platform is designed to be extensible. You can build adapters for any datasource, from SQL databases to REST APIs.</p>"},{"location":"adapters/development/#implementation-path","title":"Implementation Path","text":"<p>There are two primary ways to build an adapter. Choose the one that fits your target:</p> If you are checking... Use... Reference A standard SQL Database (Postgres, Oracle, Snowflake) <code>nl2sql-adapter-sqlalchemy</code> SQLAlchemy Adapter Reference A NoSQL DB, REST API, or custom driver Core adapter protocol Adapter Interface Reference"},{"location":"adapters/development/#option-1-the-fast-lane-sqlalchemy","title":"Option 1: The \"Fast Lane\" (SQLAlchemy)","text":"<p>For 95% of use cases, you are connecting to a SQL database that already has a Python SQLAlchemy dialect.</p> <p>Use <code>BaseSQLAlchemyAdapter</code>. It handles:</p> <ul> <li>Automatic Schema Introspection (Tables, PKs, FKs)</li> <li>Connection Pooling</li> <li>Statistic Gathering</li> <li>Transaction-based Dry Runs</li> </ul>"},{"location":"adapters/development/#example","title":"Example","text":"<pre><code>from nl2sql_sqlalchemy_adapter import BaseSQLAlchemyAdapter\n\nclass PostgresAdapter(BaseSQLAlchemyAdapter):\n    def construct_uri(self, args: Dict[str, Any]) -&gt; str:\n        # Convert args to connection string\n        return f\"postgresql://{args['user']}:{args['password']}@{args['host']}/{args['database']}\"\n</code></pre> <p>See the SQLAlchemy Adapter Reference for full API details.</p>"},{"location":"adapters/development/#option-2-the-custom-path-protocol","title":"Option 2: The \"Custom\" Path (Protocol)","text":"<p>If you need to connect to something else (e.g., ElasticSearch, a CRM API, or a raw SQL driver), implement the core adapter protocol.</p> <p>Implement <code>DatasourceAdapterProtocol</code>. You must manually handle:</p> <ul> <li>Fetching and normalizing schema metadata.</li> <li>Executing queries and formatting results.</li> <li>Implementing safety breakers (<code>row_limit</code>).</li> </ul>"},{"location":"adapters/development/#example_1","title":"Example","text":"<pre><code>from nl2sql.datasources.protocols import DatasourceAdapterProtocol\nfrom nl2sql_adapter_sdk.contracts import AdapterRequest, ResultFrame\nfrom nl2sql_adapter_sdk.capabilities import DatasourceCapability\n\nclass MyRestAdapter(DatasourceAdapterProtocol):\n    def capabilities(self):\n        return {DatasourceCapability.SUPPORTS_REST}\n\n    def fetch_schema_snapshot(self):\n        # call API, return schema\n        pass\n\n    def execute(self, request: AdapterRequest) -&gt; ResultFrame:\n        # run request, return rows\n        pass\n</code></pre> <p>See the Adapter Interface Reference for the method signatures and compliance guide.</p>"},{"location":"adapters/development/#compliance-testing","title":"Compliance Testing","text":"<p>Regardless of which path you choose, your adapter MUST pass the compliance test suite to ensure it handles types and errors correctly.</p> <pre><code># See Adapter Interface Reference for test setup\n</code></pre>"},{"location":"adapters/mssql/","title":"Microsoft SQL Server (MSSQL) Adapter","text":"<p>Support for SQL Server 2017+ and Azure SQL.</p> <p>This adapter extends <code>BaseSQLAlchemyAdapter</code> but provides specialized <code>dry_run</code> logic using <code>SET NOEXEC ON</code> to safely validate T-SQL.</p>"},{"location":"adapters/mssql/#configuration","title":"Configuration","text":"<p>Type: <code>mssql</code></p> <pre><code>connection:\n  type: \"mssql\"\n  host: \"localhost\"\n  port: 1433\n  user: \"sa\"\n  password: \"${env:DB_PASS}\"\n  database: \"my_db\"\n  driver: \"ODBC Driver 17 for SQL Server\" # Default\n  trusted_connection: false\n</code></pre>"},{"location":"adapters/mssql/#connection-details","title":"Connection Details","text":"<ul> <li>Driver: <code>pyodbc</code>. Requires system ODBC headers installed.</li> <li>URI Constructed: <code>mssql+pyodbc://{user}:{pass}@{host}:{port}/{db}?driver={driver}</code></li> </ul>"},{"location":"adapters/mssql/#features","title":"Features","text":"Feature Implementation Note Timeout Not strictly enforced by driver Rely on global statement timeout. Dry Run <code>SET NOEXEC ON</code> Validates syntax without execution. Costing <code>SET SHOWPLAN_XML ON</code> Parses XML for <code>StatementSubTreeCost</code>."},{"location":"adapters/mssql/#optimization-details","title":"Optimization Details","text":"<ul> <li>Dry Run: Uses <code>SET NOEXEC ON</code>. This is a native T-SQL session setting that compiles the query but ensures it is not executed. This is extremely safe and accurate for validation.</li> <li>Explain: Uses <code>SET SHOWPLAN_XML ON</code> to retrieve the execution plan in XML format.</li> <li>Cost Estimate: Parses the XML plan to find <code>StatementSubTreeCost</code> (estimated cost) and <code>StatementEstRows</code> (estimated rows).</li> </ul>"},{"location":"adapters/mssql/#requirements","title":"Requirements","text":"<p>You must have the MS ODBC Driver installed in your Docker image or local environment.</p> <p>Download ODBC Driver for SQL Server</p> <pre><code># Debian / Ubuntu\nsudo apt-get install unixodbc-dev\n</code></pre>"},{"location":"adapters/mysql/","title":"MySQL Adapter","text":"<p>Support for MySQL 5.7+ and 8.0+.</p> <p>Implementation</p> <p>This adapter extends <code>BaseSQLAlchemyAdapter</code>. It overrides <code>connect()</code> to handle <code>MAX_EXECUTION_TIME</code> session variables.</p>"},{"location":"adapters/mysql/#configuration","title":"Configuration","text":"<p>Type: <code>mysql</code></p> <pre><code>connection:\n  type: \"mysql\"\n  host: \"localhost\"\n  port: 3306\n  user: \"root\"\n  password: \"${env:DB_PASS}\"\n  database: \"my_db\"\n  options:\n    charset: \"utf8mb4\"\n</code></pre>"},{"location":"adapters/mysql/#connection-details","title":"Connection Details","text":"<ul> <li>Driver: <code>pymysql</code> (Pure Python).</li> <li>URI Constructed: <code>mysql+pymysql://{user}:{pass}@{host}:{port}/{db}?{options}</code></li> </ul>"},{"location":"adapters/mysql/#features","title":"Features","text":"Feature Implementation Note Timeout <code>SET MAX_EXECUTION_TIME={ms}</code> Session-level enforcement. Dry Run Transaction Rollback Starts transaction, runs SQL, rolls back. Costing <code>EXPLAIN FORMAT=JSON</code> Extracts <code>query_cost</code>. Stats <code>SELECT count(*), min(), max()</code> Standard aggregation."},{"location":"adapters/mysql/#optimization-details","title":"Optimization Details","text":"<ul> <li>Dry Run: Uses a Transaction Rollback strategy. It starts a transaction (<code>BEGIN</code>), executes the query, and immediately rolls back (<code>ROLLBACK</code>). Note: This means the query is technically executed, but its effects are reversed.</li> <li>Explain: Uses <code>EXPLAIN FORMAT=JSON {sql}</code> to get the execution plan.</li> <li>Cost Estimate: Parses the JSON output to extract <code>query_cost</code>. MySQL does not reliably provide a global \"estimated rows\" count for complex queries, so this is often returned as 0.</li> </ul>"},{"location":"adapters/mysql/#limitations","title":"Limitations","text":"<ul> <li>Row Estimation: MySQL's <code>EXPLAIN</code> does not always provide a reliable \"Total Rows\" estimate for complex joins compared to Postgres.</li> </ul>"},{"location":"adapters/postgres/","title":"PostgreSQL Adapter","text":"<p>The Postgres adapter is the Gold Standard adapter for the platform. It supports the full set of optimization features including <code>EXPLAIN</code>-based dry runs and cost estimation.</p> <p>This adapter extends <code>BaseSQLAlchemyAdapter</code>, leveraging automatic schema reflection and statistics gathering.</p>"},{"location":"adapters/postgres/#configuration","title":"Configuration","text":"<p>Type: <code>postgres</code> (or <code>postgresql</code>)</p> <pre><code>connection:\n  type: \"postgres\"\n  host: \"localhost\"\n  port: 5432\n  user: \"postgres\"\n  password: \"${env:DB_PASS}\"\n  database: \"my_db\"\n  options:\n    sslmode: \"require\" # Optional: passed to query string\n</code></pre>"},{"location":"adapters/postgres/#connection-details","title":"Connection Details","text":"<ul> <li>Driver: <code>psycopg2</code> (via <code>sqlalchemy</code>).</li> <li>URI Constructed: <code>postgresql://{user}:{pass}@{host}:{port}/{db}?{options}</code></li> </ul>"},{"location":"adapters/postgres/#features","title":"Features","text":"Feature Implementation Note Timeout Native <code>-c statement_timeout={ms}</code> Enforced server-side. Dry Run <code>EXPLAIN {sql}</code> Highly accurate validation. Costing <code>EXPLAIN (FORMAT JSON) {sql}</code> Returns \"Total Cost\" and \"Plan Rows\". Stats Optimized Queries Fetches <code>null_perc</code>, <code>distinct</code>, <code>min/max</code>."},{"location":"adapters/postgres/#optimization-details","title":"Optimization Details","text":"<p>The Postgres adapter leverages native <code>EXPLAIN</code> capabilities for robust validation and estimation:</p> <ul> <li>Dry Run: Implemented via <code>EXPLAIN {sql}</code>. This validates the SQL syntax and ensures that all tables/columns exist without actually executing the query.</li> <li>Explain: Uses <code>EXPLAIN (FORMAT JSON) {sql}</code> to retrieve the full query execution plan in structured JSON format.</li> <li>Cost Estimate: Uses the same <code>EXPLAIN (FORMAT JSON) {sql}</code> command. It parses the root <code>Plan</code> object to extract:</li> <li><code>Total Cost</code>: Used as the query cost proxy.</li> <li><code>Plan Rows</code>: Used as the estimated result size.</li> </ul>"},{"location":"adapters/postgres/#troubleshooting","title":"Troubleshooting","text":""},{"location":"adapters/postgres/#ssl-verification","title":"SSL Verification","text":"<p>If connecting to Azure or AWS RDS with strict SSL, ensure you pass the CA certificate path in options or standard libpq environment variables, or use <code>sslmode: disable</code> for testing.</p>"},{"location":"adapters/sdk/","title":"Adapter Interface Reference","text":"<p>The adapter contract lives in the adapter SDK:</p> <ul> <li><code>nl2sql.datasources.protocols.DatasourceAdapterProtocol</code></li> <li><code>nl2sql_adapter_sdk.contracts.AdapterRequest</code></li> <li><code>nl2sql_adapter_sdk.contracts.ResultFrame</code></li> </ul>"},{"location":"adapters/sdk/#interface-datasourceadapterprotocol","title":"Interface: <code>DatasourceAdapterProtocol</code>","text":"<p>Adapters expose a capability-driven interface:</p> <pre><code>from nl2sql.datasources.protocols import DatasourceAdapterProtocol\nfrom nl2sql_adapter_sdk.contracts import AdapterRequest, ResultFrame\nfrom nl2sql_adapter_sdk.capabilities import DatasourceCapability\n</code></pre>"},{"location":"adapters/sdk/#mandatory-properties","title":"Mandatory Properties","text":"Property Type Description <code>datasource_id</code> <code>str</code> Unique identifier (e.g., \"production_db\"). <code>datasource_engine_type</code> <code>str</code> Engine type string (e.g., <code>postgres</code>, <code>rest</code>). <code>row_limit</code> <code>int</code> Safety breaker (limit returned rows). <code>max_bytes</code> <code>int</code> Safety breaker (limit payload size)."},{"location":"adapters/sdk/#mandatory-methods","title":"Mandatory Methods","text":""},{"location":"adapters/sdk/#capabilities-setdatasourcecapability","title":"<code>capabilities() -&gt; set[DatasourceCapability]</code>","text":"<p>Declares supported capabilities (e.g., <code>supports_sql</code>, <code>supports_rest</code>).</p>"},{"location":"adapters/sdk/#executerequest-adapterrequest-resultframe","title":"<code>execute(request: AdapterRequest) -&gt; ResultFrame</code>","text":"<p>Executes a plan-specific request and returns a normalized <code>ResultFrame</code>.</p> <ul> <li>Args: <code>AdapterRequest</code> with <code>plan_type</code> and <code>payload</code></li> <li>Returns: <code>ResultFrame</code> with <code>columns</code>, <code>rows</code>, <code>row_count</code>, and error metadata</li> </ul>"},{"location":"adapters/sdk/#fetch_schema_snapshot","title":"<code>fetch_schema_snapshot()</code>","text":"<p>Required only if <code>supports_schema_introspection</code> is advertised.</p>"},{"location":"adapters/sdk/#optional-methods-sql-adapters","title":"Optional Methods (SQL adapters)","text":""},{"location":"adapters/sdk/#dry_runsql-str","title":"<code>dry_run(sql: str)</code>","text":"<p>Validates SQL without executing it (or safely rolling back).</p>"},{"location":"adapters/sdk/#explainsql-str","title":"<code>explain(sql: str)</code>","text":"<p>Returns the execution plan.</p>"},{"location":"adapters/sdk/#cost_estimatesql-str","title":"<code>cost_estimate(sql: str)</code>","text":"<p>Returns cost/row estimates for the Physical Validator.</p>"},{"location":"adapters/sdk/#compliance-testing","title":"Compliance Testing","text":"<p>All adapters should pass the compliance test suite (schema introspection, type mapping, error handling, and result contract validation).</p>"},{"location":"adapters/sqlalchemy/","title":"SQLAlchemy Adapter Reference","text":"<p>The SQLAlchemy Adapter (<code>nl2sql-adapter-sqlalchemy</code>) provides a helper base class for building adapters for any SQL database supported by SQLAlchemy.</p>"},{"location":"adapters/sqlalchemy/#base-class-basesqlalchemyadapter","title":"Base Class: <code>BaseSQLAlchemyAdapter</code>","text":"<p>Constructs a robust adapter by wrapping standard SQLAlchemy components.</p> <pre><code>from nl2sql_sqlalchemy_adapter import BaseSQLAlchemyAdapter\n</code></pre>"},{"location":"adapters/sqlalchemy/#features","title":"Features","text":"Feature Description Automatic Schema Uses <code>sqlalchemy.inspect</code> to reflect tables, columns, and foreign keys automatically. Automatic Stats Runs optimized generic SQL queries to fetch <code>min</code>, <code>max</code>, <code>null_percentage</code>, and <code>distinct_count</code>. Connection Pooling Manages engine lifecycle and connection pools. Transaction Safety Implements generic <code>dry_run</code> using transaction rollbacks."},{"location":"adapters/sqlalchemy/#required-overrides","title":"Required Overrides","text":""},{"location":"adapters/sqlalchemy/#construct_uriargs-dictstr-any-str","title":"<code>construct_uri(args: Dict[str, Any]) -&gt; str</code>","text":"<p>Converts a configuration dictionary into a SQLAlchemy connection string.</p> <ul> <li>Args: <code>args</code> - The <code>connection</code> dictionary from <code>datasources.yaml</code>.</li> <li>Returns: A valid URL (e.g., <code>postgresql://...</code>).</li> </ul>"},{"location":"adapters/sqlalchemy/#optional-overrides","title":"Optional Overrides","text":""},{"location":"adapters/sqlalchemy/#connect","title":"<code>connect()</code>","text":"<p>Override to provide custom connection arguments (e.g., timeouts, isolation levels).</p>"},{"location":"adapters/sqlalchemy/#get_dialect-str","title":"<code>get_dialect() -&gt; str</code>","text":"<p>Returns the logical dialect name. Defaults to the engine driver name.</p>"},{"location":"adapters/sqlalchemy/#explainsql-str-cost_estimatesql-str","title":"<code>explain(sql: str)</code> / <code>cost_estimate(sql: str)</code>","text":"<p>The base class provides stubs. Override these to implement database-specific optimization logic (e.g., <code>EXPLAIN ANALYZE</code>).</p>"},{"location":"adapters/sqlite/","title":"SQLite Adapter","text":"<p>Simple file-based adapter for local development and testing.</p> <p>Implementation</p> <p>This adapter extends <code>BaseSQLAlchemyAdapter</code>. Note that <code>timeout</code> configurations apply to the database lock, not query execution time.</p>"},{"location":"adapters/sqlite/#configuration","title":"Configuration","text":"<p>Type: <code>sqlite</code></p> <pre><code>connection:\n  type: \"sqlite\"\n  database: \"./my_data.db\" # Absolute or relative path\n</code></pre> <p>Persistence</p> <p>If using Docker, avoid relative paths like <code>./my_data.db</code> as they will be lost on container restart. Use an absolute path mapped to a volume, e.g., <code>/app/data/my_data.db</code>.</p>"},{"location":"adapters/sqlite/#connection-details","title":"Connection Details","text":"<ul> <li>Driver: Built-in <code>sqlite3</code>.</li> <li>URI Constructed: <code>sqlite:///{database}</code></li> </ul>"},{"location":"adapters/sqlite/#features","title":"Features","text":"Feature Implementation Note Timeout <code>connect_args[\"timeout\"]</code> Controls Locking timeout, not execution. Dry Run <code>EXPLAIN QUERY PLAN</code> Validates parsing (rudimentary). Costing Stubbed Returns default cost=1.0."},{"location":"adapters/sqlite/#optimization-details","title":"Optimization Details","text":"<ul> <li>Dry Run: Uses <code>EXPLAIN QUERY PLAN {sql}</code>. If this command succeeds, the SQL syntax is valid.</li> <li>Explain: Currently stubbed (returns a simple message) as SQLite's explain output is not in a standardized, easily parsable format like JSON or XML.</li> <li>Cost Estimate: Stubbed. Returns a fixed cost of <code>1.0</code> and <code>10</code> estimated rows, as SQLite does not expose cost metrics comfortably.</li> </ul>"},{"location":"adapters/sqlite/#hints","title":"Hints","text":"<ul> <li>Concurrency: SQLite is poor at high concurrency. Use for Lite Mode or single-user testing only.</li> </ul>"},{"location":"api/","title":"API Overview","text":"<p>NL2SQL exposes two API surfaces: a Python Core API for in-process use and a REST API for remote clients. Use the links below for the full technical references.</p>"},{"location":"api/#core-api-python","title":"Core API (Python)","text":"<ul> <li>Location: Core package (<code>nl2sql-core</code>)</li> <li>Interface: Direct Python class interface (<code>NL2SQL</code> class and low-level functions)</li> <li>Use Case: Embedded/SDK use in Python applications</li> <li>Reference: Core API index and per-module references:</li> <li>Core API index</li> <li>Auth API</li> <li>Datasource API</li> <li>LLM API</li> <li>Indexing API</li> <li>Query API</li> <li>Settings API</li> <li>Result API</li> </ul>"},{"location":"api/#rest-api-fastapi","title":"REST API (FastAPI)","text":"<ul> <li>Location: API package (<code>nl2sql-api</code>)</li> <li>Interface: HTTP REST endpoints</li> <li>Use Case: Remote clients, web applications, TypeScript CLI</li> <li>Reference: REST API index and per-route references:</li> <li>REST API index</li> <li>Health API</li> <li>Query API</li> <li>Datasource API</li> <li>LLM API</li> <li>Indexing API</li> </ul>"},{"location":"api/#supporting-contracts","title":"Supporting contracts","text":"<ul> <li><code>GraphState</code> (<code>nl2sql.pipeline.state.GraphState</code>)</li> <li><code>ExecutorRequest</code> / <code>ExecutorResponse</code> (<code>nl2sql.execution.contracts</code>)</li> <li><code>ResultFrame</code> (<code>nl2sql_adapter_sdk.contracts</code>)</li> </ul>"},{"location":"api/#source-references","title":"Source references","text":"<ul> <li>Context: <code>packages/core/src/nl2sql/context.py</code></li> <li>Runtime: <code>packages/core/src/nl2sql/pipeline/runtime.py</code></li> <li>Public API: <code>packages/core/src/nl2sql/public_api.py</code></li> <li>API Package: <code>packages/api/src/nl2sql_api/</code></li> </ul>"},{"location":"api/core/auth/","title":"Auth API","text":""},{"location":"api/core/auth/#purpose","title":"Purpose","text":"<p>Enforce role-based access control (RBAC) for datasource and table access.</p>"},{"location":"api/core/auth/#responsibilities","title":"Responsibilities","text":"<ul> <li>Check permissions for a user-context against policy rules.</li> <li>Return allowed datasources and tables for a user-context.</li> </ul>"},{"location":"api/core/auth/#key-modules","title":"Key Modules","text":"<ul> <li><code>packages/core/src/nl2sql/api/auth_api.py</code></li> <li><code>packages/core/src/nl2sql/auth/models.py</code></li> <li><code>packages/core/src/nl2sql/auth/rbac.py</code></li> </ul>"},{"location":"api/core/auth/#public-surface","title":"Public Surface","text":""},{"location":"api/core/auth/#usercontext","title":"UserContext","text":"<p>Source: <code>packages/core/src/nl2sql/auth/models.py</code></p> <p>Fields: | name | type | required | meaning | | --- | --- | --- | --- | | <code>user_id</code> | <code>Optional[str]</code> | no | Unique identifier for the user. | | <code>tenant_id</code> | <code>Optional[str]</code> | no | Tenant/organization identifier. | | <code>roles</code> | <code>List[str]</code> | no | Assigned RBAC roles. |</p>"},{"location":"api/core/auth/#rolepolicy","title":"RolePolicy","text":"<p>Source: <code>packages/core/src/nl2sql/auth/models.py</code></p> <p>Fields: | name | type | required | meaning | | --- | --- | --- | --- | | <code>description</code> | <code>str</code> | yes | Human-readable role description. | | <code>role</code> | <code>str</code> | yes | Role ID for auditing/logging. | | <code>allowed_datasources</code> | <code>List[str]</code> | no | Allowed datasource IDs or <code>*</code>. | | <code>allowed_tables</code> | <code>List[str]</code> | no | Allowed tables in <code>datasource.table</code> format. |</p>"},{"location":"api/core/auth/#authapicheck_permissions","title":"AuthAPI.check_permissions","text":"<p>Source: <code>packages/core/src/nl2sql/api/auth_api.py</code></p> <p>Signature: <code>check_permissions(user_context: UserContext, datasource_id: str, table: str) -&gt; bool</code></p> <p>Parameters: | name | type | required | meaning | | --- | --- | --- | --- | | <code>user_context</code> | <code>UserContext</code> | yes | User identity + roles. | | <code>datasource_id</code> | <code>str</code> | yes | Datasource ID being accessed. | | <code>table</code> | <code>str</code> | yes | Table name (<code>datasource.table</code> format enforced by policy). |</p> <p>Returns: <code>bool</code> indicating whether access is allowed.</p> <p>Raises: None directly. Policy validation errors can surface at load time.</p> <p>Side Effects: None.</p> <p>Idempotency: Yes.</p>"},{"location":"api/core/auth/#authapiget_allowed_resources","title":"AuthAPI.get_allowed_resources","text":"<p>Source: <code>packages/core/src/nl2sql/api/auth_api.py</code></p> <p>Signature: <code>get_allowed_resources(user_context: UserContext) -&gt; dict</code></p> <p>Parameters: | name | type | required | meaning | | --- | --- | --- | --- | | <code>user_context</code> | <code>UserContext</code> | yes | User identity + roles. |</p> <p>Returns: <code>dict</code> with keys <code>datasources</code> and <code>tables</code>.</p> <p>Raises: None.</p> <p>Side Effects: None.</p> <p>Idempotency: Yes.</p>"},{"location":"api/core/auth/#behavioral-contracts","title":"Behavioral Contracts","text":"<ul> <li>Policies enforce table namespacing (<code>datasource.table</code> or <code>datasource.*</code>).</li> <li>Unknown roles yield empty permissions.</li> </ul>"},{"location":"api/core/datasource/","title":"Datasource API","text":""},{"location":"api/core/datasource/#purpose","title":"Purpose","text":"<p>Register, validate, and inspect datasources backed by adapter implementations.</p>"},{"location":"api/core/datasource/#responsibilities","title":"Responsibilities","text":"<ul> <li>Register datasources from config or programmatically.</li> <li>Validate datasource connections via adapters.</li> <li>Expose adapter capabilities and details.</li> </ul>"},{"location":"api/core/datasource/#key-modules","title":"Key Modules","text":"<ul> <li><code>packages/core/src/nl2sql/api/datasource_api.py</code></li> <li><code>packages/core/src/nl2sql/datasources/registry.py</code></li> <li><code>packages/core/src/nl2sql/datasources/models.py</code></li> <li><code>packages/core/src/nl2sql/datasources/discovery.py</code></li> </ul>"},{"location":"api/core/datasource/#public-surface","title":"Public Surface","text":""},{"location":"api/core/datasource/#connectionconfig","title":"ConnectionConfig","text":"<p>Source: <code>packages/core/src/nl2sql/datasources/models.py</code></p> <p>Fields: | name | type | required | meaning | | --- | --- | --- | --- | | <code>type</code> | <code>str</code> | yes | Adapter type (e.g., <code>postgres</code>, <code>mysql</code>). | | <code>...</code> | <code>Any</code> | no | Adapter-specific connection fields (allowed via <code>extra</code>). |</p>"},{"location":"api/core/datasource/#datasourceconfig","title":"DatasourceConfig","text":"<p>Source: <code>packages/core/src/nl2sql/datasources/models.py</code></p> <p>Fields: | name | type | required | meaning | | --- | --- | --- | --- | | <code>id</code> | <code>str</code> | yes | Datasource identifier. | | <code>description</code> | <code>Optional[str]</code> | no | Human-readable description. | | <code>connection</code> | <code>ConnectionConfig</code> | yes | Connection details. | | <code>options</code> | <code>Dict[str, Any]</code> | no | Limits and adapter options. |</p>"},{"location":"api/core/datasource/#datasourcefileconfig","title":"DatasourceFileConfig","text":"<p>Source: <code>packages/core/src/nl2sql/configs/datasources.py</code></p> <p>Fields: | name | type | required | meaning | | --- | --- | --- | --- | | <code>version</code> | <code>int</code> | yes | Schema version (defaults to 1). | | <code>datasources</code> | <code>List[DatasourceConfig]</code> | yes | Datasource definitions. |</p>"},{"location":"api/core/datasource/#datasourceapiadd_datasource","title":"DatasourceAPI.add_datasource","text":"<p>Source: <code>packages/core/src/nl2sql/api/datasource_api.py</code></p> <p>Signature: <code>add_datasource(config: Union[DatasourceConfig, Dict[str, Any]]) -&gt; None</code></p> <p>Parameters: | name | type | required | meaning | | --- | --- | --- | --- | | <code>config</code> | <code>DatasourceConfig | Dict[str, Any]</code> | yes | Datasource configuration. |</p> <p>Returns: <code>None</code>.</p> <p>Raises: - <code>ValueError</code> if datasource ID is missing or adapter type is unknown. - <code>ValueError</code> if secret resolution fails.</p> <p>Side Effects: - Registers adapter instance in <code>DatasourceRegistry</code>.</p> <p>Idempotency: - Not strictly idempotent; re-registering the same ID overwrites in-memory registry.</p>"},{"location":"api/core/datasource/#datasourceapiadd_datasource_from_config","title":"DatasourceAPI.add_datasource_from_config","text":"<p>Source: <code>packages/core/src/nl2sql/api/datasource_api.py</code></p> <p>Signature: <code>add_datasource_from_config(config_path: Union[str, pathlib.Path]) -&gt; None</code></p> <p>Parameters: | name | type | required | meaning | | --- | --- | --- | --- | | <code>config_path</code> | <code>Union[str, pathlib.Path]</code> | yes | Path to <code>datasources.yaml</code>. |</p> <p>Returns: <code>None</code>.</p> <p>Raises: - <code>FileNotFoundError</code> if config is missing. - <code>RuntimeError</code> if PyYAML is not installed. - <code>ValueError</code> for schema validation or adapter initialization errors.</p> <p>Side Effects: - Registers all datasources in registry.</p> <p>Idempotency: No (registrations happen each call).</p>"},{"location":"api/core/datasource/#datasourceapilist_datasources","title":"DatasourceAPI.list_datasources","text":"<p>Signature: <code>list_datasources() -&gt; List[str]</code></p> <p>Returns: List of registered datasource IDs.</p>"},{"location":"api/core/datasource/#datasourceapiget_adapter","title":"DatasourceAPI.get_adapter","text":"<p>Signature: <code>get_adapter(datasource_id: str) -&gt; DatasourceAdapterProtocol</code></p> <p>Raises: <code>ValueError</code> if datasource ID is unknown.</p> <p>Side Effects: None.</p>"},{"location":"api/core/datasource/#datasourceapiget_capabilities","title":"DatasourceAPI.get_capabilities","text":"<p>Signature: <code>get_capabilities(datasource_id: str) -&gt; List[str]</code></p> <p>Raises: <code>ValueError</code> if datasource ID is unknown.</p>"},{"location":"api/core/datasource/#datasourceapivalidate_connection","title":"DatasourceAPI.validate_connection","text":"<p>Signature: <code>validate_connection(ds_id: str) -&gt; bool</code></p> <p>Returns: <code>True</code> if adapter connection check succeeds.</p> <p>Raises: Adapter-specific exceptions from <code>test_connection()</code>.</p> <p>Side Effects: Per-adapter network/database connection attempts.</p>"},{"location":"api/core/datasource/#datasourceapiget_datasource_details","title":"DatasourceAPI.get_datasource_details","text":"<p>Signature: <code>get_datasource_details(datasource_id: str) -&gt; Dict[str, Any]</code></p> <p>Returns: Adapter-derived metadata (connection args, limits, capabilities).</p> <p>Raises: <code>ValueError</code> if datasource ID is unknown.</p>"},{"location":"api/core/datasource/#behavioral-contracts","title":"Behavioral Contracts","text":"<ul> <li>Adapter discovery is via entry points group <code>nl2sql.adapters</code>.</li> <li>Connection secrets are resolved before adapter instantiation.</li> <li>Capabilities default to <code>SUPPORTS_SQL</code> if adapter does not declare capabilities.</li> </ul>"},{"location":"api/core/indexing/","title":"Indexing API","text":""},{"location":"api/core/indexing/#purpose","title":"Purpose","text":"<p>Index datasource schemas into the vector store for retrieval and grounding.</p>"},{"location":"api/core/indexing/#responsibilities","title":"Responsibilities","text":"<ul> <li>Run schema indexing for one or all datasources.</li> <li>Clear the vector store.</li> </ul>"},{"location":"api/core/indexing/#key-modules","title":"Key Modules","text":"<ul> <li><code>packages/core/src/nl2sql/api/indexing_api.py</code></li> <li><code>packages/core/src/nl2sql/indexing/orchestrator.py</code></li> <li><code>packages/core/src/nl2sql/indexing/vector_store.py</code></li> <li><code>packages/core/src/nl2sql/indexing/chunk_builder.py</code></li> <li><code>packages/core/src/nl2sql/indexing/enrichment_service.py</code></li> <li><code>packages/core/src/nl2sql/schema/store.py</code></li> </ul>"},{"location":"api/core/indexing/#public-surface","title":"Public Surface","text":""},{"location":"api/core/indexing/#indexingapiindex_datasource","title":"IndexingAPI.index_datasource","text":"<p>Source: <code>packages/core/src/nl2sql/api/indexing_api.py</code></p> <p>Signature: <code>index_datasource(datasource_id: str) -&gt; Dict[str, int]</code></p> <p>Parameters: | name | type | required | meaning | | --- | --- | --- | --- | | <code>datasource_id</code> | <code>str</code> | yes | Datasource ID to index. |</p> <p>Returns: Indexing statistics by chunk type (includes <code>datasource_id</code> and <code>schema_version</code>).</p> <p>Raises: - Adapter-specific errors for schema retrieval. - LLM or vector store errors during enrichment or indexing.</p> <p>Side Effects: - Reads datasource schema. - Writes schema versions to schema store. - Writes embeddings to vector store.</p> <p>Idempotency: - Re-indexing overwrites existing chunks for the same schema version.</p>"},{"location":"api/core/indexing/#indexingapiindex_all_datasources","title":"IndexingAPI.index_all_datasources","text":"<p>Signature: <code>index_all_datasources() -&gt; Dict[str, Dict[str, int]]</code></p> <p>Returns: Map of datasource ID \u2192 stats; on error, value is <code>{\"error\": \"&lt;message&gt;\"}</code>.</p> <p>Side Effects: Indexes all registered datasources; failures are captured per datasource.</p>"},{"location":"api/core/indexing/#indexingapiclear_index","title":"IndexingAPI.clear_index","text":"<p>Signature: <code>clear_index() -&gt; None</code></p> <p>Side Effects: Deletes and reinitializes the vector store collection.</p>"},{"location":"api/core/indexing/#execution-lifecycle","title":"Execution Lifecycle","text":"<ul> <li>Fetch schema via adapter.</li> <li>Enrich schema metadata using <code>indexing_enrichment</code> LLM.</li> <li>Register schema snapshot and version.</li> <li>Build schema chunks and refresh vector store.</li> </ul>"},{"location":"api/core/llm/","title":"LLM API","text":""},{"location":"api/core/llm/#purpose","title":"Purpose","text":"<p>Configure LLM providers and expose LLM configurations by name.</p>"},{"location":"api/core/llm/#responsibilities","title":"Responsibilities","text":"<ul> <li>Register LLM configs programmatically or from file.</li> <li>Provide config lookup and listing.</li> </ul>"},{"location":"api/core/llm/#key-modules","title":"Key Modules","text":"<ul> <li><code>packages/core/src/nl2sql/api/llm_api.py</code></li> <li><code>packages/core/src/nl2sql/llm/registry.py</code></li> <li><code>packages/core/src/nl2sql/llm/models.py</code></li> <li><code>packages/core/src/nl2sql/configs/llm.py</code></li> </ul>"},{"location":"api/core/llm/#public-surface","title":"Public Surface","text":""},{"location":"api/core/llm/#agentconfig","title":"AgentConfig","text":"<p>Source: <code>packages/core/src/nl2sql/llm/models.py</code></p> <p>Fields: | name | type | required | meaning | | --- | --- | --- | --- | | <code>provider</code> | <code>str</code> | yes | Provider name (only <code>openai</code> supported). | | <code>model</code> | <code>str</code> | yes | Model identifier. | | <code>temperature</code> | <code>float</code> | no | Sampling temperature (default <code>0.0</code>). | | <code>api_key</code> | <code>Optional[SecretStr]</code> | no | API key or secret reference. | | <code>name</code> | <code>str</code> | no | Agent name (default <code>default</code>). |</p>"},{"location":"api/core/llm/#llmfileconfig","title":"LLMFileConfig","text":"<p>Source: <code>packages/core/src/nl2sql/configs/llm.py</code></p> <p>Fields: | name | type | required | meaning | | --- | --- | --- | --- | | <code>version</code> | <code>int</code> | yes | Schema version (defaults to 1). | | <code>default</code> | <code>AgentConfig</code> | yes | Default LLM configuration. | | <code>agents</code> | <code>Dict[str, AgentConfig]</code> | no | Per-agent overrides. |</p>"},{"location":"api/core/llm/#llm_apiconfigure_llm","title":"LLM_API.configure_llm","text":"<p>Source: <code>packages/core/src/nl2sql/api/llm_api.py</code></p> <p>Signature: <code>configure_llm(config: Union[AgentConfig, Dict[str, Any]]) -&gt; None</code></p> <p>Parameters: | name | type | required | meaning | | --- | --- | --- | --- | | <code>config</code> | <code>AgentConfig | Dict[str, Any]</code> | yes | LLM config; <code>name</code> defaults to <code>default</code> if omitted. |</p> <p>Returns: <code>None</code>.</p> <p>Raises: - <code>ValueError</code> for unsupported provider. - <code>ImportError</code> if provider dependency is missing (<code>langchain-openai</code>).</p> <p>Side Effects: - Registers LLM config and instantiates provider client.</p> <p>Idempotency: - Re-registering same <code>name</code> overwrites config and client.</p>"},{"location":"api/core/llm/#llm_apiconfigure_llm_from_config","title":"LLM_API.configure_llm_from_config","text":"<p>Signature: <code>configure_llm_from_config(config_path: Union[str, pathlib.Path]) -&gt; None</code></p> <p>Raises: - <code>FileNotFoundError</code> if config missing. - <code>ValueError</code> for schema validation errors.</p> <p>Side Effects: - Registers all LLMs from file; also registers <code>default</code> from file.</p>"},{"location":"api/core/llm/#llm_apiget_llm","title":"LLM_API.get_llm","text":"<p>Signature: <code>get_llm(name: str) -&gt; dict</code></p> <p>Returns: LLM configuration (API key excluded). Falls back to <code>default</code> if not found.</p>"},{"location":"api/core/llm/#llm_apilist_llms","title":"LLM_API.list_llms","text":"<p>Signature: <code>list_llms() -&gt; dict</code></p> <p>Returns: Map of LLM name \u2192 config (API key excluded).</p>"},{"location":"api/core/llm/#behavioral-contracts","title":"Behavioral Contracts","text":"<ul> <li>Only provider supported in core registry is <code>openai</code> (enforced by <code>LLMRegistry</code>).</li> <li><code>LLMRegistry.get_llm()</code> falls back to <code>default</code> if name is missing.</li> <li>Determinism: OpenAI LLM is initialized with <code>seed=42</code>.</li> </ul>"},{"location":"api/core/public-facade/","title":"Public Facade API","text":""},{"location":"api/core/public-facade/#purpose","title":"Purpose","text":"<p>Provide a stable, consolidated entrypoint that wires <code>NL2SQLContext</code> and exposes modular APIs through a single class.</p>"},{"location":"api/core/public-facade/#responsibilities","title":"Responsibilities","text":"<ul> <li>Initialize registries and stores in a consistent order.</li> <li>Expose convenience methods that delegate to modular APIs.</li> <li>Provide a single object for application integration.</li> </ul>"},{"location":"api/core/public-facade/#key-modules","title":"Key Modules","text":"<ul> <li><code>packages/core/src/nl2sql/public_api.py</code></li> <li><code>packages/core/src/nl2sql/context.py</code></li> </ul>"},{"location":"api/core/public-facade/#public-surface","title":"Public Surface","text":""},{"location":"api/core/public-facade/#nl2sqlinit","title":"NL2SQL.init","text":"<p>Source: <code>packages/core/src/nl2sql/public_api.py</code></p> <p>Signature: <code>NL2SQL(ds_config_path: Optional[Union[str, pathlib.Path]] = None, secrets_config_path: Optional[Union[str, pathlib.Path]] = None, llm_config_path: Optional[Union[str, pathlib.Path]] = None, vector_store_path: Optional[Union[str, pathlib.Path]] = None, policies_config_path: Optional[Union[str, pathlib.Path]] = None)</code></p> <p>Parameters: | name | type | required | meaning | | --- | --- | --- | --- | | <code>ds_config_path</code> | <code>Optional[Union[str, pathlib.Path]]</code> | no | Datasource config path override. | | <code>secrets_config_path</code> | <code>Optional[Union[str, pathlib.Path]]</code> | no | Secrets config path override. | | <code>llm_config_path</code> | <code>Optional[Union[str, pathlib.Path]]</code> | no | LLM config path override. | | <code>vector_store_path</code> | <code>Optional[Union[str, pathlib.Path]]</code> | no | Vector store persistence path override. | | <code>policies_config_path</code> | <code>Optional[Union[str, pathlib.Path]]</code> | no | Policies config path override. |</p> <p>Returns: <code>NL2SQL</code> instance with initialized context and API modules.</p> <p>Raises: - <code>FileNotFoundError</code>, <code>ValueError</code> from config loading (see <code>ConfigManager</code>). - Provider-specific errors from secrets/LLM initialization.</p> <p>Side Effects: - Loads configs, resolves secrets, builds registries and stores.</p> <p>Idempotency: - Initialization is not idempotent; it constructs new registries and stores.</p>"},{"location":"api/core/public-facade/#convenience-methods","title":"Convenience methods","text":"<p>The public facade delegates to modular APIs with the same signatures: <code>run_query</code>, <code>add_datasource</code>, <code>add_datasource_from_config</code>, <code>list_datasources</code>, <code>get_datasource_capabilities</code>, <code>configure_llm</code>, <code>configure_llm_from_config</code>, <code>list_llms</code>, <code>get_llm</code>, <code>index_datasource</code>, <code>index_all_datasources</code>, <code>clear_index</code>, <code>check_permissions</code>, <code>get_allowed_resources</code>, <code>get_current_settings</code>, <code>get_setting</code>, <code>validate_configuration</code>.</p>"},{"location":"api/core/query/","title":"Query API","text":""},{"location":"api/core/query/#purpose","title":"Purpose","text":"<p>Execute a natural language query using the NL2SQL pipeline.</p>"},{"location":"api/core/query/#responsibilities","title":"Responsibilities","text":"<ul> <li>Invoke pipeline graph runtime.</li> <li>Return a structured <code>QueryResult</code>.</li> </ul>"},{"location":"api/core/query/#key-modules","title":"Key Modules","text":"<ul> <li><code>packages/core/src/nl2sql/api/query_api.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/runtime.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/graph.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/state.py</code></li> </ul>"},{"location":"api/core/query/#public-surface","title":"Public Surface","text":""},{"location":"api/core/query/#queryapirun_query","title":"QueryAPI.run_query","text":"<p>Source: <code>packages/core/src/nl2sql/api/query_api.py</code></p> <p>Signature: <code>run_query(natural_language: str, datasource_id: Optional[str] = None, execute: bool = True, user_context: Optional[UserContext] = None) -&gt; QueryResult</code></p> <p>Parameters: | name | type | required | meaning | | --- | --- | --- | --- | | <code>natural_language</code> | <code>str</code> | yes | User query. | | <code>datasource_id</code> | <code>Optional[str]</code> | no | Datasource override; otherwise resolved. | | <code>execute</code> | <code>bool</code> | no | Whether to execute SQL against datasource. | | <code>user_context</code> | <code>Optional[UserContext]</code> | no | RBAC context. |</p> <p>Returns: <code>QueryResult</code> with fields: <code>sql</code>, <code>results</code>, <code>final_answer</code>, <code>errors</code>, <code>warnings</code>, <code>trace_id</code>, <code>reasoning</code>.</p> <p>Raises: No exceptions are raised by <code>run_query</code>; errors are returned in <code>QueryResult.errors</code>.</p> <p>Side Effects: - Pipeline execution, LLM calls, optional database execution.</p> <p>Idempotency: - Not guaranteed; execution can depend on external systems and time.</p>"},{"location":"api/core/query/#execution-lifecycle","title":"Execution Lifecycle","text":"<ul> <li>Initialize cancellation and signal handlers.</li> <li>Build LangGraph pipeline from <code>build_graph</code>.</li> <li>Execute graph in thread pool with timeout.</li> <li>On timeout/cancel, return <code>PipelineError</code> with appropriate <code>ErrorCode</code>.</li> </ul>"},{"location":"api/core/query/#queryresult","title":"QueryResult","text":"<p>Source: <code>packages/core/src/nl2sql/api/query_api.py</code></p> <p>Fields: | name | type | required | meaning | | --- | --- | --- | --- | | <code>sql</code> | <code>Optional[str]</code> | no | Generated SQL (if any). | | <code>results</code> | <code>list</code> | no | Result rows (adapter-dependent). | | <code>final_answer</code> | <code>Optional[str]</code> | no | Natural language answer. | | <code>errors</code> | <code>list</code> | no | Pipeline errors. | | <code>trace_id</code> | <code>Optional[str]</code> | no | Trace identifier. | | <code>reasoning</code> | <code>List[dict]</code> | no | Reasoning events/logs. | | <code>warnings</code> | <code>List[dict]</code> | no | Warning events/logs. |</p>"},{"location":"api/core/result/","title":"Result API","text":""},{"location":"api/core/result/#purpose","title":"Purpose","text":"<p>Placeholder API for result management and storage.</p>"},{"location":"api/core/result/#responsibilities","title":"Responsibilities","text":"<ul> <li>None defined in current code; exists as a stub to anchor future result APIs.</li> </ul>"},{"location":"api/core/result/#key-modules","title":"Key Modules","text":"<ul> <li><code>packages/core/src/nl2sql/api/result_api.py</code></li> <li><code>packages/core/src/nl2sql/execution/contracts.py</code></li> <li><code>packages/core/src/nl2sql/execution/execution_store.py</code></li> <li><code>packages/core/src/nl2sql/execution/artifacts/</code></li> </ul>"},{"location":"api/core/result/#public-surface","title":"Public Surface","text":"<p>No public methods are defined beyond initialization.</p>"},{"location":"api/core/settings/","title":"Settings API (System)","text":""},{"location":"api/core/settings/#purpose","title":"Purpose","text":"<p>Expose runtime settings and configuration validation.</p>"},{"location":"api/core/settings/#responsibilities","title":"Responsibilities","text":"<ul> <li>Return current settings.</li> <li>Fetch specific setting keys.</li> <li>Validate configuration integrity.</li> </ul>"},{"location":"api/core/settings/#key-modules","title":"Key Modules","text":"<ul> <li><code>packages/core/src/nl2sql/api/settings_api.py</code></li> <li><code>packages/core/src/nl2sql/common/settings.py</code></li> </ul>"},{"location":"api/core/settings/#public-surface","title":"Public Surface","text":""},{"location":"api/core/settings/#settingsapiget_current_settings","title":"SettingsAPI.get_current_settings","text":"<p>Signature: <code>get_current_settings() -&gt; Dict[str, Any]</code></p> <p>Returns: Settings as a dictionary (<code>Settings.model_dump()</code>).</p>"},{"location":"api/core/settings/#settingsapiget_setting","title":"SettingsAPI.get_setting","text":"<p>Signature: <code>get_setting(key: str) -&gt; Any</code></p> <p>Returns: Value for the given key or <code>None</code>.</p>"},{"location":"api/core/settings/#settingsapivalidate_configuration","title":"SettingsAPI.validate_configuration","text":"<p>Signature: <code>validate_configuration() -&gt; bool</code></p> <p>Returns: <code>True</code> if settings can be loaded; <code>False</code> otherwise.</p>"},{"location":"api/rest/","title":"REST API (FastAPI)","text":"<p>This section documents the <code>nl2sql-api</code> FastAPI package that exposes the core engine over HTTP. Implementation lives in <code>packages/api/src/nl2sql_api/</code>.</p>"},{"location":"api/rest/#runtime-model","title":"Runtime Model","text":""},{"location":"api/rest/#application-lifecycle","title":"Application lifecycle","text":"<p>Source: <code>packages/api/src/nl2sql_api/main.py</code></p> <ul> <li><code>lifespan</code> initializes a single <code>NL2SQL</code> engine and stores it in <code>app.state.engine</code>.</li> <li>Routers are included under <code>/api/v1</code>.</li> <li>CORS is enabled for all origins (intended to be tightened in production).</li> </ul>"},{"location":"api/rest/#dependency-injection","title":"Dependency injection","text":"<p>Source: <code>packages/api/src/nl2sql_api/dependencies.py</code></p> <ul> <li><code>get_engine(request)</code> returns the singleton <code>NL2SQL</code> instance.</li> <li>Service providers (<code>DatasourceService</code>, <code>QueryService</code>, <code>LLMService</code>,   <code>IndexingService</code>, <code>HealthService</code>) are created per-request with the engine.</li> </ul>"},{"location":"api/rest/#api-index","title":"API Index","text":"API Router Path Purpose Health <code>routes/health.py</code> Liveness/readiness checks. Query <code>routes/query.py</code> Execute and fetch query results. Datasource <code>routes/datasource.py</code> Manage datasource configs. LLM <code>routes/llm.py</code> Configure and inspect LLMs. Indexing <code>routes/indexing.py</code> Index management and status."},{"location":"api/rest/#error-handling","title":"Error Handling","text":"<p>FastAPI routers wrap most failures in <code>HTTPException(status_code=500, detail=str(e))</code>. Datasource and delete endpoints map <code>ValueError</code> to <code>HTTP 404</code>.</p>"},{"location":"api/rest/#configuration","title":"Configuration","text":"<p>The REST API inherits all configuration from the Core engine. It does not add additional environment variables beyond the runtime server options in <code>nl2sql_api.server</code> (<code>--host</code>, <code>--port</code>, <code>--reload</code>).</p>"},{"location":"api/rest/datasource/","title":"Datasource API","text":""},{"location":"api/rest/datasource/#requestresponse-models","title":"Request/Response Models","text":"<p>Source: <code>packages/api/src/nl2sql_api/models/datasource.py</code></p>"},{"location":"api/rest/datasource/#datasourcerequest","title":"<code>DatasourceRequest</code>","text":"field type required meaning <code>config</code> <code>Dict[str, Any]</code> yes Datasource config payload."},{"location":"api/rest/datasource/#datasourceresponse","title":"<code>DatasourceResponse</code>","text":"field type required meaning <code>success</code> <code>bool</code> yes Operation status. <code>message</code> <code>str</code> yes Result message. <code>datasource_id</code> <code>Optional[str]</code> no Datasource ID."},{"location":"api/rest/datasource/#endpoints","title":"Endpoints","text":""},{"location":"api/rest/datasource/#post-apiv1datasource","title":"<code>POST /api/v1/datasource</code>","text":"<p>Source: <code>packages/api/src/nl2sql_api/routes/datasource.py</code></p> <p>Request model: <code>DatasourceRequest</code></p> <p>Response model: <code>DatasourceResponse</code></p> <p>Execution flow: - Delegates to <code>engine.add_datasource(...)</code>.</p> <p>Errors: - Unhandled exceptions return <code>HTTP 500</code>.</p>"},{"location":"api/rest/datasource/#get-apiv1datasource","title":"<code>GET /api/v1/datasource</code>","text":"<p>Response model: <code>Dict[str, Any]</code> (with <code>datasources</code> list)</p> <p>Execution flow: - Delegates to <code>engine.list_datasources()</code>.</p>"},{"location":"api/rest/datasource/#get-apiv1datasourcedatasource_id","title":"<code>GET /api/v1/datasource/{datasource_id}</code>","text":"<p>Response model: <code>Dict[str, Any]</code></p> <p>Execution flow: - Validates datasource ID exists. - Returns placeholder payload <code>{datasource_id, exists: true}</code>.</p> <p>Errors: - <code>ValueError</code> -&gt; <code>HTTP 404</code>. - Other exceptions -&gt; <code>HTTP 500</code>.</p>"},{"location":"api/rest/datasource/#delete-apiv1datasourcedatasource_id","title":"<code>DELETE /api/v1/datasource/{datasource_id}</code>","text":"<p>Response model: <code>Dict[str, Any]</code></p> <p>Execution flow: - Returns a fixed payload indicating removal is not supported.</p> <p>Errors: - <code>ValueError</code> -&gt; <code>HTTP 404</code>. - Other exceptions -&gt; <code>HTTP 500</code>.</p>"},{"location":"api/rest/health/","title":"Health API","text":""},{"location":"api/rest/health/#endpoints","title":"Endpoints","text":""},{"location":"api/rest/health/#get-apiv1health","title":"<code>GET /api/v1/health</code>","text":"<p>Source: <code>packages/api/src/nl2sql_api/routes/health.py</code></p> <p>Response model: <code>SuccessResponse</code></p> <p>Returns: <code>{\"success\": true, \"message\": \"NL2SQL API is running\"}</code></p>"},{"location":"api/rest/health/#get-apiv1ready","title":"<code>GET /api/v1/ready</code>","text":"<p>Source: <code>packages/api/src/nl2sql_api/routes/health.py</code></p> <p>Response model: <code>SuccessResponse</code></p> <p>Returns: <code>{\"success\": true, \"message\": \"NL2SQL API is ready\"}</code></p>"},{"location":"api/rest/indexing/","title":"Indexing API","text":""},{"location":"api/rest/indexing/#endpoints","title":"Endpoints","text":""},{"location":"api/rest/indexing/#post-apiv1indexdatasource_id","title":"<code>POST /api/v1/index/{datasource_id}</code>","text":"<p>Execution flow: - Delegates to <code>engine.indexing.index_datasource(...)</code>.</p> <p>Response: <code>{\"success\": true, \"datasource_id\": \"...\", \"indexing_stats\": {...}, \"message\": \"...\"}</code></p>"},{"location":"api/rest/indexing/#post-apiv1index-all","title":"<code>POST /api/v1/index-all</code>","text":"<p>Execution flow: - Delegates to <code>engine.indexing.index_all_datasources(...)</code>.</p>"},{"location":"api/rest/indexing/#delete-apiv1index","title":"<code>DELETE /api/v1/index</code>","text":"<p>Execution flow: - Delegates to <code>engine.indexing.clear_index()</code>.</p>"},{"location":"api/rest/indexing/#get-apiv1indexstatus","title":"<code>GET /api/v1/index/status</code>","text":"<p>Execution flow: - Returns placeholder status from <code>IndexingService.get_index_status(...)</code>.</p>"},{"location":"api/rest/llm/","title":"LLM API","text":""},{"location":"api/rest/llm/#requestresponse-models","title":"Request/Response Models","text":"<p>Source: <code>packages/api/src/nl2sql_api/models/llm.py</code></p>"},{"location":"api/rest/llm/#llmrequest","title":"<code>LLMRequest</code>","text":"field type required meaning <code>config</code> <code>Dict[str, Any]</code> yes LLM config payload."},{"location":"api/rest/llm/#llmresponse","title":"<code>LLMResponse</code>","text":"field type required meaning <code>success</code> <code>bool</code> yes Operation status. <code>message</code> <code>str</code> yes Result message. <code>llm_name</code> <code>Optional[str]</code> no LLM name."},{"location":"api/rest/llm/#endpoints","title":"Endpoints","text":""},{"location":"api/rest/llm/#post-apiv1llm","title":"<code>POST /api/v1/llm</code>","text":"<p>Execution flow: - Delegates to <code>engine.configure_llm(...)</code>.</p> <p>Errors: - Unhandled exceptions return <code>HTTP 500</code>.</p>"},{"location":"api/rest/llm/#get-apiv1llm","title":"<code>GET /api/v1/llm</code>","text":"<p>Response model: <code>Dict[str, Any]</code> (with <code>llms</code> map)</p> <p>Execution flow: - Delegates to <code>engine.list_llms()</code>.</p>"},{"location":"api/rest/llm/#get-apiv1llmllm_name","title":"<code>GET /api/v1/llm/{llm_name}</code>","text":"<p>Response model: <code>Dict[str, Any]</code></p> <p>Execution flow: - Delegates to <code>engine.get_llm(llm_name)</code>.</p>"},{"location":"api/rest/query/","title":"Query API","text":""},{"location":"api/rest/query/#requestresponse-models","title":"Request/Response Models","text":"<p>Source: <code>packages/api/src/nl2sql_api/models/query.py</code></p>"},{"location":"api/rest/query/#queryrequest","title":"<code>QueryRequest</code>","text":"field type required meaning <code>natural_language</code> <code>str</code> yes User query. <code>datasource_id</code> <code>Optional[str]</code> no Datasource override. <code>execute</code> <code>bool</code> no Execute SQL against datasource (default <code>true</code>). <code>user_context</code> <code>Optional[Dict[str, Any]]</code> no RBAC context payload."},{"location":"api/rest/query/#queryresponse","title":"<code>QueryResponse</code>","text":"field type required meaning <code>sql</code> <code>Optional[str]</code> no Generated SQL. <code>results</code> <code>list</code> no Query results. <code>final_answer</code> <code>Optional[str]</code> no Natural language answer. <code>errors</code> <code>list</code> no Pipeline errors. <code>trace_id</code> <code>Optional[str]</code> no Trace identifier. <code>reasoning</code> <code>List[Dict[str, Any]]</code> no Reasoning events/logs. <code>warnings</code> <code>List[Dict[str, Any]]</code> no Warning events/logs."},{"location":"api/rest/query/#endpoints","title":"Endpoints","text":""},{"location":"api/rest/query/#post-apiv1query","title":"<code>POST /api/v1/query</code>","text":"<p>Source: <code>packages/api/src/nl2sql_api/routes/query.py</code></p> <p>Request model: <code>QueryRequest</code></p> <p>Response model: <code>QueryResponse</code></p> <p>Execution flow: - Converts <code>user_context</code> to <code>UserContext</code> when present. - Delegates to <code>engine.run_query(...)</code>. - Maps the <code>QueryResult</code> into <code>QueryResponse</code>.</p> <p>Errors: - Unhandled exceptions return <code>HTTP 500</code> with <code>detail=str(e)</code>.</p>"},{"location":"api/rest/query/#get-apiv1querytrace_id","title":"<code>GET /api/v1/query/{trace_id}</code>","text":"<p>Source: <code>packages/api/src/nl2sql_api/routes/query.py</code></p> <p>Response model: <code>QueryResponse</code></p> <p>Execution flow: - Delegates to <code>QueryService.get_result(...)</code>.</p> <p>Notes: - <code>QueryService.get_result</code> is not implemented in current codebase. Calls will fail   unless implemented.</p>"},{"location":"architecture/determinism/","title":"Determinism Architecture","text":""},{"location":"architecture/determinism/#overview","title":"Overview","text":"<ul> <li>Determinism matters because the pipeline composes multi-step planning, execution, and aggregation; stable identifiers and ordering are required for reproducible DAGs, consistent merges, and auditability.</li> <li>The system implements determinism in specific places (hashing, sorting, and schema fingerprinting) but also contains explicit nondeterminism (LLM outputs, vector retrieval ranking, timestamps, random retry jitter, and external calls).</li> <li>Scope: planning (IDs + DAG), retrieval (vector ordering), execution ordering (DAG layers), artifacts (content hashing), and state merges.</li> </ul>"},{"location":"architecture/determinism/#determinism-domains","title":"Determinism Domains","text":""},{"location":"architecture/determinism/#inputs-and-identifier-stability","title":"Inputs and Identifier Stability","text":"<ul> <li>Deterministic: Sub-query and post-combine op IDs are derived from sorted JSON payloads with SHA-256 (<code>pipeline/nodes/decomposer/node.py</code>).</li> <li>Non-deterministic: <code>GraphState.trace_id</code> defaults to <code>uuid.uuid4()</code>; subgraph IDs incorporate this value (<code>pipeline/state.py</code>, <code>pipeline/graph_utils.py</code>), so identical inputs produce different IDs unless a trace ID is supplied.</li> <li>Non-deterministic: User query interpretation depends on LLMs in the decomposer, planner, and refiner nodes, with no local temperature/seed control visible in these nodes (<code>pipeline/nodes/decomposer/node.py</code>, <code>pipeline/nodes/ast_planner/node.py</code>, <code>pipeline/nodes/refiner/node.py</code>).</li> </ul>"},{"location":"architecture/determinism/#planner-and-dag-construction","title":"Planner and DAG Construction","text":"<ul> <li>Deterministic: Global planner sorts nodes and edges by IDs and roles before constructing the DAG, then hashes a sorted JSON payload to produce a stable <code>dag_id</code> for a given logical plan (<code>pipeline/nodes/global_planner/node.py</code>).</li> <li>Deterministic: DAG layers are computed with a topological sort that sorts ready nodes and dependents, yielding stable layer ordering given the same node/edge sets (<code>pipeline/nodes/global_planner/schemas.py</code>).</li> <li>Non-deterministic: The AST planner is LLM-driven; the PlanModel content is not stabilized inside the node (<code>pipeline/nodes/ast_planner/node.py</code>).</li> </ul>"},{"location":"architecture/determinism/#retrieval-ordering-and-chunk-selection","title":"Retrieval Ordering and Chunk Selection","text":"<ul> <li>Non-deterministic: Vector retrieval uses max marginal relevance search; ranking and ties depend on vector store behavior and similarity scores, with no secondary deterministic tie-breaker in code (<code>indexing/vector_store.py</code>).</li> <li>Non-deterministic: Datasource resolution uses vector retrieval results directly; candidate ordering follows the retrieval output and is not re-sorted (<code>pipeline/nodes/datasource_resolver/node.py</code>).</li> <li>Non-deterministic: Schema retrieval builds the table/column set from vector retrieval metadata and uses dictionary/set iteration without sorting; final <code>relevant_tables</code> ordering follows input ordering (<code>pipeline/nodes/schema_retriever/node.py</code>).</li> <li>Partially deterministic: Table chunk column names are sorted for the table chunk payload, but chunk emission order depends on the schema contract\u2019s insertion order (no sorting over <code>tables.items()</code> or <code>columns.items()</code> for column chunks) (<code>indexing/chunk_builder.py</code>).</li> </ul>"},{"location":"architecture/determinism/#schema-authority-and-versioning","title":"Schema Authority and Versioning","text":"<ul> <li>Deterministic: Schema fingerprints are computed from sorted tables, columns, and foreign keys with JSON serialization; same schema contract yields the same fingerprint (<code>schema/protocol.py</code>).</li> <li>Deterministic: Both in-memory and SQLite schema stores deduplicate by fingerprint, returning an existing version if one matches (<code>schema/in_memory_store.py</code>, <code>schema/sqlite_store.py</code>).</li> <li>Non-deterministic: Schema versions are time-based (<code>YYYYMMDDhhmmss_&lt;fingerprint&gt;</code>). Latest version selection depends on registration time and order (<code>schema/in_memory_store.py</code>, <code>schema/sqlite_store.py</code>).</li> <li>Deterministic (conditional): If <code>schema_version</code> is provided by a sub-query, schema retrieval resolves that exact snapshot; otherwise it uses the latest available version, which is time-ordered (<code>pipeline/nodes/schema_retriever/node.py</code>).</li> </ul>"},{"location":"architecture/determinism/#validation-gates","title":"Validation Gates","text":"<ul> <li>Deterministic: Logical validation normalizes names, enforces ordinal continuity, validates aliases, and uses sorted comparisons for expected schema/alias matching (<code>pipeline/nodes/validator/node.py</code>).</li> <li>Deterministic: Policy enforcement uses explicit namespaced checks and deterministic set membership (<code>pipeline/nodes/validator/node.py</code>).</li> </ul>"},{"location":"architecture/determinism/#dag-execution-order-and-node-sequencing","title":"DAG Execution Order and Node Sequencing","text":"<ul> <li>Deterministic: Aggregation executes layer-by-layer in DAG order, with ordered inputs computed by role rank and node ID, and terminal nodes sorted by ID (<code>aggregation/aggregator.py</code>).</li> <li>Deterministic: Router selects the next scan layer based on the DAG layers and existing artifact refs (<code>pipeline/graph_utils.py</code>, <code>pipeline/routes.py</code>).</li> </ul>"},{"location":"architecture/determinism/#subgraph-composition-and-routing","title":"Subgraph Composition and Routing","text":"<ul> <li>Partially deterministic: Subgraph selection chooses the first matching spec in dictionary iteration order; if multiple subgraphs satisfy capabilities, selection depends on insertion order of <code>subgraph_specs</code> (<code>pipeline/graph_utils.py</code>).</li> <li>Deterministic (conditional): Scan payloads and subgraph outputs are built from explicit state fields; content is deterministic given the input state (<code>pipeline/graph_utils.py</code>).</li> </ul>"},{"location":"architecture/determinism/#retry-mechanisms-and-backoff","title":"Retry Mechanisms and Backoff","text":"<ul> <li>Non-deterministic: Retry backoff includes random jitter; sleep time is a function of random.uniform and wall-clock timing (<code>pipeline/subgraphs/sql_agent.py</code>).</li> <li>Deterministic (conditional): Retry routing decisions depend on current retry count and error retryability flags; deterministic if state is unchanged (<code>pipeline/subgraphs/sql_agent.py</code>).</li> </ul>"},{"location":"architecture/determinism/#artifact-storage-and-hashing","title":"Artifact Storage and Hashing","text":"<ul> <li>Deterministic: Local artifact content hashes are computed from a sorted JSON payload (columns, row_count, path), giving stable hashes for the same payload (<code>execution/artifacts/local_store.py</code>).</li> <li>Non-deterministic: Artifact refs include <code>created_at=datetime.utcnow()</code>; this is time-based and changes for each creation (<code>execution/artifacts/base.py</code>).</li> <li>Deterministic (conditional): Upload paths are deterministic given <code>tenant_id</code> and <code>request_id</code>, but the request ID origin is external to this module (<code>execution/artifacts/local_store.py</code>).</li> </ul>"},{"location":"architecture/determinism/#state-mutation-and-merge-semantics","title":"State Mutation and Merge Semantics","text":"<ul> <li>Deterministic: Decomposer returns sorted sub-queries, combine groups, and post-combine ops by ID to stabilize downstream ordering (<code>pipeline/nodes/decomposer/node.py</code>).</li> <li>Potentially non-deterministic: GraphState merges dict fields with a last-write-wins reducer and concatenates lists; in parallel branches, merge order is not constrained in this code (<code>pipeline/state.py</code>).</li> </ul>"},{"location":"architecture/determinism/#hashingfingerprinting","title":"Hashing/Fingerprinting","text":"<ul> <li>Deterministic: Stable hashing is consistently performed with sorted JSON and fixed separators for sub-query IDs, DAG hashes, schema fingerprints, and artifact content hashes (<code>pipeline/nodes/decomposer/node.py</code>, <code>pipeline/nodes/global_planner/node.py</code>, <code>schema/protocol.py</code>, <code>execution/artifacts/local_store.py</code>).</li> </ul>"},{"location":"architecture/determinism/#time-based-logic-and-runtime-controls","title":"Time-Based Logic and Runtime Controls","text":"<ul> <li>Non-deterministic: Schema versions and artifact creation timestamps are derived from wall-clock time (<code>schema/in_memory_store.py</code>, <code>schema/sqlite_store.py</code>, <code>execution/artifacts/base.py</code>).</li> <li>Deterministic (conditional): Pipeline timeout handling is based on monotonic time; timeouts and cancellations depend on wall-clock progression and runtime scheduling (<code>pipeline/runtime.py</code>).</li> </ul>"},{"location":"architecture/determinism/#external-calls","title":"External Calls","text":"<ul> <li>Non-deterministic: LLM-driven nodes (decomposer, planner, refiner) invoke external LLMs without deterministic configuration in these nodes (<code>pipeline/nodes/decomposer/node.py</code>, <code>pipeline/nodes/ast_planner/node.py</code>, <code>pipeline/nodes/refiner/node.py</code>).</li> <li>Non-deterministic: Vector store retrieval depends on external embeddings and search behavior (<code>indexing/vector_store.py</code>).</li> <li>Non-deterministic: SQL execution delegates to external executors and underlying databases, which can return different results across time or state (<code>pipeline/nodes/executor/node.py</code>).</li> </ul>"},{"location":"architecture/failure_recovery/","title":"Failure + Recovery Architecture","text":""},{"location":"architecture/failure_recovery/#overview","title":"Overview","text":"<p>Failure in this system is represented as structured <code>PipelineError</code> objects accumulated in graph state, explicit exceptions that short-circuit a node/subgraph, or runtime timeouts/cancellations handled by the orchestrator. Most node failures are captured and returned in state rather than raising, so the graph can continue unless routing logic or wrappers explicitly stop or crash. Recovery is limited to a local retry loop inside the SQL agent subgraph; there is no graph-level replay or global retry.</p>"},{"location":"architecture/failure_recovery/#failure-domains","title":"Failure Domains","text":""},{"location":"architecture/failure_recovery/#input","title":"Input","text":"<ul> <li>Missing or invalid configuration raises during context construction (e.g., missing vector store path or collection name), preventing pipeline startup.</li> <li>Explicit datasource overrides fail if the datasource is unknown or not allowed, returning <code>SECURITY_VIOLATION</code> or <code>INVALID_STATE</code>.</li> <li>Missing LLMs for specific nodes (e.g., refiner) return <code>MISSING_LLM</code> errors.</li> </ul>"},{"location":"architecture/failure_recovery/#retrieval","title":"Retrieval","text":"<ul> <li>Vector store calls are wrapped by <code>VECTOR_BREAKER</code>; breaker open or retrieval errors propagate into resolver or schema retriever.</li> <li>Resolver returns <code>SCHEMA_RETRIEVAL_FAILED</code> if no candidate datasources are found.</li> <li>Schema retriever falls back to full schema snapshot if vector retrieval yields no tables; if schema store returns <code>None</code>, it silently returns an empty table list.</li> </ul>"},{"location":"architecture/failure_recovery/#planning","title":"Planning","text":"<ul> <li>Decomposer LLM failures return <code>ORCHESTRATOR_CRASH</code> (critical) and empty responses.</li> <li>AST planner LLM failures return <code>PLANNING_FAILURE</code> and a <code>None</code> plan.</li> <li>Global planner failures return <code>PLANNER_FAILED</code> and <code>execution_dag=None</code>.</li> </ul>"},{"location":"architecture/failure_recovery/#validation","title":"Validation","text":"<ul> <li>Logical validation returns structured errors for missing tables, columns, invalid plan structure, or security violations.</li> <li>Physical validation is implemented but not wired into the SQL agent subgraph flow and does not execute in the current graph.</li> </ul>"},{"location":"architecture/failure_recovery/#execution","title":"Execution","text":"<ul> <li>SQL generation failures return <code>SQL_GEN_FAILED</code>.</li> <li>Executor returns errors for missing SQL, missing datasource, missing executor, or executor crashes.</li> <li>SQL execution failures return <code>EXECUTION_FAILED</code> based on adapter result.</li> <li>Sandbox execution errors (timeouts, worker crashes) are surfaced only when the physical validator uses the sandbox.</li> </ul>"},{"location":"architecture/failure_recovery/#storage","title":"Storage","text":"<ul> <li>Schema store uses SQLite or in-memory storage; failures manifest as exceptions on read/write (not caught in callers).</li> <li>Artifact store is invoked by the SQL executor; aggregation loads artifacts via <code>AggregationEngine.load_scan()</code> using <code>ArtifactRef</code> URIs. Local, S3, and ADLS backends exist in this repository (see <code>../storage/artifact-store.md</code> for implementation details and limitations).</li> <li>Result store and execution store are in-memory only and not used for recovery.</li> </ul>"},{"location":"architecture/failure_recovery/#node-level-failures","title":"Node-Level Failures","text":""},{"location":"architecture/failure_recovery/#how-nodes-fail","title":"How nodes fail","text":"<ul> <li>Most nodes use try/except and return a <code>PipelineError</code> (with severity + error code) in their response.</li> <li>Some routing logic raises <code>PipelineError</code> directly (e.g., no compatible subgraph found).</li> <li>Nodes often return partial state plus errors (e.g., resolver returns a response plus errors).</li> </ul>"},{"location":"architecture/failure_recovery/#propagation","title":"Propagation","text":"<ul> <li><code>GraphState.errors</code> and <code>SubgraphExecutionState.errors</code> accumulate errors via list reducers.</li> <li>Downstream nodes generally do not halt unless routing logic explicitly stops (SQL agent checks) or a wrapper crashes.</li> </ul>"},{"location":"architecture/failure_recovery/#local-handling","title":"Local handling","text":"<ul> <li>SQL agent uses retry routing based on <code>PipelineError.is_retryable</code>.</li> <li>Other nodes do not retry; they return errors and allow the graph to proceed unless missing required inputs causes downstream failures.</li> </ul>"},{"location":"architecture/failure_recovery/#subgraph-failures","title":"Subgraph Failures","text":""},{"location":"architecture/failure_recovery/#containment","title":"Containment","text":"<ul> <li>Each subgraph runs with its own <code>SubgraphExecutionState</code>. Errors inside the subgraph are merged back into the main graph state.</li> </ul>"},{"location":"architecture/failure_recovery/#abort-vs-continue","title":"Abort vs continue","text":"<ul> <li>SQL agent subgraph routes to <code>END</code> when:</li> <li>The planner fails to produce a plan and errors are non-retryable.</li> <li>Logical validation returns non-retryable errors.</li> <li>Retry count reaches <code>sql_agent_max_retries</code>.</li> <li>Cancellation is detected.</li> <li>If errors are retryable and retry budget remains, the subgraph loops through <code>retry_handler -&gt; refiner -&gt; planner</code>.</li> </ul>"},{"location":"architecture/failure_recovery/#partial-recovery","title":"Partial recovery","text":"<ul> <li>Recovery is limited to re-planning and refining within the SQL agent. There is no partial recovery at the aggregator or graph layer.</li> <li>Subgraph wrapper assumes an <code>executor_response</code> is present; if executor output is missing (e.g., early failure), wrapper-level failures are possible.</li> </ul>"},{"location":"architecture/failure_recovery/#graph-level-failures","title":"Graph-Level Failures","text":""},{"location":"architecture/failure_recovery/#request-termination","title":"Request termination","text":"<ul> <li><code>run_with_graph</code> terminates early on cancellation (<code>CANCELLED</code>) or global timeout (<code>PIPELINE_TIMEOUT</code>).</li> <li>Unhandled exceptions in graph execution return <code>UNKNOWN_ERROR</code> with stack trace.</li> </ul>"},{"location":"architecture/failure_recovery/#cleanup","title":"Cleanup","text":"<ul> <li>Signal handlers are restored after execution. There is no explicit cleanup of artifacts or partial state.</li> </ul>"},{"location":"architecture/failure_recovery/#user-facing-errors","title":"User-facing errors","text":"<ul> <li>Errors are returned in the final state. <code>PipelineRunner</code> returns <code>success=True</code> on graph completion regardless of errors in state.</li> </ul>"},{"location":"architecture/failure_recovery/#retry-architecture","title":"Retry Architecture","text":""},{"location":"architecture/failure_recovery/#retry-scope","title":"Retry scope","text":"<ul> <li>Only the SQL agent subgraph retries (planner and validation loop).</li> <li>Other nodes (resolver, decomposer, global planner, generator, executor, aggregator, answer synthesizer) do not retry.</li> </ul>"},{"location":"architecture/failure_recovery/#backoff","title":"Backoff","text":"<ul> <li>Exponential backoff with jitter in <code>retry_handler</code> using:</li> <li><code>SQL_AGENT_RETRY_BASE_DELAY_SEC</code></li> <li><code>SQL_AGENT_RETRY_MAX_DELAY_SEC</code></li> <li><code>SQL_AGENT_RETRY_JITTER_SEC</code></li> </ul>"},{"location":"architecture/failure_recovery/#idempotency","title":"Idempotency","text":"<ul> <li>Subquery IDs are stable hashes, so retries regenerate the same subquery IDs.</li> <li>Executor writes artifacts keyed by subquery IDs; overwrite semantics depend on the artifact store implementation (not present in this repo).</li> </ul>"},{"location":"architecture/failure_recovery/#artifact-consistency","title":"Artifact Consistency","text":""},{"location":"architecture/failure_recovery/#partial-writes","title":"Partial writes","text":"<ul> <li>Executor returns an <code>ArtifactRef</code> only on successful adapter execution.</li> <li>Aggregator expects an artifact for each scan node; missing artifacts raise and are surfaced as <code>AGGREGATOR_FAILED</code>.</li> </ul>"},{"location":"architecture/failure_recovery/#overwrites","title":"Overwrites","text":"<ul> <li>For S3/ADLS backends, <code>RESULT_ARTIFACT_PATH_TEMPLATE</code> is intended to render paths with <code>tenant_id</code>, <code>request_id</code>, <code>subgraph_name</code>, <code>dag_node_id</code>, and <code>schema_version</code>, but path rendering is not implemented in this repo (see <code>../storage/artifact-store.md</code>).</li> <li>For the local backend, paths are <code>&lt;result_artifact_base_uri&gt;/&lt;tenant_id&gt;/&lt;request_id&gt;.parquet</code>, so repeat execution with the same trace ID targets the same file.</li> </ul>"},{"location":"architecture/failure_recovery/#cleanup_1","title":"Cleanup","text":"<ul> <li>No cleanup or rollback is implemented for artifacts or partial aggregation results.</li> </ul>"},{"location":"architecture/failure_recovery/#recovery-paths","title":"Recovery Paths","text":""},{"location":"architecture/failure_recovery/#what-can-be-retried","title":"What can be retried","text":"<ul> <li>SQL agent planner/validation loop for retryable errors (non-fatal error codes and non-critical severity).</li> </ul>"},{"location":"architecture/failure_recovery/#what-must-restart","title":"What must restart","text":"<ul> <li>Any graph-level failure (timeout, cancellation, unknown exception) requires a new run.</li> <li>Resolver failures, decomposer failures, global planner failures, generator failures, executor failures, and aggregator failures have no local recovery and require a new run.</li> </ul>"},{"location":"architecture/failure_recovery/#what-is-unrecoverable","title":"What is unrecoverable","text":"<ul> <li><code>FATAL_ERRORS</code> or <code>CRITICAL</code> severity errors (security violations, missing datasource ID, missing LLM, invalid state) terminate the subgraph or graph without retry.</li> </ul>"},{"location":"architecture/failure_recovery/#replay-support","title":"Replay Support","text":"<p>Replay is not supported. There is no persisted graph state or execution log to re-run nodes; state is kept in memory and discarded after completion. Artifact references are stored in state only and are not used for graph replay.</p>"},{"location":"architecture/failure_recovery/#known-gaps","title":"Known Gaps","text":"<ul> <li>Physical validation node exists but is not wired into the SQL agent subgraph, so dry-run and cost checks do not execute.</li> <li>LLM circuit breaker is defined but never applied to LLM calls.</li> <li>Database circuit breaker only guards physical validation; SQL execution is not wrapped and does not use the sandbox.</li> <li>Subgraph wrapper assumes executor output is present; earlier failures can cause wrapper-level errors.</li> <li>Pipeline completion does not imply success; <code>PipelineRunner</code> does not inspect <code>errors</code> and always returns <code>success=True</code> if the graph returns.</li> <li>No graph-level retries or replay; only subgraph local retries.</li> </ul>"},{"location":"architecture/failure_recovery/#related-code","title":"Related Code","text":"<ul> <li><code>packages/core/src/nl2sql/pipeline/runtime.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/graph.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/routes.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/graph_utils.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/state.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/subgraphs/sql_agent.py</code></li> <li><code>packages/core/src/nl2sql/common/errors.py</code></li> <li><code>packages/core/src/nl2sql/common/resilience.py</code></li> <li><code>packages/core/src/nl2sql/common/sandbox.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/*/node.py</code></li> <li><code>packages/core/src/nl2sql/execution/executor/sql_executor.py</code></li> <li><code>packages/core/src/nl2sql/aggregation/aggregator.py</code></li> <li><code>packages/core/src/nl2sql/indexing/vector_store.py</code></li> <li><code>packages/core/src/nl2sql/schema/*.py</code></li> </ul>"},{"location":"architecture/graph_state/","title":"GraphState Architecture","text":""},{"location":"architecture/graph_state/#overview","title":"Overview","text":"<p>GraphState is the shared, typed state container for the NL2SQL pipeline's LangGraph execution. It exists to provide a single, structured place where node outputs, errors, reasoning, and subgraph artifacts accumulate as the graph runs. Its scope is the main pipeline graph built in <code>packages/core/src/nl2sql/pipeline/graph.py</code>, and it is the input/output contract for graph nodes declared in that graph.</p>"},{"location":"architecture/graph_state/#graphstate-definition","title":"GraphState Definition","text":""},{"location":"architecture/graph_state/#primary-class","title":"Primary class","text":"<ul> <li><code>GraphState</code> in <code>packages/core/src/nl2sql/pipeline/state.py</code></li> </ul> <p>Fields (exact names and types from code): - <code>trace_id: str</code> - <code>user_query: str</code> - <code>user_context: UserContext</code> - <code>datasource_id: Optional[str]</code> - <code>datasource_resolver_response: Optional[DatasourceResolverResponse]</code> - <code>decomposer_response: Optional[DecomposerResponse]</code> - <code>global_planner_response: Optional[GlobalPlannerResponse]</code> - <code>aggregator_response: Optional[AggregatorResponse]</code> - <code>answer_synthesizer_response: Optional[AnswerSynthesizerResponse]</code> - <code>artifact_refs: Annotated[Dict[str, ArtifactRef], update_results]</code> - <code>subgraph_outputs: Annotated[Dict[str, SubgraphOutput], update_results]</code> - <code>errors: Annotated[List[PipelineError], operator.add]</code> - <code>reasoning: Annotated[List[Dict[str, Any]], operator.add]</code> - <code>warnings: Annotated[List[Dict[str, Any]], operator.add]</code> - <code>subgraph_id: Optional[str]</code> - <code>subgraph_name: Optional[str]</code></p> <p>Reducer behavior in <code>GraphState</code>: - <code>update_results(current, new)</code> merges dicts as <code>{**current, **new}</code>. - <code>operator.add</code> concatenates lists for <code>errors</code>, <code>reasoning</code>, and <code>warnings</code>.</p>"},{"location":"architecture/graph_state/#subgraph-state-boundary","title":"Subgraph state boundary","text":"<ul> <li><code>SubgraphExecutionState</code> in <code>packages/core/src/nl2sql/pipeline/state.py</code> is the per-subgraph state used by the SQL agent subgraph.</li> </ul> <p>Fields (exact names and types from code): - <code>trace_id: str</code> - <code>sub_query: Optional[SubQuery]</code> - <code>user_context: Optional[UserContext]</code> - <code>subgraph_id: Optional[str]</code> - <code>subgraph_name: Optional[str]</code> - <code>relevant_tables: List[Table]</code> - <code>ast_planner_response: Optional[ASTPlannerResponse]</code> - <code>logical_validator_response: Optional[LogicalValidatorResponse]</code> - <code>physical_validator_response: Optional[PhysicalValidatorResponse]</code> - <code>generator_response: Optional[GeneratorResponse]</code> - <code>executor_response: Optional[ExecutorResponse]</code> - <code>refiner_response: Optional[RefinerResponse]</code> - <code>retry_count: int</code> - <code>errors: Annotated[List[PipelineError], operator.add]</code> - <code>reasoning: Annotated[List[Dict[str, Any]], operator.add]</code> - <code>warnings: Annotated[List[Dict[str, Any]], operator.add]</code></p>"},{"location":"architecture/graph_state/#subgraph-output-structure","title":"Subgraph output structure","text":"<ul> <li><code>SubgraphOutput</code> in <code>packages/core/src/nl2sql/pipeline/subgraphs/schemas.py</code></li> </ul> <p>Fields: - <code>sub_query: Optional[SubQuery]</code> - <code>subgraph_id: str</code> - <code>subgraph_name: Optional[str]</code> - <code>retry_count: int</code> - <code>plan: Optional[PlanModel]</code> - <code>sql_draft: Optional[str]</code> - <code>artifact: Optional[ArtifactRef]</code> - <code>errors: List[PipelineError]</code> - <code>reasoning: List[Dict[str, Any]]</code> - <code>status: Optional[str]</code></p>"},{"location":"architecture/graph_state/#field-lifecycle","title":"Field Lifecycle","text":"<p>The lifecycle below lists creation, mutation, reads, and resets based strictly on code.</p>"},{"location":"architecture/graph_state/#trace_id","title":"<code>trace_id</code>","text":"<ul> <li>Creation: default factory in <code>GraphState</code> (<code>uuid4</code>) if not provided; always set in <code>run_with_graph</code> when <code>GraphState</code> is instantiated.</li> <li>Mutation: not mutated after creation in this repository.</li> <li>Read points: <code>build_scan_payload</code> and <code>wrap_subgraph</code> use it; <code>ExecutorRequest</code> uses it in <code>ExecutorNode</code>.</li> <li>Reset: none.</li> </ul>"},{"location":"architecture/graph_state/#user_query","title":"<code>user_query</code>","text":"<ul> <li>Creation: passed into <code>GraphState</code> in <code>run_with_graph</code>.</li> <li>Mutation: none in code.</li> <li>Read points: <code>DatasourceResolverNode</code>, <code>DecomposerNode</code>, <code>AnswerSynthesizerNode</code>.</li> <li>Reset: none.</li> </ul>"},{"location":"architecture/graph_state/#user_context","title":"<code>user_context</code>","text":"<ul> <li>Creation: passed into <code>GraphState</code> in <code>run_with_graph</code> (or default factory).</li> <li>Mutation: none in code.</li> <li>Read points: <code>DatasourceResolverNode</code> (RBAC), <code>ExecutorNode</code>, and subgraph state in <code>wrap_subgraph</code>.</li> <li>Reset: none.</li> </ul>"},{"location":"architecture/graph_state/#datasource_id","title":"<code>datasource_id</code>","text":"<ul> <li>Creation: passed into <code>GraphState</code> in <code>run_with_graph</code>.</li> <li>Mutation: none in code.</li> <li>Read points: <code>DatasourceResolverNode</code> (explicit override).</li> <li>Reset: none.</li> </ul>"},{"location":"architecture/graph_state/#datasource_resolver_response","title":"<code>datasource_resolver_response</code>","text":"<ul> <li>Creation/mutation: returned by <code>DatasourceResolverNode</code> as <code>datasource_resolver_response</code>.</li> <li>Read points: <code>resolver_route</code>, <code>DecomposerNode</code>, and <code>build_scan_payload</code>.</li> <li>Reset: none in code.</li> </ul>"},{"location":"architecture/graph_state/#decomposer_response","title":"<code>decomposer_response</code>","text":"<ul> <li>Creation/mutation: returned by <code>DecomposerNode</code> as <code>decomposer_response</code>.</li> <li>Read points: <code>GlobalPlannerNode</code>, <code>build_scan_payload</code>, and <code>wrap_subgraph</code> (to find <code>sub_query</code> by id).</li> <li>Reset: none in code.</li> </ul>"},{"location":"architecture/graph_state/#global_planner_response","title":"<code>global_planner_response</code>","text":"<ul> <li>Creation/mutation: returned by <code>GlobalPlannerNode</code> as <code>global_planner_response</code>.</li> <li>Read points: <code>build_scan_layer_router</code> and <code>EngineAggregatorNode</code>.</li> <li>Reset: none in code.</li> </ul>"},{"location":"architecture/graph_state/#aggregator_response","title":"<code>aggregator_response</code>","text":"<ul> <li>Creation/mutation: returned by <code>EngineAggregatorNode</code> as <code>aggregator_response</code>.</li> <li>Read points: <code>AnswerSynthesizerNode</code>.</li> <li>Reset: none in code.</li> </ul>"},{"location":"architecture/graph_state/#answer_synthesizer_response","title":"<code>answer_synthesizer_response</code>","text":"<ul> <li>Creation/mutation: returned by <code>AnswerSynthesizerNode</code> as <code>answer_synthesizer_response</code>.</li> <li>Read points: none in core pipeline; read in CLI reporting.</li> <li>Reset: none in code.</li> </ul>"},{"location":"architecture/graph_state/#artifact_refs","title":"<code>artifact_refs</code>","text":"<ul> <li>Creation/mutation: returned by <code>wrap_subgraph</code> as <code>artifact_refs</code> (keyed by sub-query id). Merged by <code>update_results</code>.</li> <li>Read points: <code>build_scan_layer_router</code> (to skip completed scan nodes), <code>EngineAggregatorNode</code> (inputs to aggregation).</li> <li>Reset: none in code.</li> </ul>"},{"location":"architecture/graph_state/#subgraph_outputs","title":"<code>subgraph_outputs</code>","text":"<ul> <li>Creation/mutation: returned by <code>wrap_subgraph</code> as <code>subgraph_outputs</code> (keyed by <code>subgraph_id</code>). Merged by <code>update_results</code>.</li> <li>Read points: CLI <code>run_pipeline</code> and <code>BenchmarkRunner</code> consume for reporting.</li> <li>Reset: none in code.</li> </ul>"},{"location":"architecture/graph_state/#errors","title":"<code>errors</code>","text":"<ul> <li>Creation/mutation: appended in multiple nodes; reducer is list concatenation.</li> <li>Read points: routing logic and subgraph handling (e.g., <code>refiner</code> uses <code>state.errors</code>), CLI reporting.</li> <li>Reset: none in code.</li> </ul>"},{"location":"architecture/graph_state/#reasoning","title":"<code>reasoning</code>","text":"<ul> <li>Creation/mutation: appended in multiple nodes; reducer is list concatenation.</li> <li>Read points: <code>RefinerNode</code> and CLI reporting.</li> <li>Reset: none in code.</li> </ul>"},{"location":"architecture/graph_state/#warnings","title":"<code>warnings</code>","text":"<ul> <li>Creation/mutation: appended in nodes that emit warnings in the main graph (e.g., <code>DatasourceResolverNode</code>).</li> <li>Read points: CLI reporting.</li> <li>Reset: none in code.</li> </ul>"},{"location":"architecture/graph_state/#subgraph_id","title":"<code>subgraph_id</code>","text":"<ul> <li>Creation/mutation: created in <code>build_scan_payload</code> and passed to subgraph nodes.</li> <li>Read points: <code>wrap_subgraph</code> uses it to determine <code>sub_query</code> id and build <code>SubgraphOutput</code>.</li> <li>Reset: none in code.</li> </ul>"},{"location":"architecture/graph_state/#subgraph_name","title":"<code>subgraph_name</code>","text":"<ul> <li>Creation/mutation: created in <code>build_scan_payload</code> and passed to subgraph nodes.</li> <li>Read points: <code>wrap_subgraph</code> and <code>ExecutorRequest</code> via <code>ExecutorNode</code>.</li> <li>Reset: none in code.</li> </ul>"},{"location":"architecture/graph_state/#ownership-model","title":"Ownership Model","text":"<p>Ownership is defined by which node returns updates for a field: - <code>datasource_resolver_response</code>: <code>DatasourceResolverNode</code> - <code>decomposer_response</code>: <code>DecomposerNode</code> - <code>global_planner_response</code>: <code>GlobalPlannerNode</code> - <code>aggregator_response</code>: <code>EngineAggregatorNode</code> - <code>answer_synthesizer_response</code>: <code>AnswerSynthesizerNode</code> - <code>artifact_refs</code>: <code>wrap_subgraph</code> (subgraph wrapper in <code>graph_utils.py</code>) - <code>subgraph_outputs</code>: <code>wrap_subgraph</code> - <code>errors</code>, <code>reasoning</code>: emitted by many nodes and merged by list reducers - <code>warnings</code>: emitted by nodes that return <code>warnings</code> in the main graph (currently <code>DatasourceResolverNode</code>) - <code>subgraph_id</code>, <code>subgraph_name</code>: created in <code>build_scan_payload</code>, used only for subgraph execution context - <code>trace_id</code>, <code>user_query</code>, <code>user_context</code>, <code>datasource_id</code>: owned by the pipeline entrypoint (<code>run_with_graph</code>)</p> <p>Shared vs private: - Shared: <code>errors</code>, <code>reasoning</code>, <code>warnings</code>, <code>artifact_refs</code>, <code>subgraph_outputs</code> are aggregate fields merged across branches. - Private/branch-scoped: <code>subgraph_id</code>, <code>subgraph_name</code> are injected into subgraph payloads for a single subgraph execution.</p>"},{"location":"architecture/graph_state/#state-flow-across-dag","title":"State Flow Across DAG","text":"<p>Step-by-step execution flow as defined in <code>build_graph</code> and routing: 1. <code>run_with_graph</code> constructs <code>GraphState</code> with <code>user_query</code>, <code>user_context</code>, and optional <code>datasource_id</code>, then calls <code>graph.invoke(initial_state.model_dump())</code>. 2. <code>DatasourceResolverNode</code> runs first and populates <code>datasource_resolver_response</code>, <code>reasoning</code>, and <code>errors</code>. 3. <code>resolver_route</code> decides whether to continue based on <code>datasource_resolver_response</code>. 4. <code>DecomposerNode</code> produces <code>decomposer_response</code> and reasoning. 5. <code>GlobalPlannerNode</code> produces <code>global_planner_response</code> (including the <code>ExecutionDAG</code>). 6. <code>build_scan_layer_router</code> emits <code>Send</code> branches using <code>build_scan_payload</code> for each pending scan node. The payload contains <code>subgraph_id</code>, <code>subgraph_name</code>, <code>trace_id</code>, <code>user_context</code>, <code>decomposer_response</code>, and <code>datasource_resolver_response</code>. 7. Each subgraph is wrapped by <code>wrap_subgraph</code>, which:    - Builds a <code>SubgraphExecutionState</code> using the payload and a <code>sub_query</code> resolved from <code>decomposer_response</code>.    - Invokes the subgraph and validates the result into <code>SubgraphExecutionState</code>.    - Returns updates for <code>artifact_refs</code>, <code>subgraph_outputs</code>, <code>errors</code>, and <code>reasoning</code>. 8. The router checks <code>artifact_refs</code> to decide which scan nodes are still pending. When none remain, it routes to <code>aggregator</code>. 9. <code>EngineAggregatorNode</code> consumes <code>global_planner_response</code> and <code>artifact_refs</code> to produce <code>aggregator_response</code>. 10. <code>AnswerSynthesizerNode</code> consumes <code>aggregator_response</code> and <code>decomposer_response</code> to produce <code>answer_synthesizer_response</code>.</p> <p>Subgraph internal flow uses <code>SubgraphExecutionState</code> and is defined in <code>build_sql_agent_graph</code>: <code>schema_retriever</code> -&gt; <code>ast_planner</code> -&gt; <code>logical_validator</code> -&gt; <code>generator</code> -&gt; <code>executor</code>, with a retry loop via <code>retry_handler</code> and <code>refiner</code>.</p>"},{"location":"architecture/graph_state/#mutability-rules","title":"Mutability Rules","text":"<p>From code: - <code>GraphState</code> and <code>SubgraphExecutionState</code> are Pydantic <code>BaseModel</code> classes with <code>extra=\"ignore\"</code> and <code>arbitrary_types_allowed=True</code>. No immutability or frozen settings are defined. - List fields (<code>errors</code>, <code>reasoning</code>, <code>warnings</code>) are merged using <code>operator.add</code>. - Dict fields (<code>artifact_refs</code>, <code>subgraph_outputs</code>) are merged using <code>update_results</code> ({current, new}). - Subgraph boundary is a serialization boundary: <code>wrap_subgraph</code> uses <code>SubgraphExecutionState(...).model_dump()</code> to pass data into the subgraph and <code>SubgraphExecutionState.model_validate(...)</code> to rehydrate the result.</p> <p>No explicit copy-on-write mechanisms are defined beyond Pydantic serialization at subgraph boundaries.</p>"},{"location":"architecture/graph_state/#merge-semantics","title":"Merge Semantics","text":"<p>Defined reducers in <code>GraphState</code>: - <code>update_results</code> merges dicts with <code>{**current, **new}</code>. Keys in <code>new</code> overwrite keys in <code>current</code>. - <code>operator.add</code> concatenates list fields (<code>errors</code>, <code>reasoning</code>, <code>warnings</code>).</p> <p>No explicit reducer is defined in this repository for scalar fields; any conflict resolution beyond these reducers is not specified in code here.</p>"},{"location":"architecture/graph_state/#replayability","title":"Replayability","text":"<p>GraphState carries identifiers and artifact references that could support replay, but no replay or rehydration mechanism is implemented. See <code>failure_recovery.md</code> for replay support details and limitations.</p>"},{"location":"architecture/graph_state/#determinism-impact","title":"Determinism Impact","text":"<p>Determinism guarantees and non-determinism sources are documented in <code>determinism.md</code>. GraphState only carries the artifacts produced by those nodes (IDs, DAG hashes, errors, and diagnostics).</p>"},{"location":"architecture/graph_state/#serialization-persistence","title":"Serialization + Persistence","text":"<p>Serialization: - <code>GraphState</code> and <code>SubgraphExecutionState</code> are Pydantic models. <code>run_with_graph</code> calls <code>initial_state.model_dump()</code> before invoking the graph. - <code>wrap_subgraph</code> uses <code>model_dump()</code> and <code>model_validate()</code> for subgraph boundaries.</p> <p>Persistence: - GraphState itself is not persisted in this repository. - <code>artifact_refs</code> contain <code>ArtifactRef</code> entries with <code>uri</code>, <code>backend</code>, <code>format</code>, and <code>content_hash</code>, which reference artifacts stored elsewhere by executor components.</p>"},{"location":"architecture/graph_state/#known-limitations","title":"Known Limitations","text":"<p>Based on code only: - No explicit ordering guarantee is defined for merging parallel branch dict updates beyond <code>{**current, **new}</code>. - No built-in persistence or snapshotting for GraphState is implemented. - No explicit reset or cleanup semantics for accumulated fields (<code>errors</code>, <code>reasoning</code>, <code>warnings</code>, <code>artifact_refs</code>, <code>subgraph_outputs</code>).</p>"},{"location":"architecture/graph_state/#related-code","title":"Related Code","text":"<p>GraphState and reducers: - <code>packages/core/src/nl2sql/pipeline/state.py</code></p> <p>Graph construction and routing: - <code>packages/core/src/nl2sql/pipeline/graph.py</code> - <code>packages/core/src/nl2sql/pipeline/routes.py</code> - <code>packages/core/src/nl2sql/pipeline/graph_utils.py</code></p> <p>Subgraph state and execution: - <code>packages/core/src/nl2sql/pipeline/subgraphs/sql_agent.py</code> - <code>packages/core/src/nl2sql/pipeline/subgraphs/schemas.py</code></p> <p>Mutators / consumers: - <code>packages/core/src/nl2sql/pipeline/nodes/datasource_resolver/node.py</code> - <code>packages/core/src/nl2sql/pipeline/nodes/decomposer/node.py</code> - <code>packages/core/src/nl2sql/pipeline/nodes/global_planner/node.py</code> - <code>packages/core/src/nl2sql/pipeline/nodes/aggregator/node.py</code> - <code>packages/core/src/nl2sql/pipeline/nodes/answer_synthesizer/node.py</code> - <code>packages/core/src/nl2sql/pipeline/nodes/schema_retriever/node.py</code> - <code>packages/core/src/nl2sql/pipeline/nodes/ast_planner/node.py</code> - <code>packages/core/src/nl2sql/pipeline/nodes/validator/node.py</code> - <code>packages/core/src/nl2sql/pipeline/nodes/validator/physical_node.py</code> - <code>packages/core/src/nl2sql/pipeline/nodes/generator/node.py</code> - <code>packages/core/src/nl2sql/pipeline/nodes/executor/node.py</code> - <code>packages/core/src/nl2sql/pipeline/nodes/refiner/node.py</code></p>"},{"location":"architecture/indexing/","title":"Indexing, Chunking, and Retrieval Architecture","text":"<p>This document describes current indexing behavior as implemented in code. Indexing transforms schema snapshots into typed chunks that are embedded and stored in a Chroma vector store. Retrieval uses these chunks as candidates and then resolves authoritative schema details from <code>SchemaStore</code> (see <code>../schema/store.md</code> for schema contracts and store behavior).</p>"},{"location":"architecture/indexing/#indexing-flow","title":"Indexing flow","text":"<pre><code>flowchart TD\n    Adapter[Datasource Adapter] --&gt; Snapshot[Schema Snapshot]\n    Snapshot --&gt; Enrich[LLM Enrichment]\n    Enrich --&gt; Register[SchemaStore.register_snapshot]\n    Register --&gt; Chunker[SchemaChunkBuilder]\n    Chunker --&gt; Refresh[VectorStore.refresh_schema_chunks]\n    Refresh --&gt; Store[Chroma Vector Store]</code></pre>"},{"location":"architecture/indexing/#what-is-indexed-today","title":"What is indexed today","text":"<p>Only schema-derived chunks are indexed. There is no symbolic index, behavioral index, or non-schema catalogs in the current implementation.</p>"},{"location":"architecture/indexing/#chunk-types-and-contracts","title":"Chunk types and contracts","text":"<p>Chunk types are defined in <code>nl2sql.indexing.models</code>:</p> <ul> <li>DatasourceChunk (<code>schema.datasource</code>): datasource description, domains, and example questions.</li> <li>TableChunk (<code>schema.table</code>): table name, PKs, column list, FK summaries, row counts.</li> <li>ColumnChunk (<code>schema.column</code>): column type, stats, synonyms, PII flags.</li> <li>RelationshipChunk (<code>schema.relationship</code>): FK relationships, columns, cardinality.</li> <li>MetricChunk (<code>schema.metric</code>): defined but not emitted by <code>SchemaChunkBuilder</code>.</li> </ul> <p>Chunk models live in <code>nl2sql.indexing.models</code> and are Pydantic models. IDs are deterministic and include <code>schema_version</code>.</p>"},{"location":"architecture/indexing/#chunk-index-mapping-actual","title":"Chunk \u2192 index mapping (actual)","text":"<p>All chunk types are embedded as <code>langchain_core.documents.Document</code> and stored in Chroma:</p> <ul> <li><code>Document.page_content</code> = <code>chunk.get_page_content()</code></li> <li><code>Document.metadata</code> = <code>chunk.get_metadata()</code></li> </ul> <p>There is one index (Chroma collection) per <code>VectorStore</code> configuration.</p>"},{"location":"architecture/indexing/#chunking-strategy-as-implemented","title":"Chunking strategy (as implemented)","text":"<p>Chunking is aligned to planning intent:</p> <ul> <li>Routing uses datasource-level chunks.</li> <li>Schema context uses table/metric chunks.</li> <li>Planning context uses columns and relationships for specific tables.</li> </ul> <p>If retrieval returns no candidates, the retriever falls back to full schema enumeration.</p>"},{"location":"architecture/indexing/#retrieval-pipeline-staged-code-accurate","title":"Retrieval pipeline (staged, code-accurate)","text":"<pre><code>flowchart TD\n    SubQuery --&gt; SemQuery[_build_semantic_query]\n    SemQuery --&gt; L1[retrieve_schema_context (tables/metrics)]\n    L1 --&gt;|no tables| L2[retrieve_column_candidates]\n    L2 --&gt; L3[retrieve_planning_context (columns/relationships)]\n    L3 --&gt; Snapshot[SchemaStore.get_snapshot]\n    Snapshot --&gt; Tables[relevant_tables for planner]</code></pre>"},{"location":"architecture/indexing/#stages","title":"Stages","text":"<ol> <li>Schema context: retrieves table/metric chunks.</li> <li>Column candidates: fallback to column chunks when no table matches.</li> <li>Planning context: retrieves columns/relationships for the selected tables.</li> <li>Authoritative resolution: resolves tables/columns from <code>SchemaStore</code> snapshot.</li> </ol>"},{"location":"architecture/indexing/#authoritative-vs-semantic-sources","title":"Authoritative vs semantic sources","text":"<ul> <li>Semantic candidates come from Chroma (vector search over chunks).</li> <li>Authoritative schema is resolved from <code>SchemaStore</code> snapshots.</li> <li>Vector store content is never treated as authoritative.</li> </ul>"},{"location":"architecture/indexing/#metadata-propagation","title":"Metadata propagation","text":"<ul> <li><code>datasource_id</code> and <code>schema_version</code> are included in all chunk metadata.</li> <li><code>TableChunk</code> includes <code>primary_key</code>, <code>columns</code>, <code>row_count</code>, and FK summaries.</li> <li><code>ColumnChunk</code> includes <code>dtype</code>, <code>pii</code>, <code>description</code>, and stats when available.</li> <li><code>RelationshipChunk</code> includes join column pairs and cardinality.</li> </ul> <p>Metadata is consumed by <code>SchemaRetrieverNode</code> to construct <code>Table</code> objects for planning and by <code>LogicalValidatorNode</code> to validate joins and filters.</p>"},{"location":"architecture/indexing/#versioning-and-determinism","title":"Versioning and determinism","text":"<ul> <li>Schema snapshots are fingerprinted and versioned (<code>YYYYMMDDhhmmss_&lt;fp8&gt;</code>).</li> <li>Chunk IDs embed <code>schema_version</code> for deterministic indexing.</li> <li>Retrieval uses the <code>SubQuery.schema_version</code> when available; otherwise latest snapshot.</li> <li>MMR ranking may introduce non-determinism in ordering for similar scores.</li> </ul>"},{"location":"architecture/indexing/#tenant-isolation-current-state","title":"Tenant isolation (current state)","text":"<p>Tenant scoping is not implemented in indexing:</p> <ul> <li>No <code>tenant_id</code> field exists in chunk metadata.</li> <li>Vector store queries do not filter by tenant.</li> <li>Schema store is global.</li> </ul>"},{"location":"architecture/indexing/#failure-modes-and-fallbacks","title":"Failure modes and fallbacks","text":"<p>Current failure behaviors:</p> <ul> <li>Vector store retrieval wrapped by <code>VECTOR_BREAKER</code>; failures fast\u2011fail.</li> <li>Vector retrieval errors in <code>SchemaRetrieverNode</code> return empty results with warnings.</li> <li>If no candidates are found, the retriever falls back to full schema snapshot.</li> <li>Enrichment failures return the original snapshot without enrichment.</li> </ul>"},{"location":"architecture/indexing/#performance-characteristics-current","title":"Performance characteristics (current)","text":"<ul> <li>Embedding uses OpenAI embeddings via <code>EmbeddingService</code>.</li> <li>Vector search uses Chroma MMR (<code>lambda_mult=0.7</code>, <code>fetch_k = 4*k</code>).</li> <li>No caching or sharding layers are implemented.</li> <li>Index refresh is full reindex per schema snapshot.</li> </ul>"},{"location":"architecture/indexing/#observability-hooks","title":"Observability hooks","text":"<ul> <li><code>VECTOR_BREAKER</code> logs breaker state changes.</li> <li>Indexing and retrieval log errors; no metrics are emitted in indexing code.</li> </ul>"},{"location":"architecture/indexing/#source-references","title":"Source references","text":"<ul> <li>Chunk models: <code>packages/core/src/nl2sql/indexing/models.py</code></li> <li>Chunk builder: <code>packages/core/src/nl2sql/indexing/chunk_builder.py</code></li> <li>Vector store: <code>packages/core/src/nl2sql/indexing/vector_store.py</code></li> <li>Schema retriever: <code>packages/core/src/nl2sql/pipeline/nodes/schema_retriever/node.py</code></li> <li>Indexing orchestrator: <code>packages/core/src/nl2sql/indexing/orchestrator.py</code></li> <li>Embeddings: <code>packages/core/src/nl2sql/indexing/embeddings.py</code></li> </ul>"},{"location":"architecture/invariants/","title":"Architectural Invariants","text":""},{"location":"architecture/invariants/#overview","title":"Overview","text":"<p>These invariants capture rules that are enforced by the code paths responsible for planning, validation, and execution. They exist to keep the system deterministic, secure, and stable under malformed input or misconfiguration.</p>"},{"location":"architecture/invariants/#semantic-only-subqueries-and-post-combine-ops","title":"Semantic-Only Subqueries and Post-Combine Ops","text":""},{"location":"architecture/invariants/#definition","title":"Definition","text":"<p>Sub-query intents and post-combine operations must not contain SQL tokens or physical schema keywords.</p>"},{"location":"architecture/invariants/#enforcement-points","title":"Enforcement Points","text":"<ul> <li><code>SubQuery.validate_semantic_only()</code> in <code>nl2sql.pipeline.nodes.decomposer.schemas</code></li> <li><code>PostCombineOp.validate_semantic_only()</code> in <code>nl2sql.pipeline.nodes.decomposer.schemas</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior","title":"Failure Behavior","text":"<p>Raises <code>ValueError</code> on any forbidden token detection.</p>"},{"location":"architecture/invariants/#why-it-exists","title":"Why It Exists","text":"<p>Prevents physical SQL leakage into semantic planning stages and keeps decomposer output safe to interpret downstream.</p>"},{"location":"architecture/invariants/#combine-groups-require-roles-and-join-keys","title":"Combine Groups Require Roles and Join Keys","text":""},{"location":"architecture/invariants/#definition_1","title":"Definition","text":"<p>Combine groups with <code>compare</code> or <code>join</code> operations must include a role for every input and at least one join key pair.</p>"},{"location":"architecture/invariants/#enforcement-points_1","title":"Enforcement Points","text":"<ul> <li><code>CombineGroup.validate_roles()</code> in <code>nl2sql.pipeline.nodes.decomposer.schemas</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_1","title":"Failure Behavior","text":"<p>Raises <code>ValueError</code> when roles or join keys are missing.</p>"},{"location":"architecture/invariants/#why-it-exists_1","title":"Why It Exists","text":"<p>Ensures combine operations are well-defined and deterministic for aggregation.</p>"},{"location":"architecture/invariants/#decomposer-references-must-resolve","title":"Decomposer References Must Resolve","text":""},{"location":"architecture/invariants/#definition_2","title":"Definition","text":"<p>Combine-group inputs must reference existing subqueries, and post-combine ops must reference existing combine groups.</p>"},{"location":"architecture/invariants/#enforcement-points_2","title":"Enforcement Points","text":"<ul> <li><code>DecomposerResponse.validate_references()</code> in <code>nl2sql.pipeline.nodes.decomposer.schemas</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_2","title":"Failure Behavior","text":"<p>Raises <code>ValueError</code> for unknown subquery or group references.</p>"},{"location":"architecture/invariants/#why-it-exists_2","title":"Why It Exists","text":"<p>Prevents invalid execution graphs and dangling references.</p>"},{"location":"architecture/invariants/#expression-ast-must-match-its-kind","title":"Expression AST Must Match Its Kind","text":""},{"location":"architecture/invariants/#definition_3","title":"Definition","text":"<p>Expression nodes must satisfy kind-specific required fields (e.g., binary ops require left/right and an operator).</p>"},{"location":"architecture/invariants/#enforcement-points_3","title":"Enforcement Points","text":"<ul> <li><code>Expr.model_post_init()</code> in <code>nl2sql.pipeline.nodes.ast_planner.schemas</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_3","title":"Failure Behavior","text":"<p>Raises <code>ValueError</code> when required fields are missing or operators are invalid.</p>"},{"location":"architecture/invariants/#why-it-exists_3","title":"Why It Exists","text":"<p>Guarantees the AST is structurally complete before SQL generation.</p>"},{"location":"architecture/invariants/#plan-model-is-strict-and-read-only","title":"Plan Model Is Strict and Read-Only","text":""},{"location":"architecture/invariants/#definition_4","title":"Definition","text":"<p>Plan fields are schema-validated with no unknown fields, and query type must be <code>READ</code>.</p>"},{"location":"architecture/invariants/#enforcement-points_4","title":"Enforcement Points","text":"<ul> <li><code>ConfigDict(extra=\"forbid\")</code> on plan models in <code>nl2sql.pipeline.nodes.ast_planner.schemas</code></li> <li><code>PlanModel.query_type</code> literal and <code>LogicalValidatorNode._validate_static()</code> in <code>nl2sql.pipeline.nodes.validator.node</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_4","title":"Failure Behavior","text":"<p>Pydantic validation errors for extra/invalid fields; <code>PipelineError</code> with <code>SECURITY_VIOLATION</code> for non-READ queries.</p>"},{"location":"architecture/invariants/#why-it-exists_4","title":"Why It Exists","text":"<p>Prevents mutation or unknown plan constructs from entering execution.</p>"},{"location":"architecture/invariants/#plan-ordinals-are-contiguous","title":"Plan Ordinals Are Contiguous","text":""},{"location":"architecture/invariants/#definition_5","title":"Definition","text":"<p>Ordinal fields for tables, joins, select items, group-by, and order-by must be contiguous starting at 0.</p>"},{"location":"architecture/invariants/#enforcement-points_5","title":"Enforcement Points","text":"<ul> <li><code>LogicalValidatorNode._validate_ordinals()</code> in <code>nl2sql.pipeline.nodes.validator.node</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_5","title":"Failure Behavior","text":"<p>Returns <code>PipelineError</code> with <code>INVALID_PLAN_STRUCTURE</code>.</p>"},{"location":"architecture/invariants/#why-it-exists_5","title":"Why It Exists","text":"<p>Ensures deterministic ordering and stable SQL generation.</p>"},{"location":"architecture/invariants/#table-aliases-are-unique","title":"Table Aliases Are Unique","text":""},{"location":"architecture/invariants/#definition_6","title":"Definition","text":"<p>Each table alias in the plan must be unique.</p>"},{"location":"architecture/invariants/#enforcement-points_6","title":"Enforcement Points","text":"<ul> <li><code>LogicalValidatorNode._alias_collision()</code> in <code>nl2sql.pipeline.nodes.validator.node</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_6","title":"Failure Behavior","text":"<p>Returns <code>PipelineError</code> with <code>INVALID_PLAN_STRUCTURE</code>.</p>"},{"location":"architecture/invariants/#why-it-exists_6","title":"Why It Exists","text":"<p>Prevents ambiguous column resolution.</p>"},{"location":"architecture/invariants/#expected-schema-must-match-select-list","title":"Expected Schema Must Match Select List","text":""},{"location":"architecture/invariants/#definition_7","title":"Definition","text":"<p>When <code>expected_schema</code> is provided, the plan\u2019s <code>select_items</code> count and aliases must match it.</p>"},{"location":"architecture/invariants/#enforcement-points_7","title":"Enforcement Points","text":"<ul> <li><code>LogicalValidatorNode._validate_static()</code> in <code>nl2sql.pipeline.nodes.validator.node</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_7","title":"Failure Behavior","text":"<p>Returns <code>PipelineError</code> with <code>INVALID_PLAN_STRUCTURE</code>.</p>"},{"location":"architecture/invariants/#why-it-exists_7","title":"Why It Exists","text":"<p>Keeps multi-stage subqueries contractually aligned.</p>"},{"location":"architecture/invariants/#column-references-must-resolve","title":"Column References Must Resolve","text":""},{"location":"architecture/invariants/#definition_8","title":"Definition","text":"<p>All column references must map to declared table aliases and be unambiguous.</p>"},{"location":"architecture/invariants/#enforcement-points_8","title":"Enforcement Points","text":"<ul> <li><code>ValidatorVisitor</code> in <code>nl2sql.pipeline.nodes.validator.node</code></li> <li><code>LogicalValidatorNode._validate_static()</code> converts visitor errors to <code>PipelineError</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_8","title":"Failure Behavior","text":"<p>Returns <code>PipelineError</code> with <code>COLUMN_NOT_FOUND</code> (severity depends on <code>logical_validator_strict_columns</code>).</p>"},{"location":"architecture/invariants/#why-it-exists_8","title":"Why It Exists","text":"<p>Prevents queries from referencing non-existent or ambiguous columns.</p>"},{"location":"architecture/invariants/#joins-must-be-valid-and-schema-backed","title":"Joins Must Be Valid and Schema-Backed","text":""},{"location":"architecture/invariants/#definition_9","title":"Definition","text":"<p>Join aliases must exist, join conditions must reference both sides, must include an equality pair, and must match allowed schema relationships.</p>"},{"location":"architecture/invariants/#enforcement-points_9","title":"Enforcement Points","text":"<ul> <li><code>LogicalValidatorNode._validate_static()</code> join checks in <code>nl2sql.pipeline.nodes.validator.node</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_9","title":"Failure Behavior","text":"<p>Returns <code>PipelineError</code> with <code>INVALID_PLAN_STRUCTURE</code> or <code>JOIN_TABLE_NOT_IN_PLAN</code>.</p>"},{"location":"architecture/invariants/#why-it-exists_9","title":"Why It Exists","text":"<p>Prevents invalid joins and enforces schema-authorized relationships.</p>"},{"location":"architecture/invariants/#policy-enforcement-is-namespaced-and-fail-closed","title":"Policy Enforcement Is Namespaced and Fail-Closed","text":""},{"location":"architecture/invariants/#definition_10","title":"Definition","text":"<p>Allowed tables must be namespaced as <code>datasource.table</code> or <code>datasource.*</code>, and policy checks fail if datasource ID is missing.</p>"},{"location":"architecture/invariants/#enforcement-points_10","title":"Enforcement Points","text":"<ul> <li><code>RolePolicy.validate_namespace()</code> in <code>nl2sql.security.policies</code></li> <li><code>LogicalValidatorNode._validate_policy()</code> in <code>nl2sql.pipeline.nodes.validator.node</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_10","title":"Failure Behavior","text":"<p><code>ValueError</code> for invalid policy format; <code>PipelineError</code> with <code>SECURITY_VIOLATION</code> if datasource ID is missing or a table is not allowed.</p>"},{"location":"architecture/invariants/#why-it-exists_10","title":"Why It Exists","text":"<p>Ensures access control boundaries are explicit and enforced.</p>"},{"location":"architecture/invariants/#datasource-access-is-rbac-gated","title":"Datasource Access Is RBAC-Gated","text":""},{"location":"architecture/invariants/#definition_11","title":"Definition","text":"<p>Only datasources permitted by RBAC may be selected or resolved.</p>"},{"location":"architecture/invariants/#enforcement-points_11","title":"Enforcement Points","text":"<ul> <li><code>DatasourceResolverNode._get_allowed_datasource_ids()</code> and <code>__call__()</code> in <code>nl2sql.pipeline.nodes.datasource_resolver.node</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_11","title":"Failure Behavior","text":"<p>Returns <code>PipelineError</code> with <code>SECURITY_VIOLATION</code> if no allowed datasource is available.</p>"},{"location":"architecture/invariants/#why-it-exists_11","title":"Why It Exists","text":"<p>Prevents execution against unauthorized datasources.</p>"},{"location":"architecture/invariants/#sql-execution-requires-sql-datasource-and-capability","title":"SQL Execution Requires SQL, Datasource, and Capability","text":""},{"location":"architecture/invariants/#definition_12","title":"Definition","text":"<p>Execution only proceeds when SQL text exists, a datasource ID is present, and the datasource supports SQL.</p>"},{"location":"architecture/invariants/#enforcement-points_12","title":"Enforcement Points","text":"<ul> <li><code>ExecutorNode.__call__()</code> in <code>nl2sql.pipeline.nodes.executor.node</code></li> <li><code>SqlExecutorService.validate_request()</code> in <code>nl2sql.execution.executor.sql_executor</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_12","title":"Failure Behavior","text":"<p>Returns <code>PipelineError</code> with <code>MISSING_SQL</code>, <code>MISSING_DATASOURCE_ID</code>, or <code>INVALID_STATE</code>.</p>"},{"location":"architecture/invariants/#why-it-exists_12","title":"Why It Exists","text":"<p>Prevents invalid execution requests and ensures capability compatibility.</p>"},{"location":"architecture/invariants/#execution-dag-must-be-valid-and-acyclic","title":"Execution DAG Must Be Valid and Acyclic","text":""},{"location":"architecture/invariants/#definition_13","title":"Definition","text":"<p>Post-combine ops must target known combine groups, all edges must reference existing nodes, and the DAG must be acyclic.</p>"},{"location":"architecture/invariants/#enforcement-points_13","title":"Enforcement Points","text":"<ul> <li><code>GlobalPlannerNode.__call__()</code> in <code>nl2sql.pipeline.nodes.global_planner.node</code></li> <li><code>ExecutionDAG._layered_toposort()</code> in <code>nl2sql.pipeline.nodes.global_planner.schemas</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_13","title":"Failure Behavior","text":"<p>Raises <code>ValueError</code>, leading to <code>PipelineError</code> with <code>PLANNER_FAILED</code>.</p>"},{"location":"architecture/invariants/#why-it-exists_13","title":"Why It Exists","text":"<p>Ensures deterministic and well-ordered aggregation execution.</p>"},{"location":"architecture/invariants/#relation-schemas-have-unique-column-names","title":"Relation Schemas Have Unique Column Names","text":""},{"location":"architecture/invariants/#definition_14","title":"Definition","text":"<p>Each relation schema\u2019s column names must be unique.</p>"},{"location":"architecture/invariants/#enforcement-points_14","title":"Enforcement Points","text":"<ul> <li><code>RelationSchema.validate_unique_columns()</code> in <code>nl2sql.pipeline.nodes.global_planner.schemas</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_14","title":"Failure Behavior","text":"<p>Raises <code>ValueError</code> during schema validation.</p>"},{"location":"architecture/invariants/#why-it-exists_14","title":"Why It Exists","text":"<p>Prevents ambiguous column outputs in execution DAG nodes.</p>"},{"location":"architecture/invariants/#aggregation-requires-scan-artifacts-and-single-input-post-nodes","title":"Aggregation Requires Scan Artifacts and Single-Input Post Nodes","text":""},{"location":"architecture/invariants/#definition_15","title":"Definition","text":"<p>Scan nodes must have corresponding artifacts, and post-combine nodes must have exactly one input.</p>"},{"location":"architecture/invariants/#enforcement-points_15","title":"Enforcement Points","text":"<ul> <li><code>AggregationService.execute()</code> in <code>nl2sql.aggregation.aggregator</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_15","title":"Failure Behavior","text":"<p>Raises <code>ValueError</code> when artifacts are missing or post nodes have invalid inputs.</p>"},{"location":"architecture/invariants/#why-it-exists_15","title":"Why It Exists","text":"<p>Guarantees aggregation operates on complete, correctly wired inputs.</p>"},{"location":"architecture/invariants/#context-requires-vector-store-and-schema-store-configuration","title":"Context Requires Vector Store and Schema Store Configuration","text":""},{"location":"architecture/invariants/#definition_16","title":"Definition","text":"<p>Vector store collection/path and schema store path (for SQLite backend) must be configured.</p>"},{"location":"architecture/invariants/#enforcement-points_16","title":"Enforcement Points","text":"<ul> <li><code>NL2SQLContext.__init__()</code> in <code>nl2sql.context</code></li> <li><code>build_schema_store()</code> in <code>nl2sql.schema.store</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_16","title":"Failure Behavior","text":"<p>Raises <code>ValueError</code> if required settings are missing.</p>"},{"location":"architecture/invariants/#why-it-exists_16","title":"Why It Exists","text":"<p>Prevents startup with incomplete indexing and schema storage configuration.</p>"},{"location":"architecture/invariants/#sql-generation-enforces-a-row-limit-cap","title":"SQL Generation Enforces a Row Limit Cap","text":""},{"location":"architecture/invariants/#definition_17","title":"Definition","text":"<p>Generated SQL must not exceed the datasource adapter\u2019s row limit.</p>"},{"location":"architecture/invariants/#enforcement-points_17","title":"Enforcement Points","text":"<ul> <li><code>GeneratorNode.__call__()</code> in <code>nl2sql.pipeline.nodes.generator.node</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_17","title":"Failure Behavior","text":"<p>No error; the limit is clamped to the adapter\u2019s maximum.</p>"},{"location":"architecture/invariants/#why-it-exists_17","title":"Why It Exists","text":"<p>Protects execution resources and prevents runaway result sizes.</p>"},{"location":"architecture/invariants/#pipeline-execution-is-time-bounded","title":"Pipeline Execution Is Time-Bounded","text":""},{"location":"architecture/invariants/#definition_18","title":"Definition","text":"<p>Pipeline execution must complete within <code>settings.global_timeout_sec</code>.</p>"},{"location":"architecture/invariants/#enforcement-points_18","title":"Enforcement Points","text":"<ul> <li><code>run_with_graph()</code> in <code>nl2sql.pipeline.runtime</code></li> </ul>"},{"location":"architecture/invariants/#failure-behavior_18","title":"Failure Behavior","text":"<p>Returns <code>PipelineError</code> with <code>PIPELINE_TIMEOUT</code> and a timeout response message.</p>"},{"location":"architecture/invariants/#why-it-exists_18","title":"Why It Exists","text":"<p>Ensures latency bounds and prevents hung requests.</p>"},{"location":"architecture/invariants/#categories","title":"Categories","text":"<ul> <li>State: Plan Model Is Strict and Read-Only; Relation Schemas Have Unique Column Names; Context Requires Vector Store and Schema Store Configuration</li> <li>Execution: SQL Execution Requires SQL, Datasource, and Capability; Execution DAG Must Be Valid and Acyclic; Aggregation Requires Scan Artifacts and Single-Input Post Nodes; Pipeline Execution Is Time-Bounded</li> <li>Security: Policy Enforcement Is Namespaced and Fail-Closed; Datasource Access Is RBAC-Gated; Plan Model Is Strict and Read-Only</li> <li>Determinism: Plan Ordinals Are Contiguous; Expected Schema Must Match Select List; Joins Must Be Valid and Schema-Backed</li> <li>Isolation: SQL Generation Enforces a Row Limit Cap; Context Requires Vector Store and Schema Store Configuration</li> </ul>"},{"location":"architecture/invariants/#gaps","title":"Gaps","text":"<ul> <li>Column existence enforcement can degrade to warnings when <code>logical_validator_strict_columns</code> is disabled, so missing columns do not always block execution.</li> <li>Schema version mismatch handling is policy-driven and may only emit warnings (e.g., <code>schema_version_mismatch_policy=warn</code>), so mismatch is not always enforced as a hard failure.</li> <li>Semantic-only checks are token based; they block a fixed list of SQL tokens rather than parsing for all possible SQL constructs.</li> <li>If the vector store is unavailable, datasource resolution can return a response without errors, relying on downstream stages to detect missing candidates.</li> </ul>"},{"location":"architecture/invariants/#related-code","title":"Related Code","text":"<ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/decomposer/schemas.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/ast_planner/schemas.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/validator/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/datasource_resolver/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/generator/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/executor/node.py</code></li> <li><code>packages/core/src/nl2sql/execution/executor/sql_executor.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/global_planner/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/global_planner/schemas.py</code></li> <li><code>packages/core/src/nl2sql/aggregation/aggregator.py</code></li> <li><code>packages/core/src/nl2sql/context.py</code></li> <li><code>packages/core/src/nl2sql/schema/store.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/runtime.py</code></li> <li><code>packages/core/src/nl2sql/security/policies.py</code></li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>NL2SQL is a multi-stage orchestration pipeline built on LangGraph. A single request flows through control-plane nodes (resolution, decomposition, planning), capability-driven subgraphs (SQL agents), and an aggregation layer. The system is wired by <code>NL2SQLContext</code>, which instantiates registries, stores, and policy enforcement.</p>"},{"location":"architecture/overview/#key-runtime-components","title":"Key runtime components","text":"<ul> <li><code>NL2SQLContext</code> initializes registries, stores, and policy enforcement.</li> <li><code>build_graph()</code> compiles the LangGraph control-plane pipeline.</li> <li><code>run_with_graph()</code> executes the pipeline with cancellation and timeout.</li> <li><code>GraphState</code> is the shared mutable state across pipeline nodes (see <code>graph_state.md</code>).</li> </ul>"},{"location":"architecture/overview/#system-topology","title":"System topology","text":"<pre><code>flowchart TD\n    UserQuery[User Query] --&gt; Runtime[run_with_graph()]\n    Runtime --&gt; Graph[build_graph()]\n    Graph --&gt; Resolver[DatasourceResolverNode]\n    Resolver --&gt; Decomposer[DecomposerNode]\n    Decomposer --&gt; Planner[GlobalPlannerNode]\n    Planner --&gt; Router[Scan Layer Router]\n    Router --&gt; Subgraph[SQL Agent Subgraph]\n    Subgraph --&gt; Router\n    Router --&gt; Aggregator[EngineAggregatorNode]\n    Aggregator --&gt; Synthesizer[AnswerSynthesizerNode]\n\n    subgraph Context[NL2SQLContext]\n        DS[DatasourceRegistry]\n        LLM[LLMRegistry]\n        VS[VectorStore]\n        SS[SchemaStore]\n        RBAC[RBAC]\n        AS[ArtifactStore]\n    end</code></pre>"},{"location":"architecture/overview/#subsystem-architecture","title":"Subsystem architecture","text":"<pre><code>flowchart LR\n    User[User Query]\n    Planner[Planner/Decomposer]\n    Schema[Schema Store]\n    Retrieval[Chunking + Retrieval]\n    Validator[Validation Layer]\n    Execution[Execution Layer]\n    Adapter[Adapter/Plugin Backend]\n    Artifacts[Artifact Store]\n    Obs[Observability]\n\n    User --&gt; Planner\n    Planner --&gt; Retrieval\n    Retrieval --&gt; Schema\n    Planner --&gt; Validator\n    Validator --&gt; Execution\n    Execution --&gt; Adapter\n    Execution --&gt; Artifacts\n    Planner --&gt; Obs\n    Execution --&gt; Obs</code></pre>"},{"location":"architecture/overview/#major-subsystems-and-responsibilities","title":"Major subsystems (and responsibilities)","text":"<ul> <li>Planner / Decomposer: <code>DecomposerNode</code> produces stable, semantically-scoped sub-queries; <code>GlobalPlannerNode</code> produces a deterministic <code>ExecutionDAG</code>.</li> <li>Schema Store: <code>SchemaStore</code> persists versioned schema snapshots with fingerprints.</li> <li>Chunking + Retrieval: <code>SchemaChunkBuilder</code> produces typed chunks; <code>VectorStore</code> provides staged retrieval for routing and planning context.</li> <li>Validation layer: <code>LogicalValidatorNode</code> enforces schema correctness and RBAC.</li> <li>Execution layer: <code>ExecutorNode</code> routes to capability-driven executor services (e.g., SQL executor).</li> <li>Adapter / Plugin backend: adapters implement <code>DatasourceAdapterProtocol</code> and are discovered via entry points.</li> <li>Artifact store: executor results are persisted as Parquet artifacts and referenced by <code>ArtifactRef</code>.</li> <li>Observability: structured logs, OpenTelemetry metrics, and audit events.</li> </ul>"},{"location":"architecture/overview/#end-to-end-flow","title":"End-to-end flow","text":"<pre><code>sequenceDiagram\n    participant User as User\n    participant Runtime as run_with_graph\n    participant Resolver as DatasourceResolverNode\n    participant Decomposer as DecomposerNode\n    participant Planner as GlobalPlannerNode\n    participant Router as Scan Layer Router\n    participant Subgraph as SQL Agent Subgraph\n    participant Agg as EngineAggregatorNode\n    participant Synth as AnswerSynthesizerNode\n\n    User-&gt;&gt;Runtime: user_query\n    Runtime-&gt;&gt;Resolver: GraphState\n    Resolver-&gt;&gt;Decomposer: resolved datasources\n    Decomposer-&gt;&gt;Planner: sub_queries + combine groups\n    Planner-&gt;&gt;Router: ExecutionDAG\n    Router-&gt;&gt;Subgraph: Send(sub_query)\n    Subgraph--&gt;&gt;Router: ArtifactRef + diagnostics\n    Router-&gt;&gt;Agg: all scan artifacts\n    Agg-&gt;&gt;Synth: aggregated rows\n    Synth--&gt;&gt;Runtime: final_answer + errors</code></pre>"},{"location":"architecture/overview/#determinism","title":"Determinism","text":"<p>Determinism guarantees and non-determinism sources are documented in <code>determinism.md</code>.</p>"},{"location":"architecture/overview/#source-references","title":"Source references","text":"<ul> <li>Graph construction: <code>packages/core/src/nl2sql/pipeline/graph.py</code></li> <li>Runtime execution: <code>packages/core/src/nl2sql/pipeline/runtime.py</code></li> <li>Shared state: <code>packages/core/src/nl2sql/pipeline/state.py</code></li> <li>Context initialization: <code>packages/core/src/nl2sql/context.py</code></li> </ul>"},{"location":"architecture/pipeline/","title":"Pipeline Architecture","text":"<p>This document consolidates the LangGraph pipeline architecture as implemented in code. It describes the control graph, logical DAG, routing, and execution/aggregation flow. Node and subgraph details live in their respective sections.</p>"},{"location":"architecture/pipeline/#end-to-end-flow","title":"End-to-end flow","text":"<pre><code>flowchart TD\n    Resolver[DatasourceResolverNode] --&gt; Decomposer[DecomposerNode]\n    Decomposer --&gt; Planner[GlobalPlannerNode]\n    Planner --&gt; Router[layer_router]\n    Router --&gt; SqlAgent[SQL Agent Subgraph]\n    SqlAgent --&gt; Router\n    Router --&gt; Aggregator[EngineAggregatorNode]\n    Aggregator --&gt; Synthesizer[AnswerSynthesizerNode]</code></pre>"},{"location":"architecture/pipeline/#control-graph-nodes","title":"Control graph nodes","text":"<ul> <li>DatasourceResolverNode</li> <li>DecomposerNode</li> <li>GlobalPlannerNode</li> <li><code>layer_router</code> (routing function, not a node class)</li> <li>EngineAggregatorNode</li> <li>AnswerSynthesizerNode</li> </ul>"},{"location":"architecture/pipeline/#entry-points","title":"Entry points","text":"<ul> <li><code>build_graph(ctx)</code> builds the control graph in <code>nl2sql.pipeline.graph</code>.</li> <li><code>run_with_graph(ctx, user_query, ...)</code> executes the graph with timeouts and cancellation in <code>nl2sql.pipeline.runtime</code>.</li> </ul>"},{"location":"architecture/pipeline/#control-graph-and-logical-dag","title":"Control graph and logical DAG","text":"<p>The system uses two orchestration layers:</p> <ul> <li>Control graph (LangGraph <code>StateGraph</code>): runtime nodes and routing.</li> <li>Logical DAG (<code>ExecutionDAG</code>): deterministic compute plan produced by the global planner.</li> </ul> <pre><code>graph TD\n    Scan1[scan:subquery_1] --&gt; Combine[combine:group_1]\n    Scan2[scan:subquery_2] --&gt; Combine\n    Combine --&gt; PostAgg[post_aggregate]\n    PostAgg --&gt; PostSort[post_sort]</code></pre> <p><code>ExecutionDAG.layers</code> are computed deterministically by <code>_layered_toposort()</code> and consumed by the router to dispatch scan nodes.</p>"},{"location":"architecture/pipeline/#executiondag-node-kinds-actual","title":"ExecutionDAG node kinds (actual)","text":"<p><code>ExecutionDAG</code> nodes are <code>LogicalNode</code> instances with:</p> <ul> <li><code>kind</code>: <code>scan</code>, <code>combine</code>, <code>post_filter</code>, <code>post_aggregate</code>, <code>post_project</code>, <code>post_sort</code>, <code>post_limit</code></li> <li><code>inputs</code>: upstream node IDs</li> <li><code>output_schema</code>: <code>RelationSchema</code> (column specs)</li> <li><code>attributes</code>: operation-specific metadata (e.g., join keys, filters, metrics)</li> </ul>"},{"location":"architecture/pipeline/#routing-and-subgraph-selection","title":"Routing and subgraph selection","text":"<p><code>build_scan_layer_router()</code> selects the next scan layer with missing artifacts and dispatches subgraphs in parallel using <code>Send()</code>. Scan node IDs are matched against <code>SubQuery.id</code> for datasource routing; otherwise datasource ID is resolved from node attributes.</p> <p>Subgraph selection is capability-based via <code>resolve_subgraph()</code>:</p> <pre><code>flowchart TD\n    DatasourceId[DatasourceId] --&gt; Capabilities[DatasourceCapability set]\n    Capabilities --&gt; SubgraphRegistry[SubgraphRegistry]\n    SubgraphRegistry --&gt; Match{RequiredSubset}\n    Match --&gt;|yes| Subgraph[SelectedSubgraph]\n    Match --&gt;|no| Error[PipelineError]</code></pre> <p>If no compatible subgraph is found, routing raises <code>PipelineError</code> and terminates the pipeline.</p>"},{"location":"architecture/pipeline/#sql-agent-subgraph-lifecycle","title":"SQL agent subgraph lifecycle","text":"<p>The only registered subgraph today is <code>sql_agent</code> (<code>build_sql_agent_graph()</code>).</p> <p>See <code>subgraphs/sql_agent.md</code> for full lifecycle, retry semantics, and node wiring.</p>"},{"location":"architecture/pipeline/#execution-artifacts-and-aggregation","title":"Execution, artifacts, and aggregation","text":"<p>Executors produce <code>ResultFrame</code> objects, which are persisted via <code>ArtifactStore</code> and referenced by <code>ArtifactRef</code>. Aggregation reads artifacts and applies the logical DAG.</p> <pre><code>sequenceDiagram\n    participant Adapter as DatasourceAdapterProtocol\n    participant Exec as SqlExecutorService\n    participant Store as ArtifactStore\n    participant Agg as AggregationService\n\n    Adapter-&gt;&gt;Exec: ResultFrame\n    Exec-&gt;&gt;Store: create_artifact_ref(ResultFrame)\n    Store--&gt;&gt;Exec: ArtifactRef\n    Exec--&gt;&gt;Agg: ArtifactRef\n    Agg-&gt;&gt;Agg: load_scan / combine / post_op\n    Agg--&gt;&gt;Exec: aggregated rows</code></pre> <p>Aggregation uses <code>AggregationService(PolarsDuckdbEngine())</code> and executes layers sequentially.</p>"},{"location":"architecture/pipeline/#state-model","title":"State model","text":"<p>Shared and subgraph state models are defined in <code>graph_state.md</code>. The pipeline uses:</p> <ul> <li><code>GraphState</code> for control graph execution and aggregation.</li> <li><code>SubgraphExecutionState</code> for subgraph-local execution.</li> <li><code>wrap_subgraph()</code> to merge subgraph outputs back into <code>GraphState</code>.</li> </ul>"},{"location":"architecture/pipeline/#determinism","title":"Determinism","text":"<p>Determinism guarantees and non-determinism sources are centralized in <code>determinism.md</code>.</p>"},{"location":"architecture/pipeline/#known-limitations-current","title":"Known limitations (current)","text":"<ul> <li><code>PhysicalValidatorNode</code> is implemented but not wired into the SQL agent graph.</li> <li>Execution sandbox exists but SQL executor runs in-process by default.</li> <li>Circuit breakers are not uniformly wired across all execution paths.</li> </ul>"},{"location":"architecture/pipeline/#observability-hooks-current","title":"Observability hooks (current)","text":"<p>Observability stack and callback behavior are documented in <code>../observability/stack.md</code>.</p>"},{"location":"architecture/pipeline/#source-references","title":"Source references","text":"<ul> <li>Graph builder: <code>packages/core/src/nl2sql/pipeline/graph.py</code></li> <li>Runtime: <code>packages/core/src/nl2sql/pipeline/runtime.py</code></li> <li>Routing: <code>packages/core/src/nl2sql/pipeline/routes.py</code></li> <li>Subgraph wrapper: <code>packages/core/src/nl2sql/pipeline/graph_utils.py</code></li> <li>SQL agent graph: <code>packages/core/src/nl2sql/pipeline/subgraphs/sql_agent.py</code></li> <li>Execution DAG models: <code>packages/core/src/nl2sql/pipeline/nodes/global_planner/schemas.py</code></li> <li>Aggregation service: <code>packages/core/src/nl2sql/aggregation/aggregator.py</code></li> </ul>"},{"location":"architecture/nodes/","title":"Node Catalog","text":"<p>This section indexes all control-graph nodes and SQL agent subgraph nodes. Each node has a dedicated reference page with inputs, outputs, contracts, and limitations.</p> <p>For pipeline-level wiring, see <code>../pipeline.md</code>. For subgraph wiring and lifecycle, see <code>../subgraphs/sql_agent.md</code>.</p>"},{"location":"architecture/nodes/#control-graph-nodes-main-pipeline","title":"Control graph nodes (main pipeline)","text":"<ul> <li>DatasourceResolverNode</li> <li>DecomposerNode</li> <li>GlobalPlannerNode</li> <li>EngineAggregatorNode</li> <li>AnswerSynthesizerNode</li> </ul>"},{"location":"architecture/nodes/#sql-agent-subgraph-nodes","title":"SQL agent subgraph nodes","text":"<ul> <li>SchemaRetrieverNode</li> <li>ASTPlannerNode</li> <li>LogicalValidatorNode</li> <li>PhysicalValidatorNode</li> <li>GeneratorNode</li> <li>ExecutorNode</li> <li>RefinerNode</li> </ul>"},{"location":"architecture/nodes/answer_synthesizer_node/","title":"AnswerSynthesizerNode","text":""},{"location":"architecture/nodes/answer_synthesizer_node/#overview","title":"Overview","text":"<ul> <li>Summarizes aggregated results into a user\u2011facing response using an LLM.</li> <li>Exists to convert structured results into human\u2011readable output.</li> <li>Sits after <code>EngineAggregatorNode</code> at the end of the control graph.</li> <li>Class: <code>AnswerSynthesizerNode</code></li> <li>Source: <code>packages/core/src/nl2sql/pipeline/nodes/answer_synthesizer/node.py</code></li> </ul>"},{"location":"architecture/nodes/answer_synthesizer_node/#responsibilities","title":"Responsibilities","text":"<ul> <li>Serialize aggregated results for prompt input.</li> <li>Include unmapped sub\u2011queries in the response context.</li> <li>Invoke LLM with structured output schema.</li> </ul>"},{"location":"architecture/nodes/answer_synthesizer_node/#position-in-execution-graph","title":"Position in Execution Graph","text":"<p>Upstream: - <code>EngineAggregatorNode</code></p> <p>Downstream: - Graph end.</p> <p>Trigger conditions: - Executed after aggregation completes.</p> <pre><code>flowchart LR\n    Aggregator[EngineAggregatorNode] --&gt; Synthesizer[AnswerSynthesizerNode] --&gt; EndNode[PipelineEnd]</code></pre>"},{"location":"architecture/nodes/answer_synthesizer_node/#inputs","title":"Inputs","text":"<p>From <code>GraphState</code>:</p> <ul> <li><code>user_query</code> (required)</li> <li><code>aggregator_response.terminal_results</code> (required)</li> <li><code>decomposer_response.unmapped_subqueries</code> (optional)</li> </ul> <p>Validation performed:</p> <ul> <li>If no aggregated result is available, emits <code>INVALID_STATE</code>.</li> </ul>"},{"location":"architecture/nodes/answer_synthesizer_node/#outputs","title":"Outputs","text":"<p>Mutations to <code>GraphState</code>:</p> <ul> <li><code>answer_synthesizer_response</code> (<code>AnswerSynthesizerResponse</code>)</li> <li><code>reasoning</code> summary</li> <li><code>errors</code> on failure</li> </ul> <p>Side effects:</p> <ul> <li>LLM invocation.</li> </ul>"},{"location":"architecture/nodes/answer_synthesizer_node/#internal-flow-step-by-step","title":"Internal Flow (Step-by-Step)","text":"<ol> <li>Resolve aggregated result from <code>aggregator_response</code>.</li> <li>If missing, emit <code>INVALID_STATE</code> error and stop.</li> <li>Serialize results to JSON.</li> <li>Serialize unmapped sub\u2011queries to JSON.</li> <li>Invoke LLM chain with structured output.</li> <li>Return <code>AnswerSynthesizerResponse</code>.</li> <li>On exception, emit <code>AGGREGATOR_FAILED</code>.</li> </ol>"},{"location":"architecture/nodes/answer_synthesizer_node/#contracts-interfaces","title":"Contracts &amp; Interfaces","text":"<p>Implements a LangGraph node callable:</p> <pre><code>def __call__(self, state: GraphState) -&gt; Dict[str, Any]\n</code></pre> <p>Key contracts:</p> <ul> <li><code>AggregatedResponse</code></li> <li><code>AnswerSynthesizerResponse</code></li> </ul>"},{"location":"architecture/nodes/answer_synthesizer_node/#determinism-guarantees","title":"Determinism Guarantees","text":"<ul> <li>Non\u2011deterministic unless LLM configured deterministically.</li> </ul>"},{"location":"architecture/nodes/answer_synthesizer_node/#error-handling","title":"Error Handling","text":"<p>Emits <code>PipelineError</code> with:</p> <ul> <li><code>INVALID_STATE</code> (no aggregated result)</li> <li><code>AGGREGATOR_FAILED</code> (LLM failure)</li> </ul> <p>Logs failures via <code>logger.error</code>.</p>"},{"location":"architecture/nodes/answer_synthesizer_node/#retry-idempotency","title":"Retry + Idempotency","text":"<ul> <li>No internal retry logic.</li> <li>Idempotency depends on LLM determinism.</li> </ul>"},{"location":"architecture/nodes/answer_synthesizer_node/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>LLM call dominates latency and cost.</li> <li>Serialization of aggregated results is in\u2011memory.</li> </ul>"},{"location":"architecture/nodes/answer_synthesizer_node/#observability","title":"Observability","text":"<ul> <li>Logger: <code>answer_synthesizer</code></li> <li>Adds reasoning summary to state.</li> </ul>"},{"location":"architecture/nodes/answer_synthesizer_node/#configuration","title":"Configuration","text":"<ul> <li>LLM configuration for agent name <code>answer_synthesizer</code> via <code>llm.yaml</code>.</li> </ul>"},{"location":"architecture/nodes/answer_synthesizer_node/#extension-points","title":"Extension Points","text":"<ul> <li>Modify <code>ANSWER_SYNTHESIZER_PROMPT</code>.</li> <li>Replace node in <code>build_graph()</code> for custom synthesis.</li> </ul>"},{"location":"architecture/nodes/answer_synthesizer_node/#known-limitations","title":"Known Limitations","text":"<ul> <li>No deterministic summarization guarantees without LLM configuration.</li> <li>No fallback response when LLM fails.</li> </ul>"},{"location":"architecture/nodes/answer_synthesizer_node/#related-code","title":"Related Code","text":"<ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/answer_synthesizer/node.py</code></li> </ul>"},{"location":"architecture/nodes/ast_planner_node/","title":"ASTPlannerNode","text":""},{"location":"architecture/nodes/ast_planner_node/#overview","title":"Overview","text":"<ul> <li>Generates a structured SQL AST plan (<code>PlanModel</code>) using an LLM with structured output.</li> <li>Exists to translate natural language intent into a deterministic, machine\u2011verifiable plan.</li> <li>Sits after <code>SchemaRetrieverNode</code> in the SQL agent subgraph.</li> <li>Class: <code>ASTPlannerNode</code></li> <li>Source: <code>packages/core/src/nl2sql/pipeline/nodes/ast_planner/node.py</code></li> </ul>"},{"location":"architecture/nodes/ast_planner_node/#responsibilities","title":"Responsibilities","text":"<ul> <li>Serialize <code>relevant_tables</code> into the planning prompt.</li> <li>Pass user intent, expected schema, and error feedback to the LLM.</li> <li>Return <code>ASTPlannerResponse</code> with the <code>PlanModel</code>.</li> </ul>"},{"location":"architecture/nodes/ast_planner_node/#position-in-execution-graph","title":"Position in Execution Graph","text":"<p>Upstream: - <code>SchemaRetrieverNode</code> - <code>RefinerNode</code> (retry loop)</p> <p>Downstream: - <code>LogicalValidatorNode</code> on success - <code>retry_handler</code> on retryable failure</p> <p>Trigger conditions: - Executed after schema retrieval and during retry loops.</p> <pre><code>flowchart LR\n    SchemaRetriever[SchemaRetrieverNode] --&gt; Planner[ASTPlannerNode] --&gt; Validator[LogicalValidatorNode]\n    Refiner[RefinerNode] --&gt; Planner\n    Planner --&gt; RetryHandler[retry_handler]</code></pre>"},{"location":"architecture/nodes/ast_planner_node/#inputs","title":"Inputs","text":"<p>From <code>SubgraphExecutionState</code>:</p> <ul> <li><code>sub_query.intent</code> (required)</li> <li><code>sub_query.expected_schema</code> (optional)</li> <li><code>relevant_tables</code> (required for schema grounding)</li> <li><code>errors</code> (optional feedback for retries)</li> </ul> <p>Validation performed:</p> <ul> <li>No explicit validation; relies on LLM structured output schema.</li> </ul>"},{"location":"architecture/nodes/ast_planner_node/#outputs","title":"Outputs","text":"<p>Mutations to <code>SubgraphExecutionState</code>:</p> <ul> <li><code>ast_planner_response</code> (<code>ASTPlannerResponse</code>)</li> <li><code>reasoning</code> with plan summary</li> <li><code>errors</code> on planning failure</li> </ul> <p>Side effects:</p> <ul> <li>LLM invocation via <code>llm_registry</code>.</li> </ul>"},{"location":"architecture/nodes/ast_planner_node/#internal-flow-step-by-step","title":"Internal Flow (Step-by-Step)","text":"<ol> <li>Serialize <code>relevant_tables</code> to JSON text.</li> <li>Build feedback string from existing errors.</li> <li>Build <code>expected_schema</code> payload from sub\u2011query.</li> <li>Invoke the LLM chain with prompt + structured output (<code>PlanModel</code>).</li> <li>Return <code>ASTPlannerResponse</code> with plan and reasoning.</li> <li>On exception, emit <code>PLANNING_FAILURE</code> error and return <code>plan=None</code>.</li> </ol>"},{"location":"architecture/nodes/ast_planner_node/#contracts-interfaces","title":"Contracts &amp; Interfaces","text":"<p>Implements a LangGraph node callable:</p> <pre><code>def __call__(self, state: SubgraphExecutionState) -&gt; Dict[str, Any]\n</code></pre> <p>Key contracts:</p> <ul> <li><code>PlanModel</code></li> <li><code>ASTPlannerResponse</code></li> </ul>"},{"location":"architecture/nodes/ast_planner_node/#determinism-guarantees","title":"Determinism Guarantees","text":"<ul> <li>Deterministic only if LLM is configured deterministically.</li> <li>Plan structure is validated downstream; ordering is not enforced here.</li> </ul>"},{"location":"architecture/nodes/ast_planner_node/#error-handling","title":"Error Handling","text":"<p>Emits <code>PipelineError</code> with:</p> <ul> <li><code>PLANNING_FAILURE</code> on LLM or parsing errors.</li> </ul> <p>Logs failures via <code>logger.exception</code>.</p>"},{"location":"architecture/nodes/ast_planner_node/#retry-idempotency","title":"Retry + Idempotency","text":"<ul> <li>Retries are orchestrated by the subgraph, not this node.</li> <li>Idempotency depends on LLM determinism.</li> </ul>"},{"location":"architecture/nodes/ast_planner_node/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>One LLM call per planning attempt.</li> <li>Cost is dominated by LLM latency and token usage.</li> </ul>"},{"location":"architecture/nodes/ast_planner_node/#observability","title":"Observability","text":"<ul> <li>Logger: <code>planner</code></li> <li>Adds reasoning entries to subgraph state.</li> </ul>"},{"location":"architecture/nodes/ast_planner_node/#configuration","title":"Configuration","text":"<ul> <li>LLM config under agent name <code>ast_planner</code> in <code>llm.yaml</code>.</li> </ul>"},{"location":"architecture/nodes/ast_planner_node/#extension-points","title":"Extension Points","text":"<ul> <li>Modify <code>PLANNER_PROMPT</code> and examples.</li> <li>Replace node in <code>build_sql_agent_graph()</code> to use alternate planning logic.</li> </ul>"},{"location":"architecture/nodes/ast_planner_node/#known-limitations","title":"Known Limitations","text":"<ul> <li>No deterministic safeguards beyond structured output schema.</li> <li>No explicit timeout handling in the node itself.</li> </ul>"},{"location":"architecture/nodes/ast_planner_node/#related-code","title":"Related Code","text":"<ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/ast_planner/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/ast_planner/schemas.py</code></li> </ul>"},{"location":"architecture/nodes/datasource_resolver_node/","title":"DatasourceResolverNode","text":""},{"location":"architecture/nodes/datasource_resolver_node/#overview","title":"Overview","text":"<ul> <li>Resolves candidate datasources from the user query using vector search or an explicit datasource override.</li> <li>Enforces RBAC and schema version mismatch policy before planning.</li> <li>Sits at the entry of the control graph and gates the pipeline.</li> <li>Class: <code>DatasourceResolverNode</code></li> <li>Source: <code>packages/core/src/nl2sql/pipeline/nodes/datasource_resolver/node.py</code></li> </ul>"},{"location":"architecture/nodes/datasource_resolver_node/#responsibilities","title":"Responsibilities","text":"<ul> <li>Resolve datasource candidates via <code>VectorStore.retrieve_datasource_candidates()</code>.</li> <li>Validate datasource IDs against the registered adapter list.</li> <li>Enforce RBAC datasource access.</li> <li>Detect schema version mismatches and apply configured policy.</li> <li>Populate <code>DatasourceResolverResponse</code> in <code>GraphState</code>.</li> </ul>"},{"location":"architecture/nodes/datasource_resolver_node/#position-in-execution-graph","title":"Position in Execution Graph","text":"<p>Upstream: - Entry point (no upstream nodes).</p> <p>Downstream: - <code>DecomposerNode</code> if resolution succeeds. - Terminates early if no allowed datasources.</p> <p>Trigger conditions: - Always executed as the control graph entry node.</p> <pre><code>flowchart LR\n    Resolver[DatasourceResolverNode] --&gt; Decomposer[DecomposerNode]</code></pre>"},{"location":"architecture/nodes/datasource_resolver_node/#inputs","title":"Inputs","text":"<p>From <code>GraphState</code>:</p> <ul> <li><code>user_query</code> (str, required): used for vector retrieval when no override provided.</li> <li><code>datasource_id</code> (Optional[str]): explicit override for datasource selection.</li> <li><code>user_context</code> (<code>UserContext</code>, required): RBAC enforcement.</li> </ul> <p>Context dependencies (<code>NL2SQLContext</code>):</p> <ul> <li><code>vector_store</code> (optional): if missing, resolver returns empty response.</li> <li><code>ds_registry</code>: adapter registry for supported IDs and capabilities.</li> <li><code>schema_store</code>: used to look up latest schema versions.</li> <li><code>rbac</code>: used to filter allowed datasources.</li> </ul> <p>Validation performed:</p> <ul> <li>Datasource override must exist in registry.</li> <li>RBAC must allow datasource.</li> <li>Version mismatch policy enforced via <code>settings.schema_version_mismatch_policy</code>.</li> </ul>"},{"location":"architecture/nodes/datasource_resolver_node/#outputs","title":"Outputs","text":"<p>Mutations to <code>GraphState</code>:</p> <ul> <li><code>datasource_resolver_response</code> (<code>DatasourceResolverResponse</code>)</li> <li><code>errors</code> (list of <code>PipelineError</code>) on failure</li> <li><code>reasoning</code> and <code>warnings</code> for diagnostics</li> </ul> <p>Side effects:</p> <ul> <li>None beyond vector store retrieval and registry lookups.</li> </ul>"},{"location":"architecture/nodes/datasource_resolver_node/#internal-flow-step-by-step","title":"Internal Flow (Step-by-Step)","text":"<ol> <li>If <code>state.datasource_id</code> is set, validate that it exists in the registry.</li> <li>If override is valid, check RBAC access; return error on violation.</li> <li>If no override and <code>vector_store</code> is missing, return empty response with reasoning.</li> <li>Retrieve datasource candidates with <code>VectorStore.retrieve_datasource_candidates()</code>.</li> <li>Convert candidate documents into <code>ResolvedDatasource</code> entries.</li> <li>Filter to allowed datasources using RBAC.</li> <li>Apply schema version mismatch policy (<code>fail</code> or <code>warn</code>).</li> <li>Return <code>DatasourceResolverResponse</code> with resolved, allowed, and unsupported IDs.</li> <li>On exceptions, log and return <code>SCHEMA_RETRIEVAL_FAILED</code> error.</li> </ol>"},{"location":"architecture/nodes/datasource_resolver_node/#contracts-interfaces","title":"Contracts &amp; Interfaces","text":"<p>Implements a LangGraph node callable:</p> <pre><code>def __call__(self, state: GraphState) -&gt; Dict[str, Any]\n</code></pre> <p>Key contracts:</p> <ul> <li><code>DatasourceResolverResponse</code></li> <li><code>ResolvedDatasource</code></li> <li><code>PipelineError</code></li> </ul>"},{"location":"architecture/nodes/datasource_resolver_node/#determinism-guarantees","title":"Determinism Guarantees","text":"<ul> <li>Deterministic for a fixed <code>datasource_id</code> override.</li> <li>Non-deterministic ranking may occur due to vector retrieval behavior.</li> </ul>"},{"location":"architecture/nodes/datasource_resolver_node/#error-handling","title":"Error Handling","text":"<p>Emits <code>PipelineError</code> with:</p> <ul> <li><code>INVALID_STATE</code> (missing/unknown datasource override)</li> <li><code>SECURITY_VIOLATION</code> (RBAC denial)</li> <li><code>SCHEMA_RETRIEVAL_FAILED</code> (no candidates or exceptions)</li> </ul> <p>Exceptions are caught at the node boundary and logged with <code>logger.error</code>.</p>"},{"location":"architecture/nodes/datasource_resolver_node/#retry-idempotency","title":"Retry + Idempotency","text":"<ul> <li>No internal retry logic.</li> <li>Idempotent for the same input state, except vector retrieval ranking may vary.</li> </ul>"},{"location":"architecture/nodes/datasource_resolver_node/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>External dependency on vector store retrieval (MMR search).</li> <li>RBAC checks are in-memory operations.</li> <li>Schema version lookups are store reads.</li> </ul>"},{"location":"architecture/nodes/datasource_resolver_node/#observability","title":"Observability","text":"<ul> <li>Logger: <code>datasource_resolver</code></li> <li>Adds reasoning and warnings to <code>GraphState</code> for downstream diagnostics.</li> <li>Vector retrieval uses <code>VECTOR_BREAKER</code> at the vector store layer.</li> </ul>"},{"location":"architecture/nodes/datasource_resolver_node/#configuration","title":"Configuration","text":"<ul> <li><code>settings.schema_version_mismatch_policy</code> (<code>warn</code> or <code>fail</code>)</li> </ul>"},{"location":"architecture/nodes/datasource_resolver_node/#extension-points","title":"Extension Points","text":"<ul> <li>Replace this node in <code>build_graph()</code> to change resolution behavior.</li> <li>Extend retrieval by modifying the node or the vector store retrieval strategy.</li> </ul>"},{"location":"architecture/nodes/datasource_resolver_node/#known-limitations","title":"Known Limitations","text":"<ul> <li>Vector store absence results in an empty resolver response (no fallback).</li> <li>Tenant scoping is not implemented in vector retrieval.</li> <li>Override path calls <code>_get_latest_schema_version</code>, which is not defined in this class (would raise if executed as-is).</li> </ul>"},{"location":"architecture/nodes/datasource_resolver_node/#related-code","title":"Related Code","text":"<ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/datasource_resolver/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/datasource_resolver/schemas.py</code></li> </ul>"},{"location":"architecture/nodes/decomposer_node/","title":"DecomposerNode","text":""},{"location":"architecture/nodes/decomposer_node/#overview","title":"Overview","text":"<ul> <li>Decomposes the user query into datasource\u2011scoped sub\u2011queries and combination operations.</li> <li>Produces deterministic IDs for sub\u2011queries and post\u2011combine ops.</li> <li>Sits after <code>DatasourceResolverNode</code> and before <code>GlobalPlannerNode</code>.</li> <li>Class: <code>DecomposerNode</code></li> <li>Source: <code>packages/core/src/nl2sql/pipeline/nodes/decomposer/node.py</code></li> </ul>"},{"location":"architecture/nodes/decomposer_node/#responsibilities","title":"Responsibilities","text":"<ul> <li>Invoke the LLM to produce <code>DecomposerResponse</code>.</li> <li>Filter sub\u2011queries by resolved/allowed/unsupported datasources.</li> <li>Stabilize IDs using a hash of sub\u2011query content.</li> <li>Normalize and sort combine groups and post\u2011combine operations.</li> </ul>"},{"location":"architecture/nodes/decomposer_node/#position-in-execution-graph","title":"Position in Execution Graph","text":"<p>Upstream: - <code>DatasourceResolverNode</code></p> <p>Downstream: - <code>GlobalPlannerNode</code></p> <p>Trigger conditions: - Executed only when <code>resolver_route</code> returns <code>continue</code>.</p> <pre><code>flowchart LR\n    Resolver[DatasourceResolverNode] --&gt; Decomposer[DecomposerNode] --&gt; Planner[GlobalPlannerNode]</code></pre>"},{"location":"architecture/nodes/decomposer_node/#inputs","title":"Inputs","text":"<p>From <code>GraphState</code>:</p> <ul> <li><code>user_query</code> (str, required)</li> <li><code>datasource_resolver_response</code> (<code>DatasourceResolverResponse</code>, required)</li> </ul> <p>Validation performed:</p> <ul> <li>Raises if no resolved datasources are available.</li> <li>Filters out sub\u2011queries that reference missing, restricted, or unsupported datasources.</li> </ul>"},{"location":"architecture/nodes/decomposer_node/#outputs","title":"Outputs","text":"<p>Mutations to <code>GraphState</code>:</p> <ul> <li><code>decomposer_response</code> (<code>DecomposerResponse</code>)</li> <li><code>reasoning</code> (completion status)</li> <li><code>errors</code> on failure</li> </ul> <p>Side effects:</p> <ul> <li>None beyond LLM invocation.</li> </ul>"},{"location":"architecture/nodes/decomposer_node/#internal-flow-step-by-step","title":"Internal Flow (Step-by-Step)","text":"<ol> <li>Validate resolver response; raise if no resolved datasources.</li> <li>Build <code>resolved_payload</code> and <code>schema_version_map</code>.</li> <li>Invoke LLM chain with <code>user_query</code> and resolved datasources.</li> <li>For each LLM sub\u2011query:</li> <li>Validate datasource existence and RBAC allowance.</li> <li>Assign deterministic ID via <code>_stable_id()</code>.</li> <li>Attach schema version.</li> <li>Remap combine groups to stable sub\u2011query IDs.</li> <li>Assign deterministic IDs to post\u2011combine ops.</li> <li>Sort sub\u2011queries, combine groups, and post\u2011combine ops.</li> <li>Return <code>DecomposerResponse</code>.</li> <li>On exception, emit <code>ORCHESTRATOR_CRASH</code> error.</li> </ol>"},{"location":"architecture/nodes/decomposer_node/#contracts-interfaces","title":"Contracts &amp; Interfaces","text":"<p>Implements a LangGraph node callable:</p> <pre><code>def __call__(self, state: GraphState) -&gt; Dict[str, Any]\n</code></pre> <p>Key contracts:</p> <ul> <li><code>DecomposerResponse</code></li> <li><code>SubQuery</code></li> <li><code>UnmappedSubQuery</code></li> <li><code>PostCombineOp</code></li> </ul>"},{"location":"architecture/nodes/decomposer_node/#determinism-guarantees","title":"Determinism Guarantees","text":"<ul> <li>Stable IDs are generated via SHA\u2011256 hash of sub\u2011query content.</li> <li>Ordering of outputs is explicitly sorted.</li> <li>LLM output is non\u2011deterministic unless configured deterministically.</li> </ul>"},{"location":"architecture/nodes/decomposer_node/#error-handling","title":"Error Handling","text":"<p>Emits <code>PipelineError</code> with:</p> <ul> <li><code>ORCHESTRATOR_CRASH</code> on exceptions.</li> </ul> <p>Logs failures via <code>logger.error</code>.</p>"},{"location":"architecture/nodes/decomposer_node/#retry-idempotency","title":"Retry + Idempotency","text":"<ul> <li>No internal retry logic.</li> <li>Idempotent for fixed LLM output; otherwise LLM variability can change output.</li> </ul>"},{"location":"architecture/nodes/decomposer_node/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>One LLM call per pipeline execution.</li> <li>Hashing and sorting are in\u2011memory and low cost.</li> </ul>"},{"location":"architecture/nodes/decomposer_node/#observability","title":"Observability","text":"<ul> <li>Logger: <code>decomposer</code></li> <li>Adds a reasoning entry on completion or failure.</li> </ul>"},{"location":"architecture/nodes/decomposer_node/#configuration","title":"Configuration","text":"<ul> <li>LLM configuration for agent name <code>decomposer</code> via <code>llm.yaml</code>.</li> </ul>"},{"location":"architecture/nodes/decomposer_node/#extension-points","title":"Extension Points","text":"<ul> <li>Modify <code>DECOMPOSER_PROMPT</code> for different decomposition strategies.</li> <li>Replace or wrap node in <code>build_graph()</code> for custom behavior.</li> </ul>"},{"location":"architecture/nodes/decomposer_node/#known-limitations","title":"Known Limitations","text":"<ul> <li>No fallback when LLM fails.</li> <li>Output quality depends on LLM configuration and prompt.</li> </ul>"},{"location":"architecture/nodes/decomposer_node/#related-code","title":"Related Code","text":"<ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/decomposer/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/decomposer/schemas.py</code></li> </ul>"},{"location":"architecture/nodes/engine_aggregator_node/","title":"EngineAggregatorNode","text":""},{"location":"architecture/nodes/engine_aggregator_node/#overview","title":"Overview","text":"<ul> <li>Executes the <code>ExecutionDAG</code> over persisted artifacts using the aggregation engine.</li> <li>Exists to combine scan results deterministically into final result sets.</li> <li>Sits after <code>layer_router</code> and before <code>AnswerSynthesizerNode</code>.</li> <li>Class: <code>EngineAggregatorNode</code></li> <li>Source: <code>packages/core/src/nl2sql/pipeline/nodes/aggregator/node.py</code></li> </ul>"},{"location":"architecture/nodes/engine_aggregator_node/#responsibilities","title":"Responsibilities","text":"<ul> <li>Load scan artifacts from <code>artifact_refs</code>.</li> <li>Execute combine and post\u2011operation nodes in DAG order.</li> <li>Produce <code>AggregatorResponse</code> with terminal results.</li> </ul>"},{"location":"architecture/nodes/engine_aggregator_node/#position-in-execution-graph","title":"Position in Execution Graph","text":"<p>Upstream: - <code>layer_router</code> (after all scan artifacts are available)</p> <p>Downstream: - <code>AnswerSynthesizerNode</code></p> <p>Trigger conditions: - Router dispatches to aggregator when no pending scan nodes remain.</p> <pre><code>flowchart LR\n    Router[layer_router] --&gt; Aggregator[EngineAggregatorNode] --&gt; Synth[AnswerSynthesizerNode]</code></pre>"},{"location":"architecture/nodes/engine_aggregator_node/#inputs","title":"Inputs","text":"<p>From <code>GraphState</code>:</p> <ul> <li><code>global_planner_response.execution_dag</code> (required)</li> <li><code>artifact_refs</code> (required)</li> </ul> <p>Validation performed:</p> <ul> <li>None in node; aggregation service raises on missing artifacts.</li> </ul>"},{"location":"architecture/nodes/engine_aggregator_node/#outputs","title":"Outputs","text":"<p>Mutations to <code>GraphState</code>:</p> <ul> <li><code>aggregator_response</code> (<code>AggregatorResponse</code>)</li> <li><code>reasoning</code> and <code>errors</code></li> </ul> <p>Side effects:</p> <ul> <li>Reads artifacts from artifact store.</li> </ul>"},{"location":"architecture/nodes/engine_aggregator_node/#internal-flow-step-by-step","title":"Internal Flow (Step-by-Step)","text":"<ol> <li>Read <code>execution_dag</code> and <code>artifact_refs</code>.</li> <li>Invoke <code>AggregationService.execute(dag, artifact_refs)</code>.</li> <li>Build <code>AggregatorResponse</code> with <code>terminal_results</code>.</li> <li>Return success reasoning.</li> <li>On exception, emit <code>AGGREGATOR_FAILED</code>.</li> </ol>"},{"location":"architecture/nodes/engine_aggregator_node/#contracts-interfaces","title":"Contracts &amp; Interfaces","text":"<p>Implements a LangGraph node callable:</p> <pre><code>def __call__(self, state: GraphState) -&gt; Dict[str, Any]\n</code></pre> <p>Key contracts:</p> <ul> <li><code>AggregatorResponse</code></li> <li><code>ExecutionDAG</code></li> </ul>"},{"location":"architecture/nodes/engine_aggregator_node/#determinism-guarantees","title":"Determinism Guarantees","text":"<ul> <li>Deterministic for a fixed DAG and artifact inputs.</li> <li>Aggregation order follows DAG layers and ordered inputs.</li> </ul>"},{"location":"architecture/nodes/engine_aggregator_node/#error-handling","title":"Error Handling","text":"<p>Emits <code>PipelineError</code> with:</p> <ul> <li><code>AGGREGATOR_FAILED</code></li> </ul> <p>Logs failures via <code>logger.error</code>.</p>"},{"location":"architecture/nodes/engine_aggregator_node/#retry-idempotency","title":"Retry + Idempotency","text":"<ul> <li>No internal retry logic.</li> <li>Idempotent for a fixed DAG and artifacts.</li> </ul>"},{"location":"architecture/nodes/engine_aggregator_node/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Reads Parquet artifacts and executes joins/aggregations in Polars/DuckDB.</li> <li>Cost grows with artifact size and DAG complexity.</li> </ul>"},{"location":"architecture/nodes/engine_aggregator_node/#observability","title":"Observability","text":"<ul> <li>Logger: <code>aggregator</code></li> <li>Adds reasoning entries for success and failure.</li> </ul>"},{"location":"architecture/nodes/engine_aggregator_node/#configuration","title":"Configuration","text":"<ul> <li>Uses <code>PolarsDuckdbEngine</code>; no direct settings consumed by the node.</li> </ul>"},{"location":"architecture/nodes/engine_aggregator_node/#extension-points","title":"Extension Points","text":"<ul> <li>Replace aggregation engine by modifying node to use a different <code>AggregationService</code>.</li> <li>Replace node in <code>build_graph()</code> for alternate aggregation behavior.</li> </ul>"},{"location":"architecture/nodes/engine_aggregator_node/#known-limitations","title":"Known Limitations","text":"<ul> <li>Fails if required artifacts are missing.</li> <li>No streaming or partial aggregation.</li> </ul>"},{"location":"architecture/nodes/engine_aggregator_node/#related-code","title":"Related Code","text":"<ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/aggregator/node.py</code></li> <li><code>packages/core/src/nl2sql/aggregation/aggregator.py</code></li> </ul>"},{"location":"architecture/nodes/executor_node/","title":"ExecutorNode","text":""},{"location":"architecture/nodes/executor_node/#overview","title":"Overview","text":"<ul> <li>Delegates SQL execution to an executor service based on datasource capabilities.</li> <li>Persists results via the artifact store and returns <code>ExecutorResponse</code>.</li> <li>Sits after <code>GeneratorNode</code> in the SQL agent subgraph.</li> <li>Class: <code>ExecutorNode</code></li> <li>Source: <code>packages/core/src/nl2sql/pipeline/nodes/executor/node.py</code></li> </ul>"},{"location":"architecture/nodes/executor_node/#responsibilities","title":"Responsibilities","text":"<ul> <li>Validate required execution inputs (SQL and datasource ID).</li> <li>Resolve executor via <code>ExecutorRegistry</code>.</li> <li>Build and submit <code>ExecutorRequest</code>.</li> <li>Return executor response and propagate errors.</li> </ul>"},{"location":"architecture/nodes/executor_node/#position-in-execution-graph","title":"Position in Execution Graph","text":"<p>Upstream: - <code>GeneratorNode</code></p> <p>Downstream: - Subgraph end (artifacts returned to control graph).</p> <p>Trigger conditions: - Executed when SQL generation succeeds.</p> <pre><code>flowchart LR\n    Generator[GeneratorNode] --&gt; Executor[ExecutorNode] --&gt; EndNode[SubgraphEnd]</code></pre>"},{"location":"architecture/nodes/executor_node/#inputs","title":"Inputs","text":"<p>From <code>SubgraphExecutionState</code>:</p> <ul> <li><code>generator_response.sql_draft</code> (required)</li> <li><code>sub_query.datasource_id</code> (required)</li> <li><code>sub_query.schema_version</code> (optional)</li> <li><code>trace_id</code>, <code>subgraph_name</code>, <code>user_context</code></li> </ul> <p>From <code>NL2SQLContext</code>:</p> <ul> <li><code>ds_registry</code></li> <li><code>tenant_id</code></li> </ul> <p>Validation performed:</p> <ul> <li>Missing SQL \u2192 <code>MISSING_SQL</code></li> <li>Missing datasource ID \u2192 <code>MISSING_DATASOURCE_ID</code></li> <li>No executor for datasource \u2192 <code>INVALID_STATE</code></li> </ul>"},{"location":"architecture/nodes/executor_node/#outputs","title":"Outputs","text":"<p>Mutations to <code>SubgraphExecutionState</code>:</p> <ul> <li><code>executor_response</code> (<code>ExecutorResponse</code>)</li> <li><code>errors</code> and <code>reasoning</code></li> </ul> <p>Side effects:</p> <ul> <li>Executes SQL against datasource via executor service.</li> <li>Persists results to artifact store.</li> </ul>"},{"location":"architecture/nodes/executor_node/#internal-flow-step-by-step","title":"Internal Flow (Step-by-Step)","text":"<ol> <li>Validate SQL and datasource ID.</li> <li>Resolve executor via <code>ExecutorRegistry</code>.</li> <li>Build <code>ExecutorRequest</code> with trace/subgraph identifiers and schema version.</li> <li>Execute request and return <code>ExecutorResponse</code>.</li> <li>On exception, emit <code>EXECUTOR_CRASH</code>.</li> </ol>"},{"location":"architecture/nodes/executor_node/#contracts-interfaces","title":"Contracts &amp; Interfaces","text":"<p>Implements a LangGraph node callable:</p> <pre><code>def __call__(self, state: SubgraphExecutionState) -&gt; Dict[str, Any]\n</code></pre> <p>Key contracts:</p> <ul> <li><code>ExecutorRequest</code></li> <li><code>ExecutorResponse</code></li> </ul>"},{"location":"architecture/nodes/executor_node/#determinism-guarantees","title":"Determinism Guarantees","text":"<ul> <li>Deterministic request construction.</li> <li>Execution results depend on external datasource state and are not deterministic.</li> </ul>"},{"location":"architecture/nodes/executor_node/#error-handling","title":"Error Handling","text":"<p>Emits <code>PipelineError</code> with:</p> <ul> <li><code>MISSING_SQL</code></li> <li><code>MISSING_DATASOURCE_ID</code></li> <li><code>INVALID_STATE</code></li> <li><code>EXECUTOR_CRASH</code></li> </ul>"},{"location":"architecture/nodes/executor_node/#retry-idempotency","title":"Retry + Idempotency","text":"<ul> <li>No internal retry logic.</li> <li>Idempotency depends on datasource and query semantics.</li> </ul>"},{"location":"architecture/nodes/executor_node/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>External I/O dominates (database execution).</li> <li>Artifact persistence writes Parquet results.</li> </ul>"},{"location":"architecture/nodes/executor_node/#observability","title":"Observability","text":"<ul> <li>Logger: <code>executor</code></li> <li>Propagates executor response reasoning and errors.</li> </ul>"},{"location":"architecture/nodes/executor_node/#configuration","title":"Configuration","text":"<ul> <li><code>tenant_id</code> from settings.</li> <li>Executor selection based on adapter capabilities.</li> </ul>"},{"location":"architecture/nodes/executor_node/#extension-points","title":"Extension Points","text":"<ul> <li>Register new executors in <code>ExecutorRegistry</code>.</li> <li>Replace node in <code>build_sql_agent_graph()</code> to alter execution behavior.</li> </ul>"},{"location":"architecture/nodes/executor_node/#known-limitations","title":"Known Limitations","text":"<ul> <li>Executes in\u2011process by default (sandbox not used in SQL executor).</li> <li>No built\u2011in retries on transient execution failures.</li> </ul>"},{"location":"architecture/nodes/executor_node/#related-code","title":"Related Code","text":"<ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/executor/node.py</code></li> <li><code>packages/core/src/nl2sql/execution/executor/sql_executor.py</code></li> </ul>"},{"location":"architecture/nodes/generator_node/","title":"GeneratorNode","text":""},{"location":"architecture/nodes/generator_node/#overview","title":"Overview","text":"<ul> <li>Converts the AST plan into SQL using <code>sqlglot</code>.</li> <li>Enforces adapter row limits and dialect selection.</li> <li>Sits between <code>LogicalValidatorNode</code> and <code>ExecutorNode</code>.</li> <li>Class: <code>GeneratorNode</code></li> <li>Source: <code>packages/core/src/nl2sql/pipeline/nodes/generator/node.py</code></li> </ul>"},{"location":"architecture/nodes/generator_node/#responsibilities","title":"Responsibilities","text":"<ul> <li>Convert <code>PlanModel</code> expressions into <code>sqlglot</code> expressions.</li> <li>Build a SQL query with ordered selects, joins, filters, groupings, and limits.</li> <li>Apply adapter dialect and row limit.</li> </ul>"},{"location":"architecture/nodes/generator_node/#position-in-execution-graph","title":"Position in Execution Graph","text":"<p>Upstream: - <code>LogicalValidatorNode</code></p> <p>Downstream: - <code>ExecutorNode</code></p> <p>Trigger conditions: - Executed when logical validation passes.</p> <pre><code>flowchart LR\n    Validator[LogicalValidatorNode] --&gt; Generator[GeneratorNode] --&gt; Executor[ExecutorNode]</code></pre>"},{"location":"architecture/nodes/generator_node/#inputs","title":"Inputs","text":"<p>From <code>SubgraphExecutionState</code>:</p> <ul> <li><code>ast_planner_response.plan</code> (required)</li> <li><code>sub_query.datasource_id</code> (required)</li> </ul> <p>From <code>NL2SQLContext</code>:</p> <ul> <li><code>ds_registry</code> (adapter dialect and row limits)</li> </ul> <p>Validation performed:</p> <ul> <li>Raises if datasource ID or plan is missing.</li> </ul>"},{"location":"architecture/nodes/generator_node/#outputs","title":"Outputs","text":"<p>Mutations to <code>SubgraphExecutionState</code>:</p> <ul> <li><code>generator_response</code> (<code>GeneratorResponse</code> with <code>sql_draft</code>)</li> <li><code>reasoning</code></li> <li><code>errors</code> on failure</li> </ul> <p>Side effects:</p> <ul> <li>Adapter registry access for dialect/limits.</li> </ul>"},{"location":"architecture/nodes/generator_node/#internal-flow-step-by-step","title":"Internal Flow (Step-by-Step)","text":"<ol> <li>Validate presence of datasource ID and plan.</li> <li>Resolve adapter and dialect.</li> <li>Compute effective limit from plan/adapter row limit.</li> <li>Traverse AST with <code>SqlVisitor</code> to build <code>sqlglot</code> expression tree.</li> <li>Render SQL with <code>query.sql(dialect=...)</code>.</li> <li>Return <code>GeneratorResponse</code> with SQL and reasoning.</li> <li>On exception, emit <code>SQL_GEN_FAILED</code>.</li> </ol>"},{"location":"architecture/nodes/generator_node/#contracts-interfaces","title":"Contracts &amp; Interfaces","text":"<p>Implements a LangGraph node callable:</p> <pre><code>def __call__(self, state: SubgraphExecutionState) -&gt; Dict[str, Any]\n</code></pre> <p>Key contracts:</p> <ul> <li><code>GeneratorResponse</code></li> <li><code>PlanModel</code> / <code>Expr</code></li> </ul>"},{"location":"architecture/nodes/generator_node/#determinism-guarantees","title":"Determinism Guarantees","text":"<ul> <li>Deterministic for a fixed AST and adapter dialect.</li> <li>Ordering is enforced via ordinals and sorted lists.</li> </ul>"},{"location":"architecture/nodes/generator_node/#error-handling","title":"Error Handling","text":"<p>Emits <code>PipelineError</code> with:</p> <ul> <li><code>SQL_GEN_FAILED</code></li> </ul> <p>Logs exceptions via <code>logger.exception</code>.</p>"},{"location":"architecture/nodes/generator_node/#retry-idempotency","title":"Retry + Idempotency","text":"<ul> <li>No internal retry logic.</li> <li>Idempotent for a fixed plan.</li> </ul>"},{"location":"architecture/nodes/generator_node/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>In\u2011memory AST traversal and SQL rendering.</li> <li>Cost grows with AST size.</li> </ul>"},{"location":"architecture/nodes/generator_node/#observability","title":"Observability","text":"<ul> <li>Logger: <code>generator</code></li> <li>Adds reasoning entries with the generated SQL.</li> </ul>"},{"location":"architecture/nodes/generator_node/#configuration","title":"Configuration","text":"<ul> <li>Adapter <code>row_limit</code> and <code>dialect</code> from datasource config.</li> </ul>"},{"location":"architecture/nodes/generator_node/#extension-points","title":"Extension Points","text":"<ul> <li>Extend <code>SqlVisitor</code> for new expression kinds.</li> <li>Replace node in <code>build_sql_agent_graph()</code> for alternate generation strategies.</li> </ul>"},{"location":"architecture/nodes/generator_node/#known-limitations","title":"Known Limitations","text":"<ul> <li>No SQL optimization beyond AST structure.</li> <li>No dialect fallback if adapter misconfigured.</li> </ul>"},{"location":"architecture/nodes/generator_node/#related-code","title":"Related Code","text":"<ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/generator/node.py</code></li> </ul>"},{"location":"architecture/nodes/global_planner_node/","title":"GlobalPlannerNode","text":""},{"location":"architecture/nodes/global_planner_node/#overview","title":"Overview","text":"<ul> <li>Builds a deterministic <code>ExecutionDAG</code> from decomposer output.</li> <li>Creates scan, combine, and post\u2011combine nodes with explicit edges.</li> <li>Sits between <code>DecomposerNode</code> and <code>layer_router</code>.</li> <li>Class: <code>GlobalPlannerNode</code></li> <li>Source: <code>packages/core/src/nl2sql/pipeline/nodes/global_planner/node.py</code></li> </ul>"},{"location":"architecture/nodes/global_planner_node/#responsibilities","title":"Responsibilities","text":"<ul> <li>Translate <code>SubQuery</code> objects into <code>scan</code> nodes.</li> <li>Construct <code>combine</code> nodes for join/union/compare groups.</li> <li>Construct <code>post_*</code> nodes for filter/aggregate/project/sort/limit operations.</li> <li>Create edges between nodes and validate references.</li> <li>Compute deterministic <code>ExecutionDAG</code> ordering, content hash, and dag_id.</li> </ul>"},{"location":"architecture/nodes/global_planner_node/#position-in-execution-graph","title":"Position in Execution Graph","text":"<p>Upstream: - <code>DecomposerNode</code></p> <p>Downstream: - <code>layer_router</code></p> <p>Trigger conditions: - Always executed after successful decomposition.</p> <pre><code>flowchart LR\n    Decomposer[DecomposerNode] --&gt; Planner[GlobalPlannerNode] --&gt; Router[layer_router]</code></pre>"},{"location":"architecture/nodes/global_planner_node/#inputs","title":"Inputs","text":"<p>From <code>GraphState</code>:</p> <ul> <li><code>decomposer_response.sub_queries</code></li> <li><code>decomposer_response.combine_groups</code></li> <li><code>decomposer_response.post_combine_ops</code></li> </ul> <p>Validation performed:</p> <ul> <li>Validates edges reference existing node IDs.</li> <li>Fails if post\u2011combine ops reference unknown combine groups.</li> </ul>"},{"location":"architecture/nodes/global_planner_node/#outputs","title":"Outputs","text":"<p>Mutations to <code>GraphState</code>:</p> <ul> <li><code>global_planner_response</code> (<code>GlobalPlannerResponse</code> with <code>ExecutionDAG</code>)</li> <li><code>reasoning</code> on success</li> <li><code>errors</code> on failure</li> </ul> <p>Side effects:</p> <ul> <li>None beyond in\u2011memory DAG construction.</li> </ul>"},{"location":"architecture/nodes/global_planner_node/#internal-flow-step-by-step","title":"Internal Flow (Step-by-Step)","text":"<ol> <li>Extract <code>sub_queries</code>, <code>combine_groups</code>, <code>post_combine_ops</code>.</li> <li>For each sub\u2011query, build a <code>scan</code> <code>LogicalNode</code> with <code>RelationSchema</code> from expected schema.</li> <li>For each combine group, create a <code>combine</code> node and edges from inputs.</li> <li>For each post\u2011combine op, create a <code>post_*</code> node and edge from its target combine.</li> <li>Validate that all edges reference known nodes.</li> <li>Build <code>ExecutionDAG</code> with sorted nodes/edges.</li> <li>Compute <code>content_hash</code> and <code>dag_id</code>.</li> <li>Return <code>GlobalPlannerResponse</code>.</li> <li>On exception, emit <code>PLANNER_FAILED</code> error.</li> </ol>"},{"location":"architecture/nodes/global_planner_node/#contracts-interfaces","title":"Contracts &amp; Interfaces","text":"<p>Implements a LangGraph node callable:</p> <pre><code>def __call__(self, state: GraphState) -&gt; Dict[str, Any]\n</code></pre> <p>Key contracts:</p> <ul> <li><code>ExecutionDAG</code></li> <li><code>LogicalNode</code>, <code>LogicalEdge</code></li> <li><code>RelationSchema</code>, <code>ColumnSpec</code></li> <li><code>GlobalPlannerResponse</code></li> </ul>"},{"location":"architecture/nodes/global_planner_node/#determinism-guarantees","title":"Determinism Guarantees","text":"<ul> <li>Nodes and edges are sorted before DAG creation.</li> <li><code>ExecutionDAG._layered_toposort()</code> produces stable layer ordering.</li> <li><code>content_hash</code> is a deterministic hash of nodes/edges/version.</li> </ul>"},{"location":"architecture/nodes/global_planner_node/#error-handling","title":"Error Handling","text":"<p>Emits <code>PipelineError</code> with:</p> <ul> <li><code>PLANNER_FAILED</code> on exceptions</li> </ul> <p>Logs failures via <code>logger.error</code>.</p>"},{"location":"architecture/nodes/global_planner_node/#retry-idempotency","title":"Retry + Idempotency","text":"<ul> <li>No internal retry logic.</li> <li>Deterministic for a given decomposer response.</li> </ul>"},{"location":"architecture/nodes/global_planner_node/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Pure in\u2011memory graph construction and hashing.</li> <li>Complexity scales with number of sub\u2011queries and combine ops.</li> </ul>"},{"location":"architecture/nodes/global_planner_node/#observability","title":"Observability","text":"<ul> <li>Logger: <code>global_planner</code></li> <li>Adds a reasoning entry on success or failure.</li> </ul>"},{"location":"architecture/nodes/global_planner_node/#configuration","title":"Configuration","text":"<ul> <li>No direct settings read in this node.</li> </ul>"},{"location":"architecture/nodes/global_planner_node/#extension-points","title":"Extension Points","text":"<ul> <li>Extend DAG semantics by modifying <code>kind_map</code> and <code>attributes</code> mapping.</li> <li>Replace node in <code>build_graph()</code> for custom DAG construction.</li> </ul>"},{"location":"architecture/nodes/global_planner_node/#known-limitations","title":"Known Limitations","text":"<ul> <li>Assumes <code>expected_schema</code> is present to build <code>RelationSchema</code>.</li> <li>No schema validation at DAG construction time.</li> <li>No support for custom node kinds without code changes.</li> </ul>"},{"location":"architecture/nodes/global_planner_node/#related-code","title":"Related Code","text":"<ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/global_planner/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/global_planner/schemas.py</code></li> </ul>"},{"location":"architecture/nodes/logical_validator_node/","title":"LogicalValidatorNode","text":""},{"location":"architecture/nodes/logical_validator_node/#overview","title":"Overview","text":"<ul> <li>Validates the AST plan (<code>PlanModel</code>) against schema and RBAC policies.</li> <li>Exists to enforce correctness and security before SQL generation.</li> <li>Sits between <code>ASTPlannerNode</code> and <code>GeneratorNode</code> in the SQL agent subgraph.</li> <li>Class: <code>LogicalValidatorNode</code></li> <li>Source: <code>packages/core/src/nl2sql/pipeline/nodes/validator/node.py</code></li> </ul>"},{"location":"architecture/nodes/logical_validator_node/#responsibilities","title":"Responsibilities","text":"<ul> <li>Validate query type (READ\u2011only).</li> <li>Validate ordinals, aliases, joins, and column references.</li> <li>Enforce RBAC table access using strict datasource namespacing.</li> <li>Validate literal values against column stats (when available).</li> </ul>"},{"location":"architecture/nodes/logical_validator_node/#position-in-execution-graph","title":"Position in Execution Graph","text":"<p>Upstream: - <code>ASTPlannerNode</code></p> <p>Downstream: - <code>GeneratorNode</code> on success - <code>retry_handler</code> on retryable errors</p> <p>Trigger conditions: - Executed when an AST plan is present.</p> <pre><code>flowchart LR\n    Planner[ASTPlannerNode] --&gt; Validator[LogicalValidatorNode] --&gt; Generator[GeneratorNode]\n    Validator --&gt; RetryHandler[retry_handler]</code></pre>"},{"location":"architecture/nodes/logical_validator_node/#inputs","title":"Inputs","text":"<p>From <code>SubgraphExecutionState</code>:</p> <ul> <li><code>ast_planner_response.plan</code> (required)</li> <li><code>relevant_tables</code> (required)</li> <li><code>user_context</code> (required for RBAC)</li> <li><code>sub_query.datasource_id</code> (required for namespacing)</li> <li><code>sub_query.expected_schema</code> (optional validation)</li> </ul> <p>Validation performed:</p> <ul> <li>Query type must be <code>READ</code>.</li> <li>Ordinals must be contiguous.</li> <li>Aliases must be unique.</li> <li>Joins must match known relationships.</li> <li>Column references must exist and be unambiguous.</li> </ul>"},{"location":"architecture/nodes/logical_validator_node/#outputs","title":"Outputs","text":"<p>Mutations to <code>SubgraphExecutionState</code>:</p> <ul> <li><code>logical_validator_response</code> (<code>LogicalValidatorResponse</code>)</li> <li><code>errors</code> and <code>reasoning</code></li> </ul> <p>Side effects:</p> <ul> <li>None beyond in\u2011memory validation.</li> </ul>"},{"location":"architecture/nodes/logical_validator_node/#internal-flow-step-by-step","title":"Internal Flow (Step-by-Step)","text":"<ol> <li>If plan is missing, emit <code>MISSING_PLAN</code> and stop.</li> <li>Run <code>_validate_static()</code> for structural checks.</li> <li>Run <code>_validate_policy()</code> for RBAC enforcement.</li> <li>If any errors are <code>ERROR</code>/<code>CRITICAL</code>, return with errors.</li> <li>Otherwise return success reasoning.</li> <li>On exception, emit <code>VALIDATOR_CRASH</code>.</li> </ol>"},{"location":"architecture/nodes/logical_validator_node/#contracts-interfaces","title":"Contracts &amp; Interfaces","text":"<p>Implements a LangGraph node callable:</p> <pre><code>def __call__(self, state: SubgraphExecutionState) -&gt; Dict[str, Any]\n</code></pre> <p>Key contracts:</p> <ul> <li><code>LogicalValidatorResponse</code></li> <li><code>PipelineError</code></li> </ul>"},{"location":"architecture/nodes/logical_validator_node/#determinism-guarantees","title":"Determinism Guarantees","text":"<ul> <li>Deterministic for a fixed plan and schema snapshot.</li> <li>No randomness in validation logic.</li> </ul>"},{"location":"architecture/nodes/logical_validator_node/#error-handling","title":"Error Handling","text":"<p>Emits <code>PipelineError</code> with:</p> <ul> <li><code>MISSING_PLAN</code></li> <li><code>INVALID_PLAN_STRUCTURE</code></li> <li><code>SECURITY_VIOLATION</code></li> <li><code>TABLE_NOT_FOUND</code>, <code>COLUMN_NOT_FOUND</code></li> <li><code>JOIN_TABLE_NOT_IN_PLAN</code></li> <li><code>VALIDATOR_CRASH</code></li> </ul>"},{"location":"architecture/nodes/logical_validator_node/#retry-idempotency","title":"Retry + Idempotency","text":"<ul> <li>No internal retry logic.</li> <li>Retry decisions are made by the subgraph router based on error severity.</li> </ul>"},{"location":"architecture/nodes/logical_validator_node/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>In\u2011memory validation over AST and schema context.</li> <li>Complexity grows with AST size and number of tables/joins.</li> </ul>"},{"location":"architecture/nodes/logical_validator_node/#observability","title":"Observability","text":"<ul> <li>Logger: <code>logical_validator</code></li> <li>Emits reasoning entries and logs debug traces.</li> </ul>"},{"location":"architecture/nodes/logical_validator_node/#configuration","title":"Configuration","text":"<ul> <li><code>settings.logical_validator_strict_columns</code> controls severity of missing column errors.</li> </ul>"},{"location":"architecture/nodes/logical_validator_node/#extension-points","title":"Extension Points","text":"<ul> <li>Extend validation rules by modifying <code>_validate_static()</code> or <code>_validate_policy()</code>.</li> <li>Replace node in <code>build_sql_agent_graph()</code> for custom validation.</li> </ul>"},{"location":"architecture/nodes/logical_validator_node/#known-limitations","title":"Known Limitations","text":"<ul> <li>Validation relies on retrieved schema context; missing tables can cause false negatives.</li> <li>No cross\u2011schema disambiguation beyond plan inputs.</li> </ul>"},{"location":"architecture/nodes/logical_validator_node/#related-code","title":"Related Code","text":"<ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/validator/node.py</code></li> </ul>"},{"location":"architecture/nodes/refiner_node/","title":"RefinerNode","text":""},{"location":"architecture/nodes/refiner_node/#overview","title":"Overview","text":"<ul> <li>Generates corrective feedback for planning failures using an LLM.</li> <li>Exists to improve planner outputs in retry loops.</li> <li>Sits between <code>retry_handler</code> and <code>ASTPlannerNode</code> in the SQL agent subgraph.</li> <li>Class: <code>RefinerNode</code></li> <li>Source: <code>packages/core/src/nl2sql/pipeline/nodes/refiner/node.py</code></li> </ul>"},{"location":"architecture/nodes/refiner_node/#responsibilities","title":"Responsibilities","text":"<ul> <li>Summarize planning errors and reasoning context.</li> <li>Invoke LLM to generate feedback.</li> <li>Emit <code>PLAN_FEEDBACK</code> warnings to drive retry.</li> </ul>"},{"location":"architecture/nodes/refiner_node/#position-in-execution-graph","title":"Position in Execution Graph","text":"<p>Upstream: - <code>retry_handler</code></p> <p>Downstream: - <code>ASTPlannerNode</code></p> <p>Trigger conditions: - Executed when retry handler routes to refinement.</p> <pre><code>flowchart LR\n    RetryHandler[retry_handler] --&gt; Refiner[RefinerNode] --&gt; Planner[ASTPlannerNode]</code></pre>"},{"location":"architecture/nodes/refiner_node/#inputs","title":"Inputs","text":"<p>From <code>SubgraphExecutionState</code>:</p> <ul> <li><code>errors</code></li> <li><code>reasoning</code></li> <li><code>relevant_tables</code></li> <li><code>ast_planner_response.plan</code> (optional)</li> <li><code>sub_query.intent</code></li> </ul> <p>Validation performed:</p> <ul> <li>If no LLM configured, emits <code>MISSING_LLM</code>.</li> </ul>"},{"location":"architecture/nodes/refiner_node/#outputs","title":"Outputs","text":"<p>Mutations to <code>SubgraphExecutionState</code>:</p> <ul> <li><code>refiner_response</code> (<code>RefinerResponse</code>)</li> <li><code>errors</code> (includes <code>PLAN_FEEDBACK</code> warning)</li> <li><code>reasoning</code></li> </ul> <p>Side effects:</p> <ul> <li>LLM invocation.</li> </ul>"},{"location":"architecture/nodes/refiner_node/#internal-flow-step-by-step","title":"Internal Flow (Step-by-Step)","text":"<ol> <li>If no LLM configured, emit <code>MISSING_LLM</code> and stop.</li> <li>Serialize relevant tables and failed plan.</li> <li>Build error and reasoning strings.</li> <li>Invoke LLM with refinement prompt.</li> <li>Emit <code>PLAN_FEEDBACK</code> warning and return <code>RefinerResponse</code>.</li> <li>On exception, emit <code>REFINER_FAILED</code>.</li> </ol>"},{"location":"architecture/nodes/refiner_node/#contracts-interfaces","title":"Contracts &amp; Interfaces","text":"<p>Implements a LangGraph node callable:</p> <pre><code>def __call__(self, state: SubgraphExecutionState) -&gt; Dict[str, Any]\n</code></pre> <p>Key contracts:</p> <ul> <li><code>RefinerResponse</code></li> <li><code>PipelineError</code> (<code>PLAN_FEEDBACK</code>, <code>MISSING_LLM</code>, <code>REFINER_FAILED</code>)</li> </ul>"},{"location":"architecture/nodes/refiner_node/#determinism-guarantees","title":"Determinism Guarantees","text":"<ul> <li>Non\u2011deterministic unless LLM configured deterministically.</li> </ul>"},{"location":"architecture/nodes/refiner_node/#error-handling","title":"Error Handling","text":"<p>Emits <code>PipelineError</code> with:</p> <ul> <li><code>MISSING_LLM</code></li> <li><code>PLAN_FEEDBACK</code> (warning)</li> <li><code>REFINER_FAILED</code></li> </ul> <p>Logs failures via <code>logger.error</code>.</p>"},{"location":"architecture/nodes/refiner_node/#retry-idempotency","title":"Retry + Idempotency","text":"<ul> <li>No internal retry logic.</li> <li>Idempotency depends on LLM determinism.</li> </ul>"},{"location":"architecture/nodes/refiner_node/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>LLM call dominates latency.</li> <li>Serialization of inputs is in\u2011memory.</li> </ul>"},{"location":"architecture/nodes/refiner_node/#observability","title":"Observability","text":"<ul> <li>Logger: <code>refiner</code></li> <li>Adds reasoning entries with feedback content.</li> </ul>"},{"location":"architecture/nodes/refiner_node/#configuration","title":"Configuration","text":"<ul> <li>LLM configuration for agent name <code>refiner</code> via <code>llm.yaml</code>.</li> </ul>"},{"location":"architecture/nodes/refiner_node/#extension-points","title":"Extension Points","text":"<ul> <li>Modify <code>REFINER_PROMPT</code> for different refinement behavior.</li> <li>Replace node in <code>build_sql_agent_graph()</code> for alternate retry strategies.</li> </ul>"},{"location":"architecture/nodes/refiner_node/#known-limitations","title":"Known Limitations","text":"<ul> <li>No fallback if LLM unavailable.</li> <li>Feedback quality depends on LLM and prompt.</li> </ul>"},{"location":"architecture/nodes/refiner_node/#related-code","title":"Related Code","text":"<ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/refiner/node.py</code></li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/","title":"SchemaRetrieverNode","text":""},{"location":"architecture/nodes/schema_retriever_node/#overview","title":"Overview","text":"<ul> <li>Retrieves schema context for a sub\u2011query using staged vector search and authoritative snapshots.</li> <li>Exists to constrain planning to relevant tables/columns while remaining schema\u2011grounded.</li> <li>Sits at the entry of the SQL agent subgraph.</li> <li>Class: <code>SchemaRetrieverNode</code></li> <li>Source: <code>packages/core/src/nl2sql/pipeline/nodes/schema_retriever/node.py</code></li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/#responsibilities","title":"Responsibilities","text":"<ul> <li>Build a semantic query from sub\u2011query intent, filters, group_by, expected_schema, metrics.</li> <li>Retrieve candidate tables/columns/relationships from <code>VectorStore</code>.</li> <li>Resolve authoritative tables from <code>SchemaStore</code>.</li> <li>Fall back to full schema snapshot when retrieval returns no candidates.</li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/#position-in-execution-graph","title":"Position in Execution Graph","text":"<p>Upstream: - SQL agent subgraph entry.</p> <p>Downstream: - <code>ASTPlannerNode</code></p> <p>Trigger conditions: - Always executed as the subgraph entry node.</p> <pre><code>flowchart LR\n    SchemaRetriever[SchemaRetrieverNode] --&gt; Planner[ASTPlannerNode]</code></pre>"},{"location":"architecture/nodes/schema_retriever_node/#inputs","title":"Inputs","text":"<p>From <code>SubgraphExecutionState</code>:</p> <ul> <li><code>sub_query</code> (required): includes intent, filters, group_by, expected_schema, metrics, schema_version.</li> </ul> <p>From <code>NL2SQLContext</code>:</p> <ul> <li><code>vector_store</code> (optional)</li> <li><code>schema_store</code> (required for authoritative resolution)</li> </ul> <p>Validation performed:</p> <ul> <li>If <code>sub_query</code> missing, returns empty <code>relevant_tables</code>.</li> <li>If no candidates from vector store, falls back to full snapshot.</li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/#outputs","title":"Outputs","text":"<p>Mutations to <code>SubgraphExecutionState</code>:</p> <ul> <li><code>relevant_tables</code> (list of <code>Table</code>)</li> <li><code>reasoning</code> and <code>warnings</code> in fallback paths</li> </ul> <p>Side effects:</p> <ul> <li>Vector store queries.</li> <li>Schema store reads.</li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/#internal-flow-step-by-step","title":"Internal Flow (Step-by-Step)","text":"<ol> <li>Build semantic query with <code>_build_semantic_query()</code>.</li> <li>Retrieve table/metric chunks via <code>retrieve_schema_context()</code>.</li> <li>If no table chunks, retrieve columns via <code>retrieve_column_candidates()</code>.</li> <li>If tables are identified, retrieve columns/relationships via <code>retrieve_planning_context()</code>.</li> <li>Resolve schema snapshot via <code>_resolve_snapshot()</code>.</li> <li>Build <code>Table</code> objects with <code>_build_tables_from_snapshot()</code>.</li> <li>If no tables were found, return full snapshot with warning.</li> <li>On exception, log error and return empty <code>relevant_tables</code>.</li> </ol>"},{"location":"architecture/nodes/schema_retriever_node/#contracts-interfaces","title":"Contracts &amp; Interfaces","text":"<p>Implements a LangGraph node callable:</p> <pre><code>def __call__(self, state: SubgraphExecutionState) -&gt; Dict[str, Any]\n</code></pre> <p>Key contracts:</p> <ul> <li><code>Table</code> / <code>Column</code> (node schema)</li> <li><code>SchemaSnapshot</code> (authoritative source)</li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/#determinism-guarantees","title":"Determinism Guarantees","text":"<ul> <li>Deterministic snapshot resolution for a fixed <code>schema_version</code>.</li> <li>Vector retrieval ranking may be non\u2011deterministic (MMR).</li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/#error-handling","title":"Error Handling","text":"<ul> <li>Exceptions are caught and logged via <code>logger.error</code>.</li> <li>Returns empty <code>relevant_tables</code> with error reasoning; no <code>PipelineError</code> is emitted.</li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/#retry-idempotency","title":"Retry + Idempotency","text":"<ul> <li>No internal retry logic.</li> <li>Idempotent for fixed vector retrieval output; vector ranking may vary.</li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Depends on vector store retrieval latency and schema store reads.</li> <li>In\u2011memory processing of candidates and snapshot tables.</li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/#observability","title":"Observability","text":"<ul> <li>Logger: <code>schema_retriever</code></li> <li>Emits reasoning and warnings on fallback paths.</li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/#configuration","title":"Configuration","text":"<ul> <li>No direct settings used in this node.</li> <li>Vector store configuration is set in <code>NL2SQLContext</code>.</li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/#extension-points","title":"Extension Points","text":"<ul> <li>Modify semantic query construction to include additional signals.</li> <li>Replace node in <code>build_sql_agent_graph()</code> for alternative retrieval strategies.</li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/#known-limitations","title":"Known Limitations","text":"<ul> <li>No tenant scoping in vector retrieval.</li> <li>No caching of retrieved tables across retries.</li> <li>Fallback to full schema can be expensive for large schemas.</li> </ul>"},{"location":"architecture/nodes/schema_retriever_node/#related-code","title":"Related Code","text":"<ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/schema_retriever/node.py</code></li> <li><code>packages/core/src/nl2sql/indexing/vector_store.py</code></li> </ul>"},{"location":"architecture/subgraphs/main_pipeline_graph/","title":"Main Pipeline Graph","text":""},{"location":"architecture/subgraphs/main_pipeline_graph/#overview","title":"Overview","text":"<ul> <li>Purpose: orchestrates the end-to-end NL2SQL pipeline as a LangGraph state machine.</li> <li>Why it exists architecturally: central control plane that wires global nodes, subgraphs, and aggregation into a single execution DAG.</li> <li>When it is invoked: via <code>run_with_graph()</code> runtime entry point.</li> </ul> <p>Defining function: <code>build_graph()</code> Source file path: <code>packages/core/src/nl2sql/pipeline/graph.py</code></p>"},{"location":"architecture/subgraphs/main_pipeline_graph/#boundary-definition","title":"Boundary Definition","text":"<p>This graph is the top-level orchestrator. It encapsulates datasource resolution, decomposition, global planning, routing to subgraphs, aggregation, and answer synthesis. It does NOT implement subgraph internals (SQL planning/execution), per-node business logic, or adapter-specific execution; those are delegated to nodes and registered subgraphs.</p>"},{"location":"architecture/subgraphs/main_pipeline_graph/#entry-conditions","title":"Entry Conditions","text":"<p>Required state: - <code>GraphState.user_query</code> must be provided. - <code>GraphState.trace_id</code> is auto-generated if not provided.</p> <p>Preconditions: - <code>NL2SQLContext</code> must be initialized with registries/services used by nodes (vector store, LLM registry, datasource registry, schema store, RBAC).</p> <p>Triggering parent graph: - None. This is the root graph invoked by <code>run_with_graph()</code> in <code>pipeline/runtime.py</code>.</p>"},{"location":"architecture/subgraphs/main_pipeline_graph/#exit-conditions","title":"Exit Conditions","text":"<p>Successful completion criteria: - Reaches <code>END</code> after <code>answer_synthesizer</code> when aggregation and synthesis finish without fatal interruptions.</p> <p>Failure exits: - <code>datasource_resolver</code> can short-circuit to <code>END</code> via <code>resolver_route</code> when no datasource is resolved/allowed. - <code>layer_router</code> can return <code>END</code> if no <code>execution_dag</code> or layers are present. - <code>layer_router</code> raises <code>PipelineError</code> when no compatible subgraph exists for a datasource; this propagates as an exception from the router function.</p> <p>Partial completion behavior: - If routing cannot schedule any scan-layer work but <code>execution_dag</code> exists, it routes to <code>aggregator</code> directly. - Errors added to <code>GraphState.errors</code> do not automatically stop the graph; the graph relies on routing logic and node behavior for termination.</p>"},{"location":"architecture/subgraphs/main_pipeline_graph/#internal-node-composition","title":"Internal Node Composition","text":"<p>Execution order (nominal path): - <code>datasource_resolver</code> \u2014 <code>DatasourceResolverNode</code> \u2014 <code>packages/core/src/nl2sql/pipeline/nodes/datasource_resolver/node.py</code> \u2014 resolve candidate datasources and RBAC. - <code>decomposer</code> \u2014 <code>DecomposerNode</code> \u2014 <code>packages/core/src/nl2sql/pipeline/nodes/decomposer/node.py</code> \u2014 decompose query into sub-queries. - <code>global_planner</code> \u2014 <code>GlobalPlannerNode</code> \u2014 <code>packages/core/src/nl2sql/pipeline/nodes/global_planner/node.py</code> \u2014 build execution DAG. - <code>layer_router</code> \u2014 inline lambda + <code>routes.build_scan_layer_router</code> \u2014 <code>packages/core/src/nl2sql/pipeline/routes.py</code> \u2014 route each scan layer to a subgraph or aggregator. - <code>&lt;subgraph nodes&gt;</code> \u2014 via <code>wrap_subgraph()</code> \u2014 <code>packages/core/src/nl2sql/pipeline/graph_utils.py</code> \u2014 invoke registered subgraphs per scan node. - <code>aggregator</code> \u2014 <code>EngineAggregatorNode</code> \u2014 <code>packages/core/src/nl2sql/pipeline/nodes/aggregator/node.py</code> \u2014 execute aggregation DAG. - <code>answer_synthesizer</code> \u2014 <code>AnswerSynthesizerNode</code> \u2014 <code>packages/core/src/nl2sql/pipeline/nodes/answer_synthesizer/node.py</code> \u2014 produce final answer.</p> <p>Mermaid diagram (main pipeline only): <pre><code>flowchart TD\n    datasource_resolver --&gt;|continue| decomposer\n    datasource_resolver --&gt;|end| END\n    decomposer --&gt; global_planner\n    global_planner --&gt; layer_router\n    layer_router --&gt;|subgraph| subgraph_exec\n    subgraph_exec --&gt; layer_router\n    layer_router --&gt;|aggregator| aggregator\n    layer_router --&gt;|end| END\n    aggregator --&gt; answer_synthesizer --&gt; END</code></pre></p>"},{"location":"architecture/subgraphs/main_pipeline_graph/#state-contract","title":"State Contract","text":"<p>State model: <code>GraphState</code> (<code>packages/core/src/nl2sql/pipeline/state.py</code>).</p> <p>Field ownership, reducers, and lifecycle are defined in <code>../graph_state.md</code>.</p>"},{"location":"architecture/subgraphs/main_pipeline_graph/#step-by-step-execution-flow","title":"Step-by-Step Execution Flow","text":"<ol> <li><code>datasource_resolver</code> resolves candidate datasources using vector search and RBAC.</li> <li><code>resolver_route</code> decides:</li> <li><code>continue</code> if allowed datasources exist.</li> <li><code>end</code> if none exist or response missing.</li> <li><code>decomposer</code> uses the LLM to produce <code>SubQuery</code> objects and combine groups.</li> <li><code>global_planner</code> builds a deterministic <code>ExecutionDAG</code> from sub-queries and combines.</li> <li><code>layer_router</code> inspects the DAG and current <code>artifact_refs</code>:</li> <li>If no DAG or layers, returns <code>END</code>.</li> <li>If next scan layer is empty, routes to <code>aggregator</code>.</li> <li>For each scan node, resolves a compatible subgraph and sends <code>build_scan_payload</code>.</li> <li>Each subgraph execution returns <code>artifact_refs</code>, <code>subgraph_outputs</code>, and <code>errors</code> to <code>GraphState</code>.</li> <li><code>layer_router</code> is re-entered until all scan-layer nodes produce artifacts.</li> <li><code>aggregator</code> executes the DAG using the stored <code>artifact_refs</code>.</li> <li><code>answer_synthesizer</code> summarizes aggregated results into a final answer.</li> <li>Graph reaches <code>END</code>.</li> </ol>"},{"location":"architecture/subgraphs/main_pipeline_graph/#determinism-guarantees","title":"Determinism Guarantees","text":"<p>Determinism guarantees and non-determinism sources are documented in <code>../determinism.md</code>.</p>"},{"location":"architecture/subgraphs/main_pipeline_graph/#error-propagation","title":"Error Propagation","text":"<p>Failure domains, retry scope, and recovery limitations are centralized in <code>../failure_recovery.md</code>.</p>"},{"location":"architecture/subgraphs/main_pipeline_graph/#retry-recovery","title":"Retry + Recovery","text":"<p>See <code>../failure_recovery.md</code> for retry scope and recovery behavior.</p>"},{"location":"architecture/subgraphs/main_pipeline_graph/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Blocking calls include LLM requests in <code>decomposer</code> and <code>answer_synthesizer</code>.</li> <li><code>global_planner</code> and <code>aggregator</code> are CPU-bound (DAG construction and local aggregation).</li> <li>Subgraph executions are dispatched per scan layer and can run in parallel via LangGraph routing.</li> <li>Overall pipeline is executed within a thread pool controlled by <code>settings.sandbox_exec_workers</code> and guarded by <code>settings.global_timeout_sec</code>.</li> </ul>"},{"location":"architecture/subgraphs/main_pipeline_graph/#observability","title":"Observability","text":"<ul> <li>Each node uses <code>nl2sql.common.logger.get_logger()</code> with node-specific names.</li> <li><code>GraphState.trace_id</code> is propagated into subgraph IDs and executor requests, but no code binds it into logging context (<code>trace_context</code> is defined but not used).</li> <li><code>run_with_graph()</code> accepts LangGraph callbacks via <code>config={\"callbacks\": callbacks}</code>.</li> <li>No explicit metrics or tracing spans are emitted in the graph code.</li> </ul>"},{"location":"architecture/subgraphs/main_pipeline_graph/#configuration","title":"Configuration","text":"<ul> <li><code>GLOBAL_TIMEOUT_SEC</code> controls total pipeline timeout (<code>settings.global_timeout_sec</code>).</li> <li><code>SANDBOX_EXEC_WORKERS</code> controls thread pool size (<code>settings.sandbox_exec_workers</code>).</li> <li>Subgraph selection is governed by datasource capabilities in the subgraph registry.</li> </ul>"},{"location":"architecture/subgraphs/main_pipeline_graph/#extension-points","title":"Extension Points","text":"<ul> <li>Add new subgraphs by extending <code>build_subgraph_registry()</code> and providing a <code>SubgraphSpec</code>.</li> <li>Insert additional nodes in <code>build_graph()</code> by adding nodes/edges to the <code>StateGraph</code>.</li> <li>Customize routing by replacing <code>build_scan_layer_router()</code> or <code>resolver_route()</code>.</li> </ul>"},{"location":"architecture/subgraphs/main_pipeline_graph/#known-limitations","title":"Known Limitations","text":"<ul> <li><code>execute</code> parameter in <code>build_graph()</code> is accepted but not used in graph construction.</li> <li>Errors accumulated in <code>GraphState.errors</code> do not alter routing unless explicit routes inspect them.</li> <li>No trace/span propagation into logging context, despite trace context helpers existing.</li> <li>Merge reducers for <code>artifact_refs</code>/<code>subgraph_outputs</code> overwrite on key conflicts without ordering guarantees.</li> </ul>"},{"location":"architecture/subgraphs/main_pipeline_graph/#related-code","title":"Related Code","text":"<ul> <li>Subgraph definition: <code>packages/core/src/nl2sql/pipeline/graph.py</code></li> <li>Routing utilities: <code>packages/core/src/nl2sql/pipeline/routes.py</code>, <code>packages/core/src/nl2sql/pipeline/graph_utils.py</code></li> <li>State model: <code>packages/core/src/nl2sql/pipeline/state.py</code></li> <li>Node implementations:</li> <li><code>packages/core/src/nl2sql/pipeline/nodes/datasource_resolver/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/decomposer/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/global_planner/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/aggregator/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/answer_synthesizer/node.py</code></li> </ul>"},{"location":"architecture/subgraphs/sql_agent/","title":"SQL Agent Subgraph","text":""},{"location":"architecture/subgraphs/sql_agent/#overview","title":"Overview","text":"<ul> <li>Purpose: plan, validate, generate, and execute SQL for a single sub-query.</li> <li>Why it exists architecturally: isolates SQL-capable execution into a reusable subgraph that can be scheduled per scan node in the global execution DAG.</li> <li>When it is invoked: when <code>layer_router</code> resolves a datasource with <code>SUPPORTS_SQL</code> capability and sends a scan-layer payload.</li> </ul> <p>Defining function: <code>build_sql_agent_graph()</code> Source file path: <code>packages/core/src/nl2sql/pipeline/subgraphs/sql_agent.py</code></p>"},{"location":"architecture/subgraphs/sql_agent/#boundary-definition","title":"Boundary Definition","text":"<p>This subgraph encapsulates the SQL planning and execution loop for a single <code>SubQuery</code>. It does NOT perform cross-subquery aggregation, answer synthesis, or datasource resolution; those responsibilities are owned by the main pipeline graph and upstream nodes.</p>"},{"location":"architecture/subgraphs/sql_agent/#entry-conditions","title":"Entry Conditions","text":"<p>Required state: - <code>SubgraphExecutionState.trace_id</code> - <code>SubgraphExecutionState.sub_query</code> (must include <code>id</code>, <code>datasource_id</code>, and <code>intent</code>)</p> <p>Preconditions: - <code>sub_query.datasource_id</code> must map to a registered adapter/executor. - Schema store and vector store may be used if configured; otherwise schema retrieval falls back to full schema snapshot.</p> <p>Triggering parent graph: - <code>layer_router</code> in the main pipeline graph sends <code>build_scan_payload()</code> to the subgraph via <code>wrap_subgraph()</code>.</p>"},{"location":"architecture/subgraphs/sql_agent/#exit-conditions","title":"Exit Conditions","text":"<p>Successful completion criteria: - Reaches <code>END</code> after <code>executor</code> and produces <code>executor_response</code>.</p> <p>Failure exits: - <code>check_planner</code> returns <code>end</code> when planner output is missing and retries are exhausted or errors are non-retryable. - <code>check_logical_validation</code> returns <code>end</code> when validation errors are non-retryable or retries are exhausted. - Cancellation (<code>is_cancelled()</code>) forces <code>end</code> in router checks and in retry handler.</p> <p>Partial completion behavior: - If planner or logical validation errors are retryable and retry budget remains, execution loops via <code>retry_handler</code> \u2192 <code>refiner</code> \u2192 <code>ast_planner</code>.</p>"},{"location":"architecture/subgraphs/sql_agent/#internal-node-composition","title":"Internal Node Composition","text":"<p>Execution order (nominal path): - <code>schema_retriever</code> \u2014 <code>SchemaRetrieverNode</code> \u2014 <code>packages/core/src/nl2sql/pipeline/nodes/schema_retriever/node.py</code> \u2014 retrieve relevant schema tables/columns. - <code>ast_planner</code> \u2014 <code>ASTPlannerNode</code> \u2014 <code>packages/core/src/nl2sql/pipeline/nodes/ast_planner/node.py</code> \u2014 create SQL AST plan. - <code>logical_validator</code> \u2014 <code>LogicalValidatorNode</code> \u2014 <code>packages/core/src/nl2sql/pipeline/nodes/validator/node.py</code> \u2014 validate plan correctness and policy constraints. - <code>generator</code> \u2014 <code>GeneratorNode</code> \u2014 <code>packages/core/src/nl2sql/pipeline/nodes/generator/node.py</code> \u2014 generate SQL text from AST. - <code>executor</code> \u2014 <code>ExecutorNode</code> \u2014 <code>packages/core/src/nl2sql/pipeline/nodes/executor/node.py</code> \u2014 execute SQL and produce artifacts. - <code>retry_handler</code> \u2014 <code>retry_node()</code> \u2014 <code>packages/core/src/nl2sql/pipeline/subgraphs/sql_agent.py</code> \u2014 apply exponential backoff and increment retry count. - <code>refiner</code> \u2014 <code>RefinerNode</code> \u2014 <code>packages/core/src/nl2sql/pipeline/nodes/refiner/node.py</code> \u2014 generate feedback for the planner.</p> <p>Mermaid diagram (sql_agent only): <pre><code>flowchart TD\n    schema_retriever --&gt; ast_planner\n    ast_planner --&gt;|ok| logical_validator\n    ast_planner --&gt;|retry| retry_handler\n    ast_planner --&gt;|end| END\n    logical_validator --&gt;|ok| generator\n    logical_validator --&gt;|retry| retry_handler\n    logical_validator --&gt;|end| END\n    retry_handler --&gt; refiner --&gt; ast_planner\n    generator --&gt; executor --&gt; END</code></pre></p>"},{"location":"architecture/subgraphs/sql_agent/#state-contract","title":"State Contract","text":"<p>State model: <code>SubgraphExecutionState</code> (<code>packages/core/src/nl2sql/pipeline/state.py</code>).</p> <p>Field ownership, reducers, and lifecycle are defined in <code>../graph_state.md</code>.</p>"},{"location":"architecture/subgraphs/sql_agent/#step-by-step-execution-flow","title":"Step-by-Step Execution Flow","text":"<ol> <li><code>schema_retriever</code> builds a semantic query from <code>sub_query</code> and retrieves relevant schema context; if none found, it falls back to full schema snapshot.</li> <li><code>ast_planner</code> runs an LLM planning chain to produce <code>PlanModel</code>.</li> <li><code>check_planner</code> routes:</li> <li><code>ok</code> if a plan exists.</li> <li><code>retry</code> if errors are retryable and retry budget remains.</li> <li><code>end</code> if errors are non-retryable or retries exhausted.</li> <li><code>logical_validator</code> validates the plan for structural and policy compliance.</li> <li><code>check_logical_validation</code> routes:</li> <li><code>ok</code> when no blocking errors exist.</li> <li><code>retry</code> when retryable errors exist and retry budget remains.</li> <li><code>end</code> when errors are non-retryable or retries exhausted.</li> <li><code>generator</code> converts the plan to SQL for the target datasource dialect.</li> <li><code>executor</code> runs SQL via the datasource adapter and returns artifacts/errors.</li> <li>If routed to retry: <code>retry_handler</code> waits with exponential backoff and jitter, then increments <code>retry_count</code>.</li> <li><code>refiner</code> uses LLM feedback to enrich error context, then loops back to <code>ast_planner</code>.</li> <li>Subgraph terminates at <code>END</code>.</li> </ol>"},{"location":"architecture/subgraphs/sql_agent/#determinism-guarantees","title":"Determinism Guarantees","text":"<p>Determinism guarantees and non-determinism sources are documented in <code>../determinism.md</code>.</p>"},{"location":"architecture/subgraphs/sql_agent/#error-propagation","title":"Error Propagation","text":"<p>Failure domains, retry scope, and recovery limitations are centralized in <code>../failure_recovery.md</code>.</p>"},{"location":"architecture/subgraphs/sql_agent/#retry-recovery","title":"Retry + Recovery","text":"<p>See <code>../failure_recovery.md</code> for retry scope and recovery behavior.</p>"},{"location":"architecture/subgraphs/sql_agent/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Blocking LLM calls: <code>ASTPlannerNode</code> and <code>RefinerNode</code>.</li> <li>External I/O: vector store retrieval and schema store access in <code>SchemaRetrieverNode</code>; datasource execution in <code>ExecutorNode</code>.</li> <li>CPU-bound steps: SQL generation via sqlglot in <code>GeneratorNode</code>.</li> <li>Retry loop introduces sleep-based delays on failures.</li> </ul>"},{"location":"architecture/subgraphs/sql_agent/#observability","title":"Observability","text":"<ul> <li>Node-specific loggers are used (<code>schema_retriever</code>, <code>planner</code>, <code>logical_validator</code>, <code>generator</code>, <code>executor</code>, <code>refiner</code>).</li> <li><code>trace_id</code> is stored in state and forwarded to executor requests; logging context is not explicitly set.</li> <li>No explicit metrics or tracing spans are emitted by the subgraph code.</li> </ul>"},{"location":"architecture/subgraphs/sql_agent/#configuration","title":"Configuration","text":"<ul> <li><code>SQL_AGENT_MAX_RETRIES</code></li> <li><code>SQL_AGENT_RETRY_BASE_DELAY_SEC</code></li> <li><code>SQL_AGENT_RETRY_MAX_DELAY_SEC</code></li> <li><code>SQL_AGENT_RETRY_JITTER_SEC</code></li> </ul>"},{"location":"architecture/subgraphs/sql_agent/#extension-points","title":"Extension Points","text":"<ul> <li>Modify <code>build_sql_agent_graph()</code> to insert, remove, or rewire nodes and edges.</li> <li>Add new subgraphs by registering additional <code>SubgraphSpec</code> entries in <code>build_subgraph_registry()</code>.</li> <li>Replace node implementations via dependency injection in <code>NL2SQLContext</code>.</li> </ul>"},{"location":"architecture/subgraphs/sql_agent/#known-limitations","title":"Known Limitations","text":"<ul> <li><code>physical_validator</code> node is instantiated but not wired; physical validation is unreachable.</li> <li>Generator and executor errors are not routed into the retry loop; failures proceed to <code>executor</code> or terminate at <code>END</code>.</li> <li>No subgraph-specific metrics or trace spans are emitted.</li> <li>Retry loop only considers planner and logical validation errors.</li> </ul>"},{"location":"architecture/subgraphs/sql_agent/#related-code","title":"Related Code","text":"<ul> <li>Subgraph definition: <code>packages/core/src/nl2sql/pipeline/subgraphs/sql_agent.py</code></li> <li>Subgraph registry: <code>packages/core/src/nl2sql/pipeline/subgraphs/registry.py</code></li> <li>State model: <code>packages/core/src/nl2sql/pipeline/state.py</code></li> <li>Node implementations:</li> <li><code>packages/core/src/nl2sql/pipeline/nodes/schema_retriever/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/ast_planner/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/validator/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/generator/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/executor/node.py</code></li> <li><code>packages/core/src/nl2sql/pipeline/nodes/refiner/node.py</code></li> <li>Tests:</li> <li><code>packages/core/tests/unit/test_sql_agent_subgraph.py</code></li> <li><code>packages/core/tests/unit/test_subgraph_registry.py</code></li> </ul>"},{"location":"configuration/datasources/","title":"Datasource Configuration","text":"<p>Datasource configuration lives in <code>configs/datasources.yaml</code> and declares one or more database connections that NL2SQL can query.</p>"},{"location":"configuration/datasources/#file-structure","title":"File structure","text":"<pre><code>version: 1\ndatasources:\n  - id: my_postgres\n    description: \"Primary analytics DB.\"\n    connection:\n      type: postgres\n      host: localhost\n      port: 5432\n      user: ${env:POSTGRES_USER}\n      password: ${env:POSTGRES_PASSWORD}\n      database: analytics\n    statement_timeout_ms: 8000\n    row_limit: 100\n    max_bytes: 10485760\n</code></pre>"},{"location":"configuration/datasources/#fields","title":"Fields","text":"<ul> <li><code>version</code>: schema version (currently <code>1</code>)</li> <li><code>datasources</code>: list of datasource definitions</li> <li><code>id</code>: unique datasource ID</li> <li><code>description</code>: optional human-readable description</li> <li><code>connection</code>: connection details<ul> <li><code>type</code>: adapter type (e.g., <code>postgres</code>, <code>mysql</code>, <code>mssql</code>, <code>sqlite</code>)</li> <li>adapter-specific fields (host, port, database, driver, etc.)</li> </ul> </li> <li>optional limits and metadata (commonly used):<ul> <li><code>statement_timeout_ms</code></li> <li><code>row_limit</code></li> <li><code>max_bytes</code></li> <li><code>tags</code></li> </ul> </li> </ul>"},{"location":"configuration/datasources/#notes","title":"Notes","text":"<ul> <li>Use <code>${env:VAR}</code> for environment variables or <code>${provider:key}</code> for secrets   resolved via <code>configs/secrets.yaml</code>.</li> <li>Adapter-specific fields are passed through to the adapter implementation.</li> </ul>"},{"location":"configuration/llm/","title":"LLM Configuration","text":"<p>LLM configuration lives in <code>configs/llm.yaml</code> and defines the default model plus optional per-agent overrides.</p>"},{"location":"configuration/llm/#file-structure","title":"File structure","text":"<pre><code>version: 1\ndefault:\n  provider: openai\n  model: gpt-5.2\n  temperature: 0.0\n  api_key: ${env:OPENAI_API_KEY}\nagents:\n  indexing_enrichment:\n    provider: openai\n    model: gpt-5.2\n    temperature: 0.0\n    api_key: ${env:OPENAI_API_KEY}\n</code></pre>"},{"location":"configuration/llm/#fields","title":"Fields","text":"<ul> <li><code>version</code>: schema version (currently <code>1</code>)</li> <li><code>default</code>: LLM configuration used by default</li> <li><code>agents</code>: optional map of agent name \u2192 LLM config override</li> </ul> <p>Each LLM config supports:</p> <ul> <li><code>provider</code>: LLM provider name</li> <li><code>model</code>: model identifier</li> <li><code>temperature</code>: float (defaults to <code>0.0</code>)</li> <li><code>api_key</code>: optional; can use <code>${env:VAR}</code> or <code>${provider:key}</code></li> </ul>"},{"location":"configuration/llm/#notes","title":"Notes","text":"<ul> <li><code>agents</code> overrides allow you to use specialized models for tasks like   indexing enrichment while keeping a single default model for query execution.</li> </ul>"},{"location":"configuration/policies/","title":"Policies Configuration","text":"<p>Policies live in <code>configs/policies.json</code> and define role-based access rules for datasources and tables.</p>"},{"location":"configuration/policies/#file-structure","title":"File structure","text":"<pre><code>{\n  \"version\": 1,\n  \"roles\": {\n    \"admin\": {\n      \"description\": \"System Administrator\",\n      \"role\": \"admin\",\n      \"allowed_datasources\": [\"*\"],\n      \"allowed_tables\": [\"*\"]\n    }\n  }\n}\n</code></pre>"},{"location":"configuration/policies/#fields","title":"Fields","text":"<ul> <li><code>version</code>: schema version (currently <code>1</code>)</li> <li><code>roles</code>: map of role name \u2192 policy</li> <li><code>description</code>: human-readable role description</li> <li><code>role</code>: role ID used for logging and auditing</li> <li><code>allowed_datasources</code>: list of datasource IDs or <code>*</code></li> <li><code>allowed_tables</code>: list of tables in <code>datasource.table</code> format</li> </ul>"},{"location":"configuration/policies/#table-wildcards","title":"Table wildcards","text":"<ul> <li><code>*</code> allows all tables</li> <li><code>datasource.*</code> allows all tables in a datasource</li> </ul>"},{"location":"configuration/secrets/","title":"Secrets Configuration","text":"<p>Secrets configuration is optional and lives in <code>configs/secrets.yaml</code>. It defines secret providers that can be referenced in other config files.</p>"},{"location":"configuration/secrets/#file-structure","title":"File structure","text":"<pre><code>version: 1\nproviders:\n  - id: aws-main\n    type: aws\n    region_name: \"us-east-1\"\n</code></pre>"},{"location":"configuration/secrets/#providers","title":"Providers","text":"<p>Each provider has:</p> <ul> <li><code>id</code>: unique provider ID used in secret references (e.g. <code>${aws-main:db_password}</code>)</li> <li><code>type</code>: <code>aws</code>, <code>azure</code>, <code>hashi</code>, or <code>env</code></li> </ul> <p>Provider-specific fields:</p> <ul> <li>aws: <code>region_name</code>, <code>profile_name</code></li> <li>azure: <code>vault_url</code>, <code>client_id</code>, <code>client_secret</code>, <code>tenant_id</code></li> <li>hashi: <code>url</code>, <code>token</code>, <code>mount_point</code></li> <li>env: no additional fields</li> </ul>"},{"location":"configuration/secrets/#secret-references","title":"Secret references","text":"<p>Use <code>${provider_id:key}</code> in config files to resolve a secret from a provider. Use <code>${env:VAR}</code> to read directly from environment variables.</p>"},{"location":"configuration/system/","title":"Configuration System","text":"<p>Configuration is split into environment variables (runtime settings) and file-based configuration (datasources, LLMs, policies, secrets). <code>Settings</code> loads env vars; <code>ConfigManager</code> validates file formats and resolves secrets.</p>"},{"location":"configuration/system/#settings-environment-variables","title":"Settings (environment variables)","text":""},{"location":"configuration/system/#paths","title":"Paths","text":"Env var Default Description <code>OPENAI_API_KEY</code> <code>\u2014</code> API key for OpenAI provider (optional if using other providers). <code>LLM_CONFIG</code> <code>configs/llm.yaml</code> Path to the LLM config file. <code>DATASOURCE_CONFIG</code> <code>configs/datasources.yaml</code> Path to the datasource config file. <code>BENCHMARK_CONFIG</code> <code>configs/benchmark_suite.yaml</code> Path to the benchmark suite file. <code>SAMPLE_QUESTIONS</code> <code>configs/sample_questions.yaml</code> Path to the sample questions file. <code>POLICIES_CONFIG</code> <code>configs/policies.json</code> Path to the RBAC policies file. <code>SECRETS_CONFIG</code> <code>configs/secrets.yaml</code> Path to the secrets config file. <code>VECTOR_STORE</code> <code>./chroma_db</code> Persist directory for the vector store. <code>VECTOR_STORE_COLLECTION</code> <code>nl2sql_store</code> Collection name for schema embeddings."},{"location":"configuration/system/#storage","title":"Storage","text":"Env var Default Description <code>SCHEMA_STORE_BACKEND</code> <code>sqlite</code> Schema store backend identifier. <code>SCHEMA_STORE_PATH</code> <code>data/schema_store.db</code> SQLite database path for schema store persistence. <code>SCHEMA_STORE_MAX_VERSIONS</code> <code>3</code> Max schema versions retained per datasource. <code>RESULT_ARTIFACT_BACKEND</code> <code>local</code> Artifact backend: <code>local</code>, <code>s3</code>, <code>adls</code>. <code>RESULT_ARTIFACT_BASE_URI</code> <code>./artifacts</code> Base URI or path for artifact storage. <code>RESULT_ARTIFACT_PATH_TEMPLATE</code> <code>&lt;tenant_id&gt;/&lt;request_id&gt;/&lt;subgraph_name&gt;/&lt;dag_node_id&gt;/&lt;schema_version&gt;/part-00000.parquet</code> Template for artifact paths. <code>RESULT_ARTIFACT_S3_BUCKET</code> <code>\u2014</code> S3 bucket for artifact storage. <code>RESULT_ARTIFACT_S3_PREFIX</code> <code>\u2014</code> S3 prefix for artifact storage. <code>RESULT_ARTIFACT_ADLS_ACCOUNT</code> <code>\u2014</code> ADLS storage account name. <code>RESULT_ARTIFACT_ADLS_CONTAINER</code> <code>\u2014</code> ADLS container name. <code>RESULT_ARTIFACT_ADLS_CONNECTION_STRING</code> <code>\u2014</code> ADLS connection string, if using key-based auth."},{"location":"configuration/system/#execution","title":"Execution","text":"Env var Default Description <code>GLOBAL_TIMEOUT_SEC</code> <code>60</code> Global timeout in seconds for pipeline execution. <code>SANDBOX_EXEC_WORKERS</code> <code>4</code> Max workers for latency-sensitive execution pool. <code>SANDBOX_INDEX_WORKERS</code> <code>2</code> Max workers for throughput-heavy indexing pool."},{"location":"configuration/system/#behavior","title":"Behavior","text":"Env var Default Description <code>SCHEMA_VERSION_MISMATCH_POLICY</code> <code>warn</code> Action on schema version mismatch: <code>warn</code>, <code>fail</code>, <code>ignore</code>. <code>SQL_AGENT_MAX_RETRIES</code> <code>3</code> Max retry attempts for SQL agent refinement. <code>SQL_AGENT_RETRY_BASE_DELAY_SEC</code> <code>1.0</code> Base delay for SQL agent retries (seconds). <code>SQL_AGENT_RETRY_MAX_DELAY_SEC</code> <code>10.0</code> Max delay for SQL agent retries (seconds). <code>SQL_AGENT_RETRY_JITTER_SEC</code> <code>0.5</code> Max jitter added to SQL agent retry delays (seconds). <code>LOGICAL_VALIDATOR_STRICT_COLUMNS</code> <code>false</code> Treat missing columns as errors in logical validation. <code>TENANT_ID</code> <code>default_tenant</code> Default tenant ID for requests."},{"location":"configuration/system/#limits","title":"Limits","text":"Env var Default Description <code>DEFAULT_ROW_LIMIT</code> <code>10000</code> Default row limit for SQL execution safeguards. <code>DEFAULT_MAX_BYTES</code> <code>10485760</code> Default max bytes limit for SQL execution safeguards. <code>DEFAULT_STATEMENT_TIMEOUT_MS</code> <code>30000</code> Default statement timeout for SQL execution safeguards."},{"location":"configuration/system/#routing","title":"Routing","text":"Env var Default Description <code>ROUTER_L1_THRESHOLD</code> <code>0.4</code> Distance threshold for Layer 1 vector search. <code>ROUTER_L2_THRESHOLD</code> <code>0.6</code> Relaxed distance threshold for Layer 2 voting."},{"location":"configuration/system/#observability","title":"Observability","text":"Env var Default Description <code>OBSERVABILITY_EXPORTER</code> <code>none</code> Exporter for metrics/traces: <code>none</code>, <code>console</code>, <code>otlp</code>. <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> <code>\u2014</code> Endpoint for OTLP exporter. <code>AUDIT_LOG_PATH</code> <code>logs/audit_events.log</code> Path to the persistent audit log file."},{"location":"configuration/system/#environment-file-loading","title":"Environment file loading","text":"Env var Default Description <code>ENV_FILE_PATH</code> <code>\u2014</code> Path to a specific <code>.env</code> file to load. <code>ENV</code> <code>\u2014</code> Environment name used to load <code>.env.{ENV}</code>. <code>APP_ENV</code> <code>\u2014</code> Alternate environment name used to load <code>.env.{APP_ENV}</code>."},{"location":"configuration/system/#config-manager-flow","title":"Config manager flow","text":"<pre><code>flowchart TD\n    Env[Environment Variables] --&gt; Settings[Settings]\n    Settings --&gt; ConfigManager[ConfigManager]\n    ConfigManager --&gt; Datasources[datasources.yaml]\n    ConfigManager --&gt; LLM[llm.yaml]\n    ConfigManager --&gt; Policies[policies.json]\n    ConfigManager --&gt; Secrets[secrets.yaml]</code></pre>"},{"location":"configuration/system/#secret-resolution","title":"Secret resolution","text":"<p><code>SecretManager</code> resolves secret references (<code>${provider:key}</code>):</p> <ul> <li>Default provider: <code>env</code></li> <li>Two-phase loading: secrets are resolved before registries initialize</li> </ul>"},{"location":"configuration/system/#configuration-file-contracts","title":"Configuration file contracts","text":"<ul> <li><code>configs/datasources.yaml</code>: datasource IDs, connection types, and options.   See Datasource config.</li> <li><code>configs/llm.yaml</code>: model/provider settings per agent name.   See LLM config.</li> <li><code>configs/policies.json</code>: RBAC policies and allowed tables/datasources.   See Policies config.</li> <li><code>configs/secrets.yaml</code>: optional secret providers.   See Secrets config.</li> </ul>"},{"location":"configuration/system/#source-references","title":"Source references","text":"<ul> <li>Settings: <code>packages/core/src/nl2sql/common/settings.py</code></li> <li>ConfigManager: <code>packages/core/src/nl2sql/configs/manager.py</code></li> <li>SecretManager: <code>packages/core/src/nl2sql/secrets/manager.py</code></li> </ul>"},{"location":"deployment/architecture/","title":"Deployment Architecture","text":"<p>NL2SQL is a Python engine that you embed in a service of your choice. The runtime is a single process that initializes <code>NL2SQLContext</code>, loads configuration, and invokes the pipeline per request.</p>"},{"location":"deployment/architecture/#runtime-process-layout","title":"Runtime process layout","text":"<pre><code>flowchart TD\n    App[Service Process] --&gt; Context[NL2SQLContext]\n    Context --&gt; Registries[Registries + Stores]\n    Context --&gt; Graph[LangGraph Pipeline]\n    Graph --&gt; Exec[Executor Services]\n    Exec --&gt; DB[(Datasource)]</code></pre>"},{"location":"deployment/architecture/#deployment-inputs","title":"Deployment inputs","text":"<ul> <li>Environment variables for <code>Settings</code> (paths and runtime parameters).</li> <li>Configuration files under <code>configs/</code>.</li> <li>Adapter packages installed in the runtime environment.</li> <li>Optional persistence volumes for vector store, schema store, and artifacts.</li> </ul>"},{"location":"deployment/architecture/#runtime-modes","title":"Runtime modes","text":"<ul> <li>Local dev: local vector store + SQLite schema store + local artifact store.</li> <li>Production: persistent volumes or external stores, stable secret providers, observability exporter enabled.</li> </ul>"},{"location":"deployment/architecture/#scaling-and-isolation","title":"Scaling and isolation","text":"<ul> <li>Orchestration runs in-process and can scale horizontally with your service.</li> <li>Sandbox process pools are available for isolation but are not enforced by default in the SQL executor.</li> </ul>"},{"location":"deployment/architecture/#source-references","title":"Source references","text":"<ul> <li>Context initialization: <code>packages/core/src/nl2sql/context.py</code></li> <li>Settings: <code>packages/core/src/nl2sql/common/settings.py</code></li> </ul>"},{"location":"deployment/docker/","title":"Docker Deployment","text":"<p>NL2SQL can be deployed as a standalone HTTP service using the <code>nl2sql-api</code> package and Docker. You can also embed the core library in your own service.</p>"},{"location":"deployment/docker/#container-requirements","title":"Container requirements","text":"<ul> <li>Install <code>nl2sql-core</code> and add adapters via extras (e.g. <code>nl2sql-core[postgres]</code>).</li> <li>Provide configuration files under <code>configs/</code> (or configure paths via env vars).</li> <li>Set required environment variables (LLM keys, config paths, storage paths).</li> </ul>"},{"location":"deployment/docker/#persistence-mounts","title":"Persistence mounts","text":"<p>If using persistent stores, mount these paths:</p> <ul> <li><code>VECTOR_STORE</code> for the Chroma vector store.</li> <li><code>SCHEMA_STORE_PATH</code> for the SQLite schema store.</li> <li><code>RESULT_ARTIFACT_BASE_URI</code> for artifact storage (local backend).</li> </ul>"},{"location":"deployment/docker/#notes","title":"Notes","text":"<ul> <li>Schema indexing is not automatic; you must run indexing in your operational flow.</li> <li>Observability exporters require network access and corresponding env vars.</li> </ul>"},{"location":"deployment/docker/#reference-configuration","title":"Reference configuration","text":"<p>See <code>configuration/system.md</code> for settings and defaults.</p>"},{"location":"deployment/kubernetes/","title":"Kubernetes Deployment","text":"<p>Kubernetes deployment depends on the service you build around <code>run_with_graph()</code>. The core engine expects configuration files and environment variables to be available to the runtime container.</p>"},{"location":"deployment/kubernetes/#configuration-and-secrets","title":"Configuration and secrets","text":"<p>Mount <code>configs/</code> and inject secrets so that:</p> <ul> <li><code>LLM_CONFIG</code>, <code>DATASOURCE_CONFIG</code>, <code>POLICIES_CONFIG</code>, <code>SECRETS_CONFIG</code> resolve correctly.</li> <li>provider keys (e.g., <code>OPENAI_API_KEY</code>) are injected into env.</li> </ul>"},{"location":"deployment/kubernetes/#persistence","title":"Persistence","text":"<p>If you persist schema/vector stores or artifacts, mount volumes for:</p> <ul> <li><code>VECTOR_STORE</code> (Chroma persistence directory)</li> <li><code>SCHEMA_STORE_PATH</code> (SQLite schema store)</li> <li><code>RESULT_ARTIFACT_BASE_URI</code> (artifact storage base path)</li> </ul>"},{"location":"deployment/kubernetes/#operational-considerations","title":"Operational considerations","text":"<ul> <li>Run schema indexing as a separate job or init container.</li> <li>Configure observability exporters via env (<code>OBSERVABILITY_EXPORTER</code>, <code>OTEL_EXPORTER_OTLP_ENDPOINT</code>).</li> </ul>"},{"location":"deployment/kubernetes/#reference-configuration","title":"Reference configuration","text":"<p>See <code>configuration/system.md</code> for settings and defaults.</p>"},{"location":"development/","title":"Development Guide","text":"<p>This guide focuses on extension points: adapters, executors, subgraphs, chunking, and planner logic.</p>"},{"location":"development/#extension-guides","title":"Extension guides","text":"<ul> <li><code>extensions/add-adapter.md</code> for new datasource adapters.</li> <li><code>extensions/add-chunk-types.md</code> for new schema chunk types.</li> <li><code>extensions/extend-planner.md</code> for planner/AST updates.</li> <li><code>extensions/add-execution-backend.md</code> for new executor services.</li> </ul>"},{"location":"development/#subgraph-extension-overview","title":"Subgraph extension overview","text":"<ol> <li>Implement a <code>build_*_graph(ctx)</code> function that returns a LangGraph runnable.</li> <li>Register it in <code>build_subgraph_registry()</code> with required capabilities.</li> </ol> <pre><code>flowchart TD\n    Subgraph[build_*_graph] --&gt; Spec[SubgraphSpec]\n    Spec --&gt; Registry[build_subgraph_registry]\n    Registry --&gt; Router[build_scan_layer_router]</code></pre>"},{"location":"development/#source-references","title":"Source references","text":"<ul> <li>Subgraph registry: <code>packages/core/src/nl2sql/pipeline/subgraphs/registry.py</code></li> <li>Adapter protocol: <code>packages/adapter-sdk/src/nl2sql_adapter_sdk/protocols.py</code></li> <li>Executor registry: <code>packages/core/src/nl2sql/execution/executor/registry.py</code></li> </ul>"},{"location":"extensions/add-adapter/","title":"Adding a New Adapter","text":"<p>This guide describes how to add a new datasource adapter and make it available to NL2SQL.</p>"},{"location":"extensions/add-adapter/#1-implement-the-adapter-contract","title":"1. Implement the adapter contract","text":"<p>All adapters must implement <code>DatasourceAdapterProtocol</code>. See <code>../adapters/sdk.md</code> for the authoritative contract and required fields/methods.</p> <p>If you are building a SQL adapter, extending <code>BaseSQLAlchemyAdapter</code> can provide schema introspection and default SQL execution. See <code>../adapters/sqlalchemy.md</code>.</p>"},{"location":"extensions/add-adapter/#2-expose-an-entry-point","title":"2. Expose an entry point","text":"<p>Adapters are discovered via <code>nl2sql.adapters</code> entry points. Ensure your package exposes:</p> <pre><code>nl2sql.adapters = \n  postgres = my_pkg.adapters.PostgresAdapter\n</code></pre>"},{"location":"extensions/add-adapter/#3-configure-the-datasource","title":"3. Configure the datasource","text":"<p>Add a datasource entry in <code>configs/datasources.yaml</code>:</p> <pre><code>datasources:\n  - id: sales_db\n    description: \"Sales warehouse\"\n    connection:\n      type: postgres\n      host: ${env:PG_HOST}\n      user: ${env:PG_USER}\n      password: ${env:PG_PASSWORD}\n</code></pre> <p>Secrets are resolved by <code>SecretManager</code> before the registry initializes.</p>"},{"location":"extensions/add-adapter/#4-validate-capabilities","title":"4. Validate capabilities","text":"<p>Ensure <code>capabilities()</code> advertises the correct set for routing and execution:</p> <ul> <li><code>supports_sql</code> if SQL execution is supported.</li> <li><code>supports_schema_introspection</code> if schema can be fetched.</li> <li><code>supports_dry_run</code> or <code>supports_cost_estimate</code> if available.</li> </ul>"},{"location":"extensions/add-adapter/#source-references","title":"Source references","text":"<ul> <li>Adapter protocol: <code>packages/adapter-sdk/src/nl2sql_adapter_sdk/protocols.py</code></li> <li>Adapter discovery: <code>packages/core/src/nl2sql/datasources/discovery.py</code></li> <li>Datasource registry: <code>packages/core/src/nl2sql/datasources/registry.py</code></li> <li>SQLAlchemy base adapter: <code>packages/adapter-sqlalchemy/src/nl2sql_sqlalchemy_adapter/adapter.py</code></li> </ul>"},{"location":"extensions/add-chunk-types/","title":"Adding New Chunk Types","text":"<p>Chunking is the backbone of retrieval. This guide explains how to add new schema chunk types.</p>"},{"location":"extensions/add-chunk-types/#1-add-a-new-chunk-model","title":"1. Add a new chunk model","text":"<p>Define a new chunk class in <code>nl2sql.indexing.models</code>:</p> <ul> <li>Inherit from <code>BaseChunk</code>.</li> <li>Provide a stable, deterministic ID.</li> <li>Implement <code>get_page_content()</code> and metadata via <code>get_metadata()</code>.</li> </ul>"},{"location":"extensions/add-chunk-types/#2-extend-the-chunk-builder","title":"2. Extend the chunk builder","text":"<p>Update <code>SchemaChunkBuilder</code> to emit the new chunk type:</p> <ul> <li>Add a <code>_build_*_chunks()</code> method.</li> <li>Include it in <code>build()</code> alongside existing chunk builders.</li> </ul>"},{"location":"extensions/add-chunk-types/#3-update-retrieval-filters","title":"3. Update retrieval filters","text":"<p>If your chunk type should be retrievable:</p> <ul> <li>Add it to the appropriate <code>VectorStore.retrieve_*</code> filters.</li> <li>Update <code>SchemaRetrieverNode</code> logic if new chunk types influence planning.</li> </ul>"},{"location":"extensions/add-chunk-types/#4-update-plannervalidator-if-required","title":"4. Update planner/validator (if required)","text":"<p>If the chunk type carries new semantics, ensure:</p> <ul> <li><code>ASTPlannerNode</code> prompt includes the relevant context.</li> <li><code>LogicalValidatorNode</code> can enforce any new constraints.</li> </ul>"},{"location":"extensions/add-chunk-types/#source-references","title":"Source references","text":"<ul> <li>Chunk models: <code>packages/core/src/nl2sql/indexing/models.py</code></li> <li>Chunk builder: <code>packages/core/src/nl2sql/indexing/chunk_builder.py</code></li> <li>Vector store retrieval: <code>packages/core/src/nl2sql/indexing/vector_store.py</code></li> <li>Schema retriever: <code>packages/core/src/nl2sql/pipeline/nodes/schema_retriever/node.py</code></li> </ul>"},{"location":"extensions/add-execution-backend/","title":"Adding an Execution Backend","text":"<p>Execution backends are implemented as <code>ExecutorService</code> instances and registered in <code>ExecutorRegistry</code>.</p>"},{"location":"extensions/add-execution-backend/#1-implement-the-executor-service","title":"1. Implement the executor service","text":"<p>Create a class that implements:</p> <ul> <li><code>validate_request(request) -&gt; list[PipelineError]</code></li> <li><code>execute(request) -&gt; ExecutorResponse</code></li> </ul> <p>Use <code>ExecutorRequest</code>/<code>ExecutorResponse</code> as the contract.</p>"},{"location":"extensions/add-execution-backend/#2-register-the-executor","title":"2. Register the executor","text":"<p>Register the executor in <code>ExecutorRegistry</code> for a capability key. The registry will select executors based on datasource capabilities.</p>"},{"location":"extensions/add-execution-backend/#3-update-routing-if-needed","title":"3. Update routing (if needed)","text":"<p>If a new subgraph is required for the backend, add a subgraph spec in:</p> <ul> <li><code>packages/core/src/nl2sql/pipeline/subgraphs/registry.py</code></li> </ul> <p>Ensure its <code>required_capabilities</code> match the adapter\u2019s advertised capabilities.</p>"},{"location":"extensions/add-execution-backend/#4-artifact-handling","title":"4. Artifact handling","text":"<p>If the backend returns tabular results:</p> <ul> <li>Return a <code>ResultFrame</code>.</li> <li>Persist it via <code>ArtifactStore</code> to produce an <code>ArtifactRef</code>.</li> </ul>"},{"location":"extensions/add-execution-backend/#source-references","title":"Source references","text":"<ul> <li>Executor service base: <code>packages/core/src/nl2sql/execution/executor/base.py</code></li> <li>Executor registry: <code>packages/core/src/nl2sql/execution/executor/registry.py</code></li> <li>Executor contracts: <code>packages/core/src/nl2sql/execution/contracts.py</code></li> </ul>"},{"location":"extensions/extend-planner/","title":"Extending the Planner","text":"<p>The planner generates a structured AST (<code>PlanModel</code>) which is validated and compiled into SQL. Extending planner logic typically requires changes to schema models, prompts, validator, and generator.</p>"},{"location":"extensions/extend-planner/#1-update-ast-schemas","title":"1. Update AST schemas","text":"<p>Modify <code>PlanModel</code> or related AST models in:</p> <ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/ast_planner/schemas.py</code></li> </ul> <p>Keep these rules in mind:</p> <ul> <li>Models are strict (<code>extra=\"forbid\"</code>).</li> <li>Ordinals must be contiguous (validated by the logical validator).</li> <li>Expression kinds must pass <code>model_post_init</code> validation.</li> </ul>"},{"location":"extensions/extend-planner/#2-update-planner-prompt","title":"2. Update planner prompt","text":"<p>Update <code>PLANNER_PROMPT</code> and examples in:</p> <ul> <li><code>packages/core/src/nl2sql/pipeline/nodes/ast_planner/prompts.py</code></li> </ul> <p>Ensure structured output still matches <code>PlanModel</code>.</p>"},{"location":"extensions/extend-planner/#3-update-validator-logic","title":"3. Update validator logic","text":"<p>The logical validator enforces:</p> <ul> <li>query type (READ-only)</li> <li>table/column existence</li> <li>alias uniqueness</li> <li>join relationships</li> <li>expected schema alignment</li> </ul> <p>Update <code>LogicalValidatorNode</code> to recognize any new AST constructs.</p>"},{"location":"extensions/extend-planner/#4-update-sql-generator","title":"4. Update SQL generator","text":"<p><code>GeneratorNode</code> and <code>SqlVisitor</code> translate the AST into <code>sqlglot</code> expressions:</p> <ul> <li>Extend <code>SqlVisitor</code> to handle new expression kinds or operators.</li> <li>Update <code>GeneratorNode._generate_sql()</code> if new clauses are added.</li> </ul>"},{"location":"extensions/extend-planner/#source-references","title":"Source references","text":"<ul> <li>AST schemas: <code>packages/core/src/nl2sql/pipeline/nodes/ast_planner/schemas.py</code></li> <li>Planner node: <code>packages/core/src/nl2sql/pipeline/nodes/ast_planner/node.py</code></li> <li>Logical validator: <code>packages/core/src/nl2sql/pipeline/nodes/validator/node.py</code></li> <li>SQL generator: <code>packages/core/src/nl2sql/pipeline/nodes/generator/node.py</code></li> </ul>"},{"location":"getting_started/","title":"Getting Started","text":"<p>NL2SQL supports three ways to get started. Choose the guide that matches how you want to use the platform:</p> <ul> <li>PyPI (Python API): Install <code>nl2sql-core</code> and use it programmatically.</li> <li>Docker (REST API): Run the API service and integrate over HTTP.</li> <li>From Source (Development): Clone the repo for local development and contributions.</li> </ul>"},{"location":"getting_started/#choose-your-path","title":"Choose your path","text":"<ul> <li>PyPI (Python API)</li> <li>Docker (REST API)</li> <li>From Source (Development)</li> <li>Demo Data (CLI-first)</li> </ul>"},{"location":"getting_started/#configuration-prerequisites","title":"Configuration prerequisites","text":"<p>All paths require configuration files. Start with the example configs:</p> <ul> <li><code>configs/datasources.example.yaml</code> \u2192 copy to <code>configs/datasources.yaml</code></li> <li><code>configs/llm.example.yaml</code> \u2192 copy to <code>configs/llm.yaml</code></li> <li><code>configs/policies.example.json</code> \u2192 copy to <code>configs/policies.json</code></li> <li><code>configs/secrets.example.yaml</code> \u2192 copy to <code>configs/secrets.yaml</code> (optional)</li> </ul> <p>See <code>configuration/system.md</code> for environment variables and defaults.</p>"},{"location":"getting_started/demo/","title":"Demo Data (CLI-first)","text":"<p>Use the CLI to generate deterministic demo data and configs, then query them from the CLI. This keeps data generation out of the API runtime and gives you a realistic multi-database scenario with cross-database relationships.</p>"},{"location":"getting_started/demo/#1-install-the-cli","title":"1. Install the CLI","text":"<pre><code># Install from PyPI\npip install nl2sql-cli\n\n# Or install from source (dev)\npip install -e packages/cli\n</code></pre>"},{"location":"getting_started/demo/#2-generate-demo-data-with-the-cli","title":"2. Generate demo data with the CLI","text":"<pre><code>nl2sql setup --demo\n</code></pre> <p>This writes:</p> <ul> <li>SQLite databases in <code>data/demo_lite/</code></li> <li><code>configs/datasources.demo.yaml</code></li> <li><code>configs/llm.demo.yaml</code></li> <li><code>configs/policies.demo.json</code></li> <li><code>configs/sample_questions.demo.yaml</code></li> <li><code>.env.demo</code></li> </ul> <p>For the lite demo, setup also runs schema indexing once automatically.</p>"},{"location":"getting_started/demo/#3-use-demo-data-with-the-cli","title":"3. Use demo data with the CLI","text":"<pre><code># Run a query against demo data\nENV=demo nl2sql run \"Show me broken machines in Austin\"\n\n# Index schemas if you need to re-index after regenerating demo data\nENV=demo nl2sql index\n</code></pre> <p>Note: the demo datasource config uses relative database paths (e.g. <code>data/demo_lite/*.db</code>), so run the CLI from the repo root.</p>"},{"location":"getting_started/demo/#demo-data-architecture","title":"Demo data architecture","text":"<p>The demo models a manufacturing organization with multiple databases and vendors:</p> <ul> <li><code>manufacturing_ref</code> (Postgres/SQLite): shared reference data (factories, roles, shifts)</li> <li><code>manufacturing_ops</code> (Postgres/SQLite): operational data (employees, machines, maintenance)</li> <li><code>manufacturing_supply</code> (MySQL/SQLite): supply chain data (products, suppliers, inventory)</li> <li><code>manufacturing_history</code> (MSSQL/SQLite): historical data (sales orders, production runs)</li> </ul> <p>Cross-database relationships are logical (not enforced by DB constraints), so they mirror real-world enterprise setups where data is distributed across systems.</p>"},{"location":"getting_started/demo/#entity-relationships","title":"Entity relationships","text":"<pre><code>erDiagram\n    FACTORIES {\n        int id PK\n        text name\n        text region\n        int capacity\n    }\n    MACHINE_TYPES {\n        int id PK\n        text model\n        text producer\n        int maintenance_interval_days\n    }\n    SHIFTS {\n        int id PK\n        text name\n        text start_time\n        text end_time\n    }\n    DEPARTMENTS {\n        int id PK\n        text name\n    }\n    EMPLOYEE_ROLES {\n        int id PK\n        text title\n        int department_id\n    }\n    CUSTOMER_SEGMENTS {\n        int id PK\n        text name\n    }\n    EMPLOYEES {\n        int id PK\n        text name\n        int factory_id\n        int shift_id\n        date hire_date\n        int role_id\n        int department_id\n        text status\n    }\n    MACHINES {\n        int id PK\n        int factory_id\n        int type_id\n        text status\n        date installation_date\n        date last_maintenance_date\n    }\n    MAINTENANCE_LOGS {\n        int id PK\n        int machine_id\n        date date\n        text description\n        int technician_id\n        text severity\n        int downtime_hours\n    }\n    PRODUCTS {\n        int id PK\n        text sku\n        text name\n        decimal base_cost\n        text category\n    }\n    SUPPLIERS {\n        int id PK\n        text name\n        text country\n    }\n    INVENTORY {\n        int product_id\n        int factory_id\n        int quantity\n        date last_updated\n    }\n    SUPPLIER_PRODUCTS {\n        int supplier_id\n        int product_id\n    }\n    SALES_ORDERS {\n        int id PK\n        text customer_name\n        date order_date\n        decimal total_amount\n        text status\n        int customer_segment_id\n        int factory_id\n    }\n    SALES_ITEMS {\n        int id PK\n        int order_id\n        int product_id\n        int quantity\n        decimal unit_price\n        decimal discount_pct\n    }\n    PRODUCTION_RUNS {\n        int id PK\n        int factory_id\n        date date\n        int output_quantity\n        int shift_id\n        text status\n    }\n\n    EMPLOYEES ||--o{ FACTORIES : \"works_at\"\n    EMPLOYEES ||--o{ SHIFTS : \"assigned_to\"\n    EMPLOYEES ||--o{ EMPLOYEE_ROLES : \"has_role\"\n    EMPLOYEE_ROLES ||--o{ DEPARTMENTS : \"in_department\"\n    MACHINES ||--o{ FACTORIES : \"located_at\"\n    MACHINES ||--o{ MACHINE_TYPES : \"is_type\"\n    MAINTENANCE_LOGS ||--o{ MACHINES : \"logs_for\"\n    MAINTENANCE_LOGS ||--o{ EMPLOYEES : \"performed_by\"\n    INVENTORY ||--o{ PRODUCTS : \"tracks\"\n    INVENTORY ||--o{ FACTORIES : \"stored_at\"\n    SUPPLIER_PRODUCTS ||--o{ PRODUCTS : \"supplies\"\n    SUPPLIER_PRODUCTS ||--o{ SUPPLIERS : \"sourced_from\"\n    SALES_ITEMS ||--o{ SALES_ORDERS : \"belongs_to\"\n    SALES_ITEMS ||--o{ PRODUCTS : \"sells\"\n    SALES_ORDERS ||--o{ CUSTOMER_SEGMENTS : \"segment\"\n    SALES_ORDERS ||--o{ FACTORIES : \"fulfilled_by\"\n    PRODUCTION_RUNS ||--o{ FACTORIES : \"produced_at\"\n    PRODUCTION_RUNS ||--o{ SHIFTS : \"run_shift\"</code></pre>"},{"location":"getting_started/demo/#data-scenarios-and-volumes","title":"Data scenarios and volumes","text":"<ul> <li>Employees: ~500 across five factories, with roles, departments, and hire dates</li> <li>Machines: ~150 with maintenance intervals and last maintenance dates</li> <li>Maintenance logs: ~250 with severity and downtime hours</li> <li>Inventory: all products across factories with last updated timestamps</li> <li>Sales orders: ~5,000 with seasonal spikes in Q4</li> <li>Production runs: daily runs per factory over the last year</li> </ul> <p>Embedded scenarios: - Low-stock alerts for specific products and factories - Maintenance backlogs for older machines - Seasonal sales spikes and production variability - Data skew across factories to mimic regional load</p>"},{"location":"getting_started/demo/#sample-queries","title":"Sample queries","text":"<p>Single-database examples: - \"Which machines are overdue for maintenance based on last_maintenance_date?\" - \"Show sales orders by status for the last 30 days\" - \"List products with low inventory across all factories\"</p> <p>Cross-database examples (requires multi-datasource querying): - \"Which factories have the highest sales for EV Battery Pack Long Range in Q4?\" - \"Show inventory levels for products with pending orders this month\" - \"Compare production output vs sales orders by factory for the last quarter\" - \"List maintenance technicians assigned to machines with recent error logs\"</p>"},{"location":"getting_started/demo/#relationship-guide","title":"Relationship guide","text":"<p>Common join paths: - <code>manufacturing_ops.employees.factory_id</code> -&gt; <code>manufacturing_ref.factories.id</code> - <code>manufacturing_ops.machines.type_id</code> -&gt; <code>manufacturing_ref.machine_types.id</code> - <code>manufacturing_supply.inventory.product_id</code> -&gt; <code>manufacturing_supply.products.id</code> - <code>manufacturing_history.sales_items.product_id</code> -&gt; <code>manufacturing_supply.products.id</code> - <code>manufacturing_history.sales_orders.factory_id</code> -&gt; <code>manufacturing_ref.factories.id</code></p>"},{"location":"getting_started/demo/#refreshing-demo-data","title":"Refreshing demo data","text":"<p>Regenerate data at any time with:</p> <pre><code>nl2sql setup --demo\n</code></pre> <p>This overwrites all demo databases and regenerates sample questions and configs.</p>"},{"location":"getting_started/docker/","title":"Docker (REST API)","text":"<p>Use this path when you want the HTTP API only.</p>"},{"location":"getting_started/docker/#build-the-image","title":"Build the image","text":"<p>From the repo root:</p> <pre><code># Dev image (local source)\ndocker build -f packages/api/Dockerfile.dev -t nl2sql-api-dev .\n\n# PyPI image (stable releases)\ndocker build -f packages/api/Dockerfile -t nl2sql-api .\n</code></pre>"},{"location":"getting_started/docker/#install-adapters","title":"Install adapters","text":"<p>You choose adapters at build time:</p> <pre><code>docker build -f packages/api/Dockerfile --build-arg NL2SQL_EXTRAS=postgres -t nl2sql-api .\n</code></pre> <p>For dev images:</p> <pre><code>docker build -f packages/api/Dockerfile.dev --build-arg NL2SQL_EXTRAS=all -t nl2sql-api-dev .\n</code></pre>"},{"location":"getting_started/docker/#run-the-api","title":"Run the API","text":"<pre><code>docker run --rm -p 8000:8000 nl2sql-api\n</code></pre> <p>To set environment selection:</p> <pre><code>docker run --rm -p 8000:8000 -e ENV=demo nl2sql-api\n</code></pre>"},{"location":"getting_started/docker/#configuration","title":"Configuration","text":"<p>Mount or bake your config files into the container:</p> <ul> <li><code>configs/datasources.yaml</code></li> <li><code>configs/llm.yaml</code></li> <li><code>configs/policies.json</code></li> <li><code>configs/secrets.yaml</code> (optional)</li> </ul> <p>See <code>configuration/system.md</code> for environment variables and defaults.</p>"},{"location":"getting_started/docker/#api-usage","title":"API usage","text":"<pre><code>curl -X POST http://localhost:8000/api/v1/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"natural_language\":\"Top 5 customers by revenue last quarter?\"}'\n</code></pre>"},{"location":"getting_started/pypi/","title":"PyPI (Python API)","text":"<p>Use this path when you want to embed NL2SQL directly in a Python application. See the Core API documentation at docs/api/index.md.</p>"},{"location":"getting_started/pypi/#install","title":"Install","text":"<pre><code># Core only\npip install nl2sql-core\n\n# install all available adapters. (Never prefer this, until you really need all adapters.)\npip install \"nl2sql-core[all]\"\n\n# Core with selected adapters\npip install \"nl2sql-core[postgres]\"\npip install \"nl2sql-core[mysql,mssql]\"\n</code></pre>"},{"location":"getting_started/pypi/#configure","title":"Configure","text":"<p>Create config files in your working directory:</p> <ul> <li><code>configs/datasources.yaml</code></li> <li><code>configs/llm.yaml</code></li> <li><code>configs/policies.json</code></li> <li><code>configs/secrets.yaml</code> (optional)</li> </ul> <p>Start from the examples in <code>configs/datasources.example.yaml</code>, <code>configs/llm.demo.yaml</code>, and <code>configs/policies.example.json</code>. Detailed schemas for each file:</p> <ul> <li>Datasources</li> <li>LLMs</li> <li>Policies</li> <li>Secrets</li> </ul>"},{"location":"getting_started/pypi/#configuration-paths-at-startup","title":"Configuration paths at startup","text":"<p>You can pass config paths directly when creating the engine:</p> <pre><code>from nl2sql import NL2SQL\n\nengine = NL2SQL(\n    ds_config_path=\"configs/datasources.yaml\",\n    llm_config_path=\"configs/llm.yaml\",\n    secrets_config_path=\"configs/secrets.yaml\",\n    policies_config_path=\"configs/policies.json\"\n)\n</code></pre> <p>Use startup configuration for fixed, app-wide settings. Use runtime registration when you need to add or override datasources/LLMs per request, per tenant, or on a long-running process without restart.</p>"},{"location":"getting_started/pypi/#datasource-configuration","title":"Datasource configuration","text":"<p><code>configs/datasources.yaml</code> defines one or more database connections and limits:</p> <pre><code>version: 1\ndatasources:\n  - id: my_postgres\n    description: \"Primary analytics DB.\"\n    connection:\n      type: postgres\n      host: localhost\n      port: 5432\n      user: ${env:POSTGRES_USER}\n      password: ${env:POSTGRES_PASSWORD}\n      database: analytics\n    statement_timeout_ms: 8000\n    row_limit: 100\n    max_bytes: 10485760\n</code></pre> <p>Install the adapter extras that match <code>connection.type</code>:</p> <pre><code>pip install \"nl2sql-core[postgres]\"\n</code></pre> <p>See Adapters for the full list of available adapters. See Datasource config for field definitions.</p>"},{"location":"getting_started/pypi/#llm-configuration","title":"LLM configuration","text":"<p><code>configs/llm.yaml</code> selects the default model and any per-agent overrides:</p> <p><pre><code>version: 1\ndefault:\n  provider: openai\n  model: gpt-5.2\n  temperature: 0.0\n  api_key: ${env:OPENAI_API_KEY}\nagents:\n  indexing_enrichment:\n    provider: openai\n    model: gpt-5.2\n    temperature: 0.0\n    api_key: ${env:OPENAI_API_KEY}\n</code></pre> See LLM config for full schema details.</p>"},{"location":"getting_started/pypi/#add-a-datasource-at-runtime","title":"Add a datasource at runtime","text":"<p>You can register a datasource programmatically without editing config files:</p> <pre><code>from nl2sql import NL2SQL\n\nengine = NL2SQL()\nengine.add_datasource({\n    \"id\": \"runtime_pg\",\n    \"description\": \"Runtime Postgres datasource\",\n    \"connection\": {\n        \"type\": \"postgres\",\n        \"host\": \"localhost\",\n        \"port\": 5432,\n        \"user\": \"postgres\",\n        \"password\": \"postgres\",\n        \"database\": \"analytics\"\n    },\n    \"statement_timeout_ms\": 8000,\n    \"row_limit\": 100,\n    \"max_bytes\": 10485760\n})\n\nengine.index_datasource(\"runtime_pg\")\n</code></pre> <p>You can also register datasources from a config file at runtime:</p> <pre><code>from nl2sql import NL2SQL\n\nengine = NL2SQL()\nengine.add_datasource_from_config(\"configs/datasources.yaml\")\n</code></pre>"},{"location":"getting_started/pypi/#add-an-llm-at-runtime","title":"Add an LLM at runtime","text":"<p>You can also register or override LLMs programmatically:</p> <pre><code>from nl2sql import NL2SQL\n\nengine = NL2SQL()\nengine.configure_llm({\n    \"name\": \"default\",\n    \"provider\": \"openai\",\n    \"model\": \"gpt-5.2\",\n    \"temperature\": 0.0,\n    \"api_key\": \"${env:OPENAI_API_KEY}\"\n})\n</code></pre> <p>You can also load LLMs from a config file at runtime:</p> <pre><code>from nl2sql import NL2SQL\n\nengine = NL2SQL()\nengine.configure_llm_from_config(\"configs/llm.yaml\")\n</code></pre>"},{"location":"getting_started/pypi/#policies-configuration-rbac","title":"Policies configuration (RBAC)","text":"<p><code>configs/policies.json</code> defines which roles can access which datasources/tables:</p> <p><pre><code>{\n  \"version\": 1,\n  \"roles\": {\n    \"admin\": {\n      \"description\": \"System Administrator\",\n      \"role\": \"admin\",\n      \"allowed_datasources\": [\"*\"],\n      \"allowed_tables\": [\"*\"]\n    }\n  }\n}\n</code></pre> See Policies config for allowed table formats and wildcards.</p>"},{"location":"getting_started/pypi/#secrets-configuration-optional","title":"Secrets configuration (optional)","text":"<p><code>configs/secrets.yaml</code> is only needed when you want to pull secrets from a provider:</p> <p><pre><code>version: 1\nproviders:\n  # - id: azure-prod\n  #   type: azure\n  #   vault_url: \"https://my-vault.vault.azure.net/\"\n</code></pre> See Secrets config for supported providers and fields.</p> <p>Secrets providers are installed via extras and selected in <code>configs/secrets.yaml</code>:</p>"},{"location":"getting_started/pypi/#run-a-query","title":"Run a query","text":"<pre><code>from nl2sql import NL2SQL\nfrom nl2sql.auth.models import UserContext\n\nengine = NL2SQL()\nuser_ctx = UserContext(user_id=\"user123\", roles=[\"admin\"])\n\nresult = engine.run_query(\n    \"Top 5 customers by revenue last quarter?\",\n    user_context=user_ctx\n)\n\nprint(result.final_answer)\nprint(result.sql)\n</code></pre> <p>The <code>QueryResult</code> includes <code>sql</code>, <code>results</code>, <code>final_answer</code>, <code>errors</code>, <code>warnings</code>, and a <code>trace_id</code> for observability.</p>"},{"location":"getting_started/pypi/#lifecycle-configure-index-query","title":"Lifecycle (configure \u2192 index \u2192 query)","text":"<ol> <li>Configure datasources, LLMs, and policies.</li> <li>Index schemas at least once (and after schema changes).</li> <li>Run queries.</li> </ol>"},{"location":"getting_started/pypi/#index-schema-required","title":"Index schema (required)","text":"<pre><code>engine.index_datasource(\"my_postgres\")\nengine.index_all_datasources()\n</code></pre>"},{"location":"getting_started/pypi/#configure-providers","title":"Configure providers","text":"<p>Secrets providers are installed via extras and selected in <code>configs/secrets.yaml</code>:</p> <pre><code>pip install \"nl2sql-core[aws]\"\npip install \"nl2sql-core[azure]\"\npip install \"nl2sql-core[hashicorp]\"\n</code></pre> <p>See Configuration system for environment variables and defaults.</p>"},{"location":"getting_started/source/","title":"From Source (Development)","text":"<p>Use this path when you want to contribute or run the latest changes.</p>"},{"location":"getting_started/source/#clone-and-install","title":"Clone and install","text":"<pre><code>git clone https://github.com/nadeem4/nl2sql.git\ncd nl2sql\n\npython -m venv venv\nsource venv/bin/activate\n\npip install -e packages/adapter-sdk\npip install -e packages/core\npip install -e packages/adapter-sqlalchemy\npip install -e packages/adapters/postgres\n</code></pre> <p>Add any other adapters you need (mysql, mssql, sqlite).</p>"},{"location":"getting_started/source/#configuration","title":"Configuration","text":"<p>Create config files in your working directory:</p> <ul> <li><code>configs/datasources.yaml</code></li> <li><code>configs/llm.yaml</code></li> <li><code>configs/policies.json</code></li> <li><code>configs/secrets.yaml</code> (optional)</li> </ul> <p>Start from <code>configs/*.example.yaml</code> and <code>configs/*.example.json</code>.</p>"},{"location":"getting_started/source/#run-locally","title":"Run locally","text":"<p>Python API:</p> <pre><code>python -c \"from nl2sql import NL2SQL; print(NL2SQL().run_query('hello'))\"\n</code></pre> <p>API service (Docker):</p> <pre><code>python run.py --dockerfile packages/api/Dockerfile.dev --extras postgres\n</code></pre>"},{"location":"observability/error-handling/","title":"Error Handling + Circuit Breakers","text":"<p>NL2SQL represents failures as structured <code>PipelineError</code> objects and propagates them through state. Retries are managed at the subgraph level, and circuit breakers provide fast-fail safety for external dependencies.</p>"},{"location":"observability/error-handling/#error-contract","title":"Error contract","text":"<p><code>PipelineError</code> includes:</p> <ul> <li><code>node</code>, <code>message</code>, <code>severity</code>, <code>error_code</code></li> <li><code>is_retryable</code> derived from severity and error code</li> </ul> <p>Common error codes include <code>MISSING_SQL</code>, <code>EXECUTION_FAILED</code>, <code>PIPELINE_TIMEOUT</code>, <code>SECURITY_VIOLATION</code>.</p>"},{"location":"observability/error-handling/#circuit-breakers","title":"Circuit breakers","text":"<p><code>create_breaker()</code> configures <code>pybreaker.CircuitBreaker</code> instances with observability hooks:</p> <ul> <li><code>LLM_BREAKER</code></li> <li><code>VECTOR_BREAKER</code></li> <li><code>DB_BREAKER</code></li> </ul> <p>Retrieval calls in <code>VectorStore</code> are wrapped with <code>VECTOR_BREAKER</code>. Other breakers are available but not uniformly wired across all execution paths.</p>"},{"location":"observability/error-handling/#failure-flow","title":"Failure flow","text":"<pre><code>flowchart TD\n    Node[Pipeline Node] --&gt; Error[PipelineError]\n    Error --&gt; State[GraphState.errors]\n    State --&gt; Retry{is_retryable?}\n    Retry --&gt;|yes| Refine[RefinerNode / retry loop]\n    Retry --&gt;|no| Stop[Terminate branch]</code></pre> <p>See <code>../architecture/failure_recovery.md</code> for failure domains, retry scope, and recovery limitations.</p>"},{"location":"observability/error-handling/#cancellation-and-timeouts","title":"Cancellation and timeouts","text":"<ul> <li><code>run_with_graph()</code> enforces a global timeout (<code>Settings.global_timeout_sec</code>).</li> <li>Cancellation is honored through <code>nl2sql.common.cancellation</code> checks.</li> </ul>"},{"location":"observability/error-handling/#source-references","title":"Source references","text":"<ul> <li>Error contracts: <code>packages/core/src/nl2sql/common/errors.py</code></li> <li>Circuit breakers: <code>packages/core/src/nl2sql/common/resilience.py</code></li> <li>Retry logic: <code>packages/core/src/nl2sql/pipeline/subgraphs/sql_agent.py</code></li> </ul>"},{"location":"observability/stack/","title":"Observability Stack","text":"<p>NL2SQL provides observability through structured logging, OpenTelemetry metrics, and audit events. The system is designed to be callback-driven, so telemetry is only emitted when callbacks are supplied to <code>run_with_graph()</code>.</p>"},{"location":"observability/stack/#telemetry-flow","title":"Telemetry flow","text":"<pre><code>sequenceDiagram\n    participant Pipeline as run_with_graph()\n    participant Callback as PipelineMonitorCallback\n    participant Metrics as OpenTelemetry metrics\n    participant Audit as EventLogger\n    participant Logs as Logger/JsonFormatter\n\n    Pipeline-&gt;&gt;Callback: callbacks\n    Callback-&gt;&gt;Metrics: configure_metrics()\n    Callback-&gt;&gt;Audit: log_event(llm_interaction)\n    Callback-&gt;&gt;Logs: node start/end logs</code></pre>"},{"location":"observability/stack/#metrics","title":"Metrics","text":"<p><code>configure_metrics()</code> installs an OpenTelemetry meter provider. Exported metrics include:</p> <ul> <li><code>nl2sql.node.duration</code> (histogram)</li> <li><code>nl2sql.token.usage</code> (counter)</li> </ul> <p>Legacy token and latency events are recorded in <code>TOKEN_LOG</code> and <code>LATENCY_LOG</code>.</p>"},{"location":"observability/stack/#audit-logging","title":"Audit logging","text":"<p><code>EventLogger</code> writes JSON events to a rotating log file configured by <code>Settings.audit_log_path</code>. Payloads are sanitized to redact sensitive keys.</p>"},{"location":"observability/stack/#structured-logging","title":"Structured logging","text":"<p>Logging is configured at import time; JSON formatting is enabled when <code>Settings.observability_exporter == \"otlp\"</code>. Trace and tenant context helpers exist (<code>trace_context</code>, <code>tenant_context</code>), but the pipeline does not set them; callers must establish context if they want trace/tenant IDs in logs.</p>"},{"location":"observability/stack/#source-references","title":"Source references","text":"<ul> <li>Metrics: <code>packages/core/src/nl2sql/common/metrics.py</code></li> <li>Audit logging: <code>packages/core/src/nl2sql/common/event_logger.py</code></li> <li>Pipeline callbacks: <code>packages/core/src/nl2sql/services/callbacks/monitor.py</code></li> <li>Logging: <code>packages/core/src/nl2sql/common/logger.py</code></li> </ul>"},{"location":"schema/store/","title":"Schema &amp; Metadata Layer","text":"<p>The schema layer provides authoritative, versioned snapshots and structured retrieval to ground planning. It is split into:</p> <ul> <li>Schema contracts: structural schema (tables, columns, foreign keys).</li> <li>Schema metadata: descriptive enrichment and statistics.</li> <li>Schema store: versioned persistence with fingerprinting.</li> <li>Schema retrieval: staged lookup via schema chunks.</li> </ul>"},{"location":"schema/store/#schema-contracts-and-metadata","title":"Schema contracts and metadata","text":"<p>Schema contracts are defined in <code>nl2sql_adapter_sdk.schema</code>:</p> <ul> <li><code>SchemaContract</code>: datasource_id, engine_type, tables</li> <li><code>TableContract</code>: table ref, column contracts, foreign keys</li> <li><code>ColumnContract</code>: name, type, nullability, primary key flag</li> <li><code>ForeignKeyContract</code>: constrained/referred columns, cardinality, business meaning</li> </ul> <p>Metadata complements contracts:</p> <ul> <li><code>SchemaMetadata</code>: datasource description/domains, table metadata</li> <li><code>TableMetadata</code>: table description, row_count, column metadata</li> <li><code>ColumnMetadata</code>: description, stats, synonyms, PII flag</li> <li><code>ColumnStatistics</code>: distinct counts, min/max, sample values</li> </ul>"},{"location":"schema/store/#snapshotting-and-fingerprinting","title":"Snapshotting and fingerprinting","text":"<p>Each <code>SchemaSnapshot</code> (contract + metadata) is versioned using a deterministic fingerprint:</p> <ul> <li>Fingerprint uses datasource ID, engine type, sorted tables, sorted columns, and sorted foreign keys.</li> <li>Resulting hash is used to deduplicate identical snapshots.</li> </ul>"},{"location":"schema/store/#schema-store-backends","title":"Schema store backends","text":"<p><code>build_schema_store()</code> constructs a store based on settings:</p> <ul> <li><code>InMemorySchemaStore</code>: in-memory, versioned snapshots.</li> <li><code>SqliteSchemaStore</code>: persistent storage with indexes on fingerprint and timestamps.</li> </ul> <p>Schema versions are timestamped and include a fingerprint prefix (e.g., <code>YYYYMMDDhhmmss_&lt;fp8&gt;</code>). Old versions are evicted beyond <code>schema_store_max_versions</code>.</p>"},{"location":"schema/store/#retrieval-and-authority","title":"Retrieval and authority","text":"<p>Schema retrieval resolves authoritative tables/columns from <code>SchemaStore</code>, not from the vector store. Vector store chunks are only used to identify candidates; final schema is resolved via snapshot. If retrieval yields no candidates, the retriever falls back to the full snapshot. <code>schema_version_mismatch_policy</code> governs mismatches between chunk versions and store versions.</p> <p>See <code>../architecture/indexing.md</code> for retrieval stages, chunk types, and vector store behavior.</p>"},{"location":"schema/store/#source-references","title":"Source references","text":"<ul> <li>Contracts and metadata: <code>packages/adapter-sdk/src/nl2sql_adapter_sdk/schema.py</code></li> <li>Fingerprinting: <code>packages/core/src/nl2sql/schema/protocol.py</code></li> <li>Schema store factory: <code>packages/core/src/nl2sql/schema/store.py</code></li> <li>Sqlite store: <code>packages/core/src/nl2sql/schema/sqlite_store.py</code></li> <li>Schema retriever: <code>packages/core/src/nl2sql/pipeline/nodes/schema_retriever/node.py</code></li> </ul>"},{"location":"security/model/","title":"Security Model","text":"<p>Security is enforced at planning time and execution time through RBAC, schema validation, and audit logging. The system is designed to fail closed when authorization cannot be verified.</p>"},{"location":"security/model/#security-controls","title":"Security controls","text":"<ul> <li>RBAC policies loaded from <code>configs/policies.json</code> and evaluated by <code>RBAC</code>.</li> <li>Logical validation enforces schema constraints and policy-based table access.</li> <li>Audit logging records LLM interactions and security-relevant events.</li> <li>Execution isolation can be provided via sandboxed process pools (available, not required by default).</li> </ul> <pre><code>flowchart TD\n    User[UserContext] --&gt; RBAC[RBAC]\n    RBAC --&gt; Validator[LogicalValidatorNode]\n    Validator --&gt; Executor[ExecutorNode]\n    Executor --&gt; Audit[EventLogger]</code></pre>"},{"location":"security/model/#rbac-enforcement-details","title":"RBAC enforcement details","text":"<p><code>LogicalValidatorNode</code> enforces strict namespacing:</p> <ul> <li>Allowed tables must match <code>datasource.table</code> or <code>datasource.*</code>.</li> <li>If no datasource ID is present, validation fails closed.</li> <li>Wildcard access is supported via <code>*</code> in policy lists.</li> </ul>"},{"location":"security/model/#validation-gates","title":"Validation gates","text":"<p>Validation rules are enforced by <code>LogicalValidatorNode</code> and documented in <code>../architecture/invariants.md</code> and <code>../architecture/nodes/logical_validator_node.md</code>.</p>"},{"location":"security/model/#source-references","title":"Source references","text":"<ul> <li>RBAC: <code>packages/core/src/nl2sql/auth/rbac.py</code></li> <li>Validator node: <code>packages/core/src/nl2sql/pipeline/nodes/validator/node.py</code></li> <li>Audit logger: <code>packages/core/src/nl2sql/common/event_logger.py</code></li> </ul>"},{"location":"security/multi-tenant/","title":"Multi-Tenant Isolation Model","text":"<p>Multi-tenancy is enforced through context propagation, artifact partitioning, and policy filtering. Tenant identity originates from <code>Settings.tenant_id</code> and is passed through execution requests and artifact paths.</p>"},{"location":"security/multi-tenant/#tenant-context-propagation","title":"Tenant context propagation","text":"<pre><code>flowchart TD\n    Settings[Settings.tenant_id] --&gt; Context[NL2SQLContext.tenant_id]\n    Context --&gt; Executor[ExecutorNode/ExecutorRequest]\n    Executor --&gt; Artifacts[ArtifactStore.get_upload_path]</code></pre>"},{"location":"security/multi-tenant/#storage-isolation","title":"Storage isolation","text":"<p>The local artifact backend persists data under:</p> <pre><code>&lt;result_artifact_base_uri&gt;/&lt;tenant_id&gt;/&lt;request_id&gt;.parquet\n</code></pre> <p>This ensures per-tenant isolation for artifacts, and downstream aggregation only reads referenced artifacts from the current request.</p> <p>Tenant isolation is not applied to indexing; the vector store and schema store are global in the current implementation (see <code>../architecture/indexing.md</code>).</p>"},{"location":"security/multi-tenant/#authorization-model","title":"Authorization model","text":"<p>RBAC evaluates <code>UserContext.roles</code> against policies defined in <code>configs/policies.json</code>. The validator enforces datasource/table scope at planning time.</p>"},{"location":"security/multi-tenant/#source-references","title":"Source references","text":"<ul> <li>Tenant settings: <code>packages/core/src/nl2sql/common/settings.py</code></li> <li>Context initialization: <code>packages/core/src/nl2sql/context.py</code></li> <li>Artifact paths: <code>packages/core/src/nl2sql/execution/artifacts/local_store.py</code></li> <li>RBAC: <code>packages/core/src/nl2sql/auth/rbac.py</code></li> </ul>"},{"location":"storage/artifact-store/","title":"Artifact Store Architecture","text":"<p>Execution results are persisted as artifacts. The artifact store abstracts storage backends (local, S3, ADLS) and emits <code>ArtifactRef</code> metadata used by aggregation and downstream consumers.</p>"},{"location":"storage/artifact-store/#storage-lifecycle","title":"Storage lifecycle","text":"<pre><code>flowchart TD\n    ResultFrame[ResultFrame] --&gt; Store[ArtifactStore.create_artifact_ref]\n    Store --&gt; Parquet[Parquet Object]\n    Parquet --&gt; Ref[ArtifactRef]\n    Ref --&gt; Aggregation[EngineAggregatorNode]</code></pre>"},{"location":"storage/artifact-store/#artifactref-fields","title":"ArtifactRef fields","text":"<p><code>ArtifactRef</code> contains:</p> <ul> <li><code>uri</code>, <code>backend</code>, <code>format</code></li> <li><code>row_count</code>, <code>columns</code>, <code>bytes</code></li> <li><code>content_hash</code>, <code>created_at</code></li> <li>optional <code>schema_version</code></li> <li><code>path_template</code></li> </ul>"},{"location":"storage/artifact-store/#backends","title":"Backends","text":"<ul> <li>Local: writes Parquet to <code>result_artifact_base_uri</code>.</li> <li>S3: writes artifacts to S3 (settings-backed).</li> <li>ADLS: writes artifacts to Azure Data Lake Storage.</li> </ul> <p>Backend selection is controlled by <code>Settings.result_artifact_backend</code>.</p>"},{"location":"storage/artifact-store/#path-templating","title":"Path templating","text":"<p><code>Settings.result_artifact_path_template</code> defines the intended path layout for object-store backends:</p> <pre><code>&lt;tenant_id&gt;/&lt;request_id&gt;/&lt;subgraph_name&gt;/&lt;dag_node_id&gt;/&lt;schema_version&gt;/part-00000.parquet\n</code></pre> <p>S3/ADLS backends call <code>_render_path(metadata)</code> to apply this template, but that helper is not implemented in this repository. As written, S3/ADLS artifact stores will raise at runtime when they attempt to render a path. Local storage does not use the template.</p>"},{"location":"storage/artifact-store/#tenant-aware-paths-local-backend","title":"Tenant-aware paths (local backend)","text":"<p>The local backend ignores <code>result_artifact_path_template</code> and uses:</p> <pre><code>&lt;result_artifact_base_uri&gt;/&lt;tenant_id&gt;/&lt;request_id&gt;.parquet\n</code></pre>"},{"location":"storage/artifact-store/#source-references","title":"Source references","text":"<ul> <li>Artifact store base: <code>packages/core/src/nl2sql/execution/artifacts/base.py</code></li> <li>Local store: <code>packages/core/src/nl2sql/execution/artifacts/local_store.py</code></li> <li>Artifact contracts: <code>packages/core/src/nl2sql/execution/contracts.py</code></li> </ul>"},{"location":"testing/architecture/","title":"Testing Architecture","text":"<p>Tests are organized by scope and live under <code>packages/**/tests</code>. The core engine has unit, integration, and end-to-end tests.</p>"},{"location":"testing/architecture/#test-layout","title":"Test layout","text":"<pre><code>packages/core/tests/\n  unit/          # Node-level tests, registries, stores\n  integration/   # Pipeline components with real data\n  e2e/           # End-to-end flows\n</code></pre> <p><code>pytest.ini</code> defines test paths and markers.</p> <pre><code>flowchart TD\n    Unit[Unit Tests] --&gt; Core[Core Components]\n    Integration[Integration Tests] --&gt; Pipeline[Pipeline Nodes]\n    E2E[End-to-end Tests] --&gt; Orchestration[Full Orchestration]</code></pre>"},{"location":"testing/architecture/#runtime-coverage-targets","title":"Runtime coverage targets","text":"<ul> <li>Pipeline nodes (<code>test_node_*.py</code>)</li> <li>Subgraph orchestration (<code>test_sql_agent_subgraph.py</code>)</li> <li>DAG layering (<code>test_graph_layers.py</code>)</li> <li>Registry and store behavior (<code>test_*_registry.py</code>, <code>test_schema_store.py</code>)</li> </ul>"},{"location":"testing/architecture/#deterministic-validation","title":"Deterministic validation","text":"<p>Tests enforce deterministic behavior by:</p> <ul> <li>using structured output schemas for planner/decomposer</li> <li>validating stable IDs and DAG layering</li> <li>running logical validator checks for expected schema alignment</li> </ul>"},{"location":"testing/architecture/#adapter-compliance-testing","title":"Adapter compliance testing","text":"<p>Adapter SDK includes testing utilities to validate schema introspection and result contracts for new adapters.</p>"},{"location":"testing/architecture/#source-references","title":"Source references","text":"<ul> <li>Test configuration: <code>pytest.ini</code></li> <li>Core tests: <code>packages/core/tests/</code></li> </ul>"}]}