import pandas as pd
import sqlglot
from sqlglot import exp
from typing import List, Dict, Any, Optional

class ModelEvaluator:
    """Evaluates the correctness of AI-generated SQL and its execution results."""

    @staticmethod
    def compare_sql_semantic(generated_sql: str, expected_sql: str) -> bool:
        """Compares two SQL queries semantically by normalizing them to ASTs.
        
        Strict Mode: Raises generic exceptions (ValueError) if parsing fails.

        Args:
            generated_sql (str): The SQL generated by the model.
            expected_sql (str): The ground truth SQL.

        Returns:
            bool: True if semantically equivalent, False otherwise.

        Raises:
            ValueError: If either SQL query is invalid or unparseable.
        """
        if not generated_sql or not expected_sql:
            return False
            
        if generated_sql.strip() == expected_sql.strip():
            return True

        try:
            gen_ast = sqlglot.parse_one(generated_sql)
        except Exception as e:
            raise ValueError(f"Generated SQL is invalid/unparseable: {e}")

        try:
            exp_ast = sqlglot.parse_one(expected_sql)
        except Exception as e:
            raise ValueError(f"Ground Truth SQL is invalid/unparseable: {e}")
            
        return gen_ast.sql() == exp_ast.sql()

    @staticmethod
    def compare_results(
        generated_rows: List[Dict[str, Any]], 
        expected_rows: List[Dict[str, Any]], 
        order_matters: bool = False
    ) -> bool:
        """Compares two result sets (lists of dicts).

        Args:
            generated_rows (List[Dict[str, Any]]): Rows returned by the AI query.
            expected_rows (List[Dict[str, Any]]): Rows returned by the ground truth query.
            order_matters (bool): If True, strictly enforces row order.

        Returns:
            bool: True if the result sets match, False otherwise.
        """
        if len(generated_rows) != len(expected_rows):
            return False
            
        if not generated_rows and not expected_rows:
            return True
            
        try:
            df_gen = pd.DataFrame(generated_rows)
            df_exp = pd.DataFrame(expected_rows)
            
            df_gen.columns = df_gen.columns.str.lower()
            df_exp.columns = df_exp.columns.str.lower()
            
            df_gen = df_gen.reindex(sorted(df_gen.columns), axis=1)
            df_exp = df_exp.reindex(sorted(df_exp.columns), axis=1)
            
            if list(df_gen.columns) != list(df_exp.columns):
                return False
            
            if not order_matters:
                df_gen = df_gen.sort_values(by=list(df_gen.columns)).reset_index(drop=True)
                df_exp = df_exp.sort_values(by=list(df_exp.columns)).reset_index(drop=True)
            

            pd.testing.assert_frame_equal(df_gen, df_exp, check_dtype=False, check_like=True, atol=1e-5)
            return True
            
        except AssertionError:
            return False
        except Exception:
            # Fallback for complex types or other issues
            return False

    @staticmethod
    def calculate_aggregate_metrics(results: List[Dict[str, Any]], total_samples: int) -> Dict[str, Any]:
        """Calculates high-level metrics from a list of result dictionaries.

        Args:
            results (List[Dict[str, Any]]): List of evaluation result dictionaries.
            total_samples (int): Total number of samples evaluated.

        Returns:
            Dict[str, Any]: Dictionary containing aggregate metrics:
                - routing_accuracy
                - execution_accuracy 
                - semantic_sql_accuracy
                - valid_sql_rate
                - layer_distribution (count and percentage)
        """
        if not total_samples:
            return {}

        correct_routing = sum(1 for r in results if r.get("routing_match"))
        correct_sql = sum(1 for r in results if r.get("sql_match"))
        correct_semantic = sum(1 for r in results if r.get("semantic_sql_match"))
        valid_sql = sum(1 for r in results if r.get("status") not in ["EXEC_FAIL", "NO_SQL", "ERROR", "ROUTE_FAIL", "BAD_CONFIG", "GT_FAIL", "NO_GT", "INVALID_GT", "INVALID_SQL"])
        
        layer_counts = {"layer_1": 0, "layer_2": 0, "layer_3": 0, "fallback": 0}
        for r in results:
            layer = r.get("routing_layer", "unknown")
            if layer in layer_counts:
                layer_counts[layer] += 1
        

        total_with_gt = sum(1 for r in results if r.get("status") not in ["NO_GT", "INVALID_GT"])
        
        metrics = {
            "routing_accuracy": (correct_routing / total_samples) * 100,
            "execution_accuracy": (correct_sql / total_with_gt) * 100 if total_with_gt > 0 else 0.0,
            "semantic_sql_accuracy": (correct_semantic / total_with_gt) * 100 if total_with_gt > 0 else 0.0,
            "valid_sql_rate": (valid_sql / total_samples) * 100,
            "layer_distribution": layer_counts,
            "layer_percentages": {k: (v / total_samples) * 100 for k, v in layer_counts.items()}
        }
        
        return metrics
